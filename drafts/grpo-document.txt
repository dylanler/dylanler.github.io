Hugging Face's logo
Hugging Face
Models
Datasets
Spaces
Posts
Docs
Enterprise
Pricing



TRL

TRL

Search documentation
⌘K

main

EN

 12,222
Getting started
TRL
Installation
Quickstart
Conceptual Guides
Dataset Formats
Training FAQ
Understanding Logs
How-to guides
Command Line Interface (CLI)
Customizing the Training
Reducing Memory Usage
Speeding Up Training
Distributing Training
Using Trained Models
Integrations
DeepSpeed
Liger Kernel
PEFT
Unsloth
Examples
Example Overview
Community Tutorials
Sentiment Tuning
Training StackLlama
Detoxifying a Language Model
Learning to Use Tools
Multi Adapter RLHF
API
Trainers
AlignProp
BCO
CPO
DDPO
DPO
Online DPO
GKD
GRPO
KTO
Nash-MD
ORPO
PPO
PRM
Reward
RLOO
SFT
Iterative SFT
XPO
Model Classes
Best of N Sampling
Judges
Callbacks
Data Utilities
Text Environments
Script Utilities
Others
You are viewing main version, which requires installation from source. If you'd like regular pip install, checkout the latest stable version (v0.15.2).
GRPO Trainer


Overview
TRL supports the GRPO Trainer for training language models, as described in the paper DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models by Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, Daya Guo.

The abstract from the paper is the following:

Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.

This post-training method was contributed by Quentin Gallouédec.

Quick start
This example demonstrates how to train a model using the GRPO method. We train a Qwen 0.5B Instruct model with the prompts from the TLDR dataset (completion column is ignored!). You can view the data in the dataset here:


Below is the script to train the model.

Copied
# train_grpo.py
from datasets import load_dataset
from trl import GRPOConfig, GRPOTrainer

dataset = load_dataset("trl-lib/tldr", split="train")

# Define the reward function, which rewards completions that are close to 20 characters
def reward_len(completions, **kwargs):
    return [-abs(20 - len(completion)) for completion in completions]

training_args = GRPOConfig(output_dir="Qwen2-0.5B-GRPO", logging_steps=10)
trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=reward_len,
    args=training_args,
    train_dataset=dataset,
)
trainer.train()
Execute the script using the following command:

Copied
accelerate launch train_grpo.py
Distributed across 8 GPUs, the training takes approximately 1 day.



Looking deeper into the GRPO method
GRPO is an online learning algorithm, meaning it improves iteratively by using the data generated by the trained model itself during training. The intuition behind GRPO objective is to maximize the advantage of the generated completions, while ensuring that the model remains close to the reference policy. To understand how GRPO works, it can be broken down into four main steps: Generating completions, computing the advantage, estimating the KL divergence, and computing the loss.



Generating completions
At each training step, we sample a batch of prompts and generate a set of 
G
G completions for each prompt (denoted as 
o
i
o 
i
​
 ).

Computing the advantage
For each of the 
G
G sequences, we compute the reward using a reward model. To align with the comparative nature of reward models—typically trained on datasets of comparisons between outputs for the same question—the advantage is calculated to reflect these relative comparisons. It is normalized as follows:
A
^
i
,
t
=
r
i
−
mean
(
r
)
std
(
r
)
A
^
  
i,t
​
 = 
std(r)
r 
i
​
 −mean(r)
​
 

This approach gives the method its name: Group Relative Policy Optimization (GRPO).

Estimating the KL divergence
KL divergence is estimated using the approximator introduced by Schulman et al. (2020). The approximator is defined as follows:
D
KL
[
π
θ
∥
π
ref
]
=
π
ref
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
−
log
⁡
π
ref
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
−
1
,
D 
KL
​
 [π 
θ
​
 ∥π 
ref
​
 ]= 
π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
π 
ref
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
​
 −log 
π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
π 
ref
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
​
 −1,

Computing the loss
The objective is to maximize the advantage while ensuring that the model remains close to the reference policy. Consequently, the loss is defined as follows:
L
GRPO
(
θ
)
=
−
1
G
∑
i
=
1
G
1
∣
o
i
∣
∑
t
=
1
∣
o
i
∣
[
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
[
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
]
no grad
A
^
i
,
t
−
β
D
KL
[
π
θ
∥
π
ref
]
]
,
L 
GRPO
​
 (θ)=− 
G
1
​
  
i=1
∑
G
​
  
∣o 
i
​
 ∣
1
​
  
t=1
∑
∣o 
i
​
 ∣
​
 [ 
[π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )] 
no grad
​
 
π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
​
  
A
^
  
i,t
​
 −βD 
KL
​
 [π 
θ
​
 ∥π 
ref
​
 ]],

where the first term represents the scaled advantage and the second term penalizes deviations from the reference policy through KL divergence.

In the original paper, this formulation is generalized to account for multiple updates after each generation (denoted 
μ
μ, can be set with num_iterations in GRPOConfig) by leveraging the clipped surrogate objective:
L
GRPO
(
θ
)
=
−
1
G
∑
i
=
1
G
1
∣
o
i
∣
∑
t
=
1
∣
o
i
∣
[
min
⁡
(
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
π
θ
old
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
A
^
i
,
t
,
 
clip
(
π
θ
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
π
θ
old
(
o
i
,
t
∣
q
,
o
i
,
<
t
)
,
1
−
ϵ
,
1
+
ϵ
)
A
^
i
,
t
)
−
β
D
KL
[
π
θ
∥
π
ref
]
]
,
L 
GRPO
​
 (θ)=− 
G
1
​
  
i=1
∑
G
​
  
∣o 
i
​
 ∣
1
​
  
t=1
∑
∣o 
i
​
 ∣
​
 [min( 
π 
θ 
old
​
 
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
​
  
A
^
  
i,t
​
 ,clip( 
π 
θ 
old
​
 
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
π 
θ
​
 (o 
i,t
​
 ∣q,o 
i,<t
​
 )
​
 ,1−ϵ,1+ϵ) 
A
^
  
i,t
​
 )−βD 
KL
​
 [π 
θ
​
 ∥π 
ref
​
 ]],

where 
clip
(
⋅
,
1
−
ϵ
,
1
+
ϵ
)
clip(⋅,1−ϵ,1+ϵ) ensures that updates do not deviate excessively from the reference policy by bounding the policy ratio between 
1
−
ϵ
1−ϵ and 
1
+
ϵ
1+ϵ. When 
μ
=
1
μ=1 (default in TRL), the clipped surrogate objective simplifies to the original objective.

Logged metrics
The GRPO Trainer logs the following metrics:

completion_length: The average completion length.
reward/{reward_func_name}: The reward computed by each reward function.
reward: The average reward.
reward_std : The average standard deviation within reward groups.
kl : The average KL divergence between the model and the reference model calculated on completions.
Customization
Speed up training with vLLM-powered generation
Generation is often the main bottleneck that makes training slow with online methods. To accelerate generation, you can use vLLM, a library that enables fast generation. To enable it, pass use_vllm=True in the training arguments.

Copied
from trl import GRPOConfig

training_args = GRPOConfig(..., use_vllm=True)
For more information, see Speeding up training with vLLM.

Using a custom reward function
The GRPOTrainer supports using custom reward functions instead of dense reward models. To ensure compatibility, your reward function must satisfy the following requirements:

Input arguments:

The function must accept the following as keyword arguments:

prompts (contains the prompts),
completions (contains the generated completions),
All columns names (but prompt) that the dataset may have. For example, if the dataset contains a column named ground_truth, the function will be called with ground_truth as a keyword argument.
The easiest way to comply with this requirement is to use **kwargs in the function signature.

Depending on the dataset format, the input will vary:

For standard format, prompts and completions will be lists of strings.
For conversational format, prompts and completions will be lists of message dictionaries.
Return value: The function must return a list of floats. Each float represents the reward corresponding to a single completion.

Example 1: Reward longer completions
Below is an example of a reward function for a standard format that rewards longer completions:

Copied
def reward_func(completions, **kwargs):
    """Reward function that gives higher scores to longer completions."""
    return [float(len(completion)) for completion in completions]
You can test it as follows:

Copied
prompts = ["The sky is", "The sun is"]
completions = [" blue.", " in the sky."]
print(reward_func(prompts=prompts, completions=completions))
[6.0, 12.0]
Example 2: Reward completions with specific format
Below is an example of a reward function that checks if the completion has a specific format. This example is inspired by the format reward function used in the paper DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. It is designed for conversational format, where prompts and completions consist of structured messages.

Copied
import re

def format_reward_func(completions, **kwargs):
    """Reward function that checks if the completion has a specific format."""
    pattern = r"^<think>.*?</think><answer>.*?</answer>$"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, content) for content in completion_contents]
    return [1.0 if match else 0.0 for match in matches]
You can test this function as follows:

Copied
prompts = [
    [{"role": "assistant", "content": "What is the result of (1 + 2) * 4?"}],
    [{"role": "assistant", "content": "What is the result of (3 + 1) * 2?"}],
]
completions = [
    [{"role": "assistant", "content": "<think>The sum of 1 and 2 is 3, which we multiply by 4 to get 12.</think><answer>(1 + 2) * 4 = 12</answer>"}],
    [{"role": "assistant", "content": "The sum of 3 and 1 is 4, which we multiply by 2 to get 8. So (3 + 1) * 2 = 8."}],
]
format_reward_func(prompts=prompts, completions=completions)
[1.0, 0.0]
Example 3: Reward completions based on a reference
Below is an example of a reward function that checks if the completion is correct. This example is inspired by the accuracy reward function used in the paper DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. This example is designed for standard format, where the dataset contains a column named ground_truth.

Copied
import re

def reward_func(completions, ground_truth, **kwargs):
    # Regular expression to capture content inside \boxed{}
    matches = [re.search(r"\\boxed\{(.*?)\}", completion) for completion in completions]
    contents = [match.group(1) if match else "" for match in matches]
    # Reward 1 if the content is the same as the ground truth, 0 otherwise
    return [1.0 if c == gt else 0.0 for c, gt in zip(contents, ground_truth)]
You can test this function as follows:

Copied
prompts = ["Problem: Solve the equation $2x + 3 = 7$. Solution:", "Problem: Solve the equation $3x - 5 = 10$."]
completions = [r" The solution is \boxed{2}.", r" The solution is \boxed{6}."]
ground_truth = ["2", "5"]
reward_func(prompts=prompts, completions=completions, ground_truth=ground_truth)
[1.0, 0.0]
Passing the reward function to the trainer
To use your custom reward function, pass it to the GRPOTrainer as follows:

Copied
from trl import GRPOTrainer

trainer = GRPOTrainer(
    reward_funcs=reward_func,
    ...,
)
If you have multiple reward functions, you can pass them as a list:

Copied
from trl import GRPOTrainer

trainer = GRPOTrainer(
    reward_funcs=[reward_func1, reward_func2],
    ...,
)
and the reward will be computed as the sum of the rewards from each function, or the weighted sum if reward_weights is provided in the config.

Note that GRPOTrainer supports multiple reward functions of different types. See the parameters documentation for more details.

GRPOTrainer
class trl.GRPOTrainer
<
source
>
( model: typing.Union[str, transformers.modeling_utils.PreTrainedModel]reward_funcs: typing.Union[str, transformers.modeling_utils.PreTrainedModel, typing.Callable[[list, list], list[float]], list[typing.Union[str, transformers.modeling_utils.PreTrainedModel, typing.Callable[[list, list], list[float]]]]]args: typing.Optional[trl.trainer.grpo_config.GRPOConfig] = Nonetrain_dataset: typing.Union[datasets.arrow_dataset.Dataset, datasets.iterable_dataset.IterableDataset, NoneType] = Noneeval_dataset: typing.Union[datasets.arrow_dataset.Dataset, datasets.iterable_dataset.IterableDataset, dict[str, typing.Union[datasets.arrow_dataset.Dataset, datasets.iterable_dataset.IterableDataset]], NoneType] = Noneprocessing_class: typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = Nonereward_processing_classes: typing.Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, list[transformers.tokenization_utils_base.PreTrainedTokenizerBase], NoneType] = Nonecallbacks: typing.Optional[list[transformers.trainer_callback.TrainerCallback]] = Noneoptimizers: tuple = (None, None)peft_config: typing.Optional[ForwardRef('PeftConfig')] = None )

Parameters

model (Union[str, PreTrainedModel]) — Model to be trained. Can be either:
A string, being the model id of a pretrained model hosted inside a model repo on huggingface.co, or a path to a directory containing model weights saved using save_pretrained, e.g., './my_model_directory/'. The model is loaded using from_pretrained with the keywork arguments in args.model_init_kwargs.
A PreTrainedModel object. Only causal language models are supported.
reward_funcs (Union[RewardFunc, list[RewardFunc]]) — Reward functions to be used for computing the rewards. To compute the rewards, we call all the reward functions with the prompts and completions and sum the rewards. Can be either:
A single reward function, such as:
A string: The model ID of a pretrained model hosted inside a model repo on huggingface.co, or a path to a directory containing model weights saved using save_pretrained, e.g., './my_model_directory/'. The model is loaded using from_pretrained with num_labels=1 and the keyword arguments in args.model_init_kwargs.
A PreTrainedModel object: Only sequence classification models are supported.
A custom reward function: The function is provided with the prompts and the generated completions, plus any additional columns in the dataset. It should return a list of rewards. For more details, see Using a custom reward function.
A list of reward functions, where each item can independently be any of the above types. Mixing different types within the list (e.g., a string model ID and a custom reward function) is allowed.
args (GRPOConfig, optional, defaults to None) — Configuration for this trainer. If None, a default configuration is used.
train_dataset (Dataset or IterableDataset) — Dataset to use for training. It must include a column "prompt". Any additional columns in the dataset is ignored. The format of the samples can be either:
Standard: Each sample contains plain text.
Conversational: Each sample contains structured messages (e.g., role and content).
eval_dataset (Dataset, IterableDataset or dict[str, Union[Dataset, IterableDataset]]) — Dataset to use for evaluation. It must meet the same requirements as train_dataset.
processing_class (PreTrainedTokenizerBase, optional, defaults to None) — Processing class used to process the data. The padding side must be set to “left”. If None, the processing class is loaded from the model’s name with from_pretrained.
reward_processing_classes (Union[PreTrainedTokenizerBase, list[PreTrainedTokenizerBase]], optional, defaults to None) — Processing classes corresponding to the reward functions specified in reward_funcs. Can be either:
A single processing class: Used when reward_funcs contains only one reward function.
A list of processing classes: Must match the order and length of the reward functions in reward_funcs. If set to None, or if an element of the list corresponding to a PreTrainedModel is None, the tokenizer for the model is automatically loaded using from_pretrained. For elements in reward_funcs that are custom reward functions (not PreTrainedModel), the corresponding entries in reward_processing_classes are ignored.
callbacks (list of TrainerCallback, optional, defaults to None) — List of callbacks to customize the training loop. Will add those to the list of default callbacks detailed in here.
If you want to remove one of the default callbacks used, use the remove_callback method.

optimizers (tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by get_linear_schedule_with_warmup controlled by args.
peft_config (~peft.PeftConfig, optional, defaults to None) — PEFT configuration used to wrap the model. If None, the model is not wrapped.
Trainer for the Group Relative Policy Optimization (GRPO) method. This algorithm was initially proposed in the paper DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.

Example:

Copied
from datasets import load_dataset
from trl import GRPOTrainer

dataset = load_dataset("trl-lib/tldr", split="train")

def reward_func(completions, **kwargs):
    # Dummy reward function that rewards completions with more unique letters.
    return [float(len(set(completion))) for completion in completions]

trainer = GRPOTrainer(
    model="Qwen/Qwen2-0.5B-Instruct",
    reward_funcs=reward_func,
    train_dataset=dataset,
)

trainer.train()
create_model_card
<
source
>
( model_name: typing.Optional[str] = Nonedataset_name: typing.Optional[str] = Nonetags: typing.Union[str, list[str], NoneType] = None )

Parameters

model_name (str or None, optional, defaults to None) — Name of the model.
dataset_name (str or None, optional, defaults to None) — Name of the dataset used for training.
tags (str, list[str] or None, optional, defaults to None) — Tags to be associated with the model card.
Creates a draft of a model card using the information available to the Trainer.

GRPOConfig
class trl.GRPOConfig
<
source
>
( output_dir: typing.Optional[str] = Noneoverwrite_output_dir: bool = Falsedo_train: bool = Falsedo_eval: bool = Falsedo_predict: bool = Falseeval_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'prediction_loss_only: bool = Falseper_device_train_batch_size: int = 8per_device_eval_batch_size: int = 8per_gpu_train_batch_size: typing.Optional[int] = Noneper_gpu_eval_batch_size: typing.Optional[int] = Nonegradient_accumulation_steps: int = 1eval_accumulation_steps: typing.Optional[int] = Noneeval_delay: typing.Optional[float] = 0torch_empty_cache_steps: typing.Optional[int] = Nonelearning_rate: float = 1e-06weight_decay: float = 0.0adam_beta1: float = 0.9adam_beta2: float = 0.999adam_epsilon: float = 1e-08max_grad_norm: float = 1.0num_train_epochs: float = 3.0max_steps: int = -1lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'lr_scheduler_kwargs: typing.Union[dict, str, NoneType] = <factory>warmup_ratio: float = 0.0warmup_steps: int = 0log_level: typing.Optional[str] = 'passive'log_level_replica: typing.Optional[str] = 'warning'log_on_each_node: bool = Truelogging_dir: typing.Optional[str] = Nonelogging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'logging_first_step: bool = Falselogging_steps: float = 500logging_nan_inf_filter: bool = Truesave_strategy: typing.Union[transformers.trainer_utils.SaveStrategy, str] = 'steps'save_steps: float = 500save_total_limit: typing.Optional[int] = Nonesave_safetensors: typing.Optional[bool] = Truesave_on_each_node: bool = Falsesave_only_model: bool = Falserestore_callback_states_from_checkpoint: bool = Falseno_cuda: bool = Falseuse_cpu: bool = Falseuse_mps_device: bool = Falseseed: int = 42data_seed: typing.Optional[int] = Nonejit_mode_eval: bool = Falseuse_ipex: bool = Falsebf16: bool = Falsefp16: bool = Falsefp16_opt_level: str = 'O1'half_precision_backend: str = 'auto'bf16_full_eval: bool = Falsefp16_full_eval: bool = Falsetf32: typing.Optional[bool] = Nonelocal_rank: int = -1ddp_backend: typing.Optional[str] = Nonetpu_num_cores: typing.Optional[int] = Nonetpu_metrics_debug: bool = Falsedebug: typing.Union[str, typing.List[transformers.debug_utils.DebugOption]] = ''dataloader_drop_last: bool = Falseeval_steps: typing.Optional[float] = Nonedataloader_num_workers: int = 0dataloader_prefetch_factor: typing.Optional[int] = Nonepast_index: int = -1run_name: typing.Optional[str] = Nonedisable_tqdm: typing.Optional[bool] = Noneremove_unused_columns: typing.Optional[bool] = Falselabel_names: typing.Optional[typing.List[str]] = Noneload_best_model_at_end: typing.Optional[bool] = Falsemetric_for_best_model: typing.Optional[str] = Nonegreater_is_better: typing.Optional[bool] = Noneignore_data_skip: bool = Falsefsdp: typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType] = ''fsdp_min_num_params: int = 0fsdp_config: typing.Union[dict, str, NoneType] = Nonetp_size: typing.Optional[int] = 0fsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = Noneaccelerator_config: typing.Union[dict, str, NoneType] = Nonedeepspeed: typing.Union[dict, str, NoneType] = Nonelabel_smoothing_factor: float = 0.0optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'optim_args: typing.Optional[str] = Noneadafactor: bool = Falsegroup_by_length: bool = Falselength_column_name: typing.Optional[str] = 'length'report_to: typing.Union[NoneType, str, typing.List[str]] = Noneddp_find_unused_parameters: typing.Optional[bool] = Noneddp_bucket_cap_mb: typing.Optional[int] = Noneddp_broadcast_buffers: typing.Optional[bool] = Nonedataloader_pin_memory: bool = Truedataloader_persistent_workers: bool = Falseskip_memory_metrics: bool = Trueuse_legacy_prediction_loop: bool = Falsepush_to_hub: bool = Falseresume_from_checkpoint: typing.Optional[str] = Nonehub_model_id: typing.Optional[str] = Nonehub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'hub_token: typing.Optional[str] = Nonehub_private_repo: typing.Optional[bool] = Nonehub_always_push: bool = Falsegradient_checkpointing: bool = Falsegradient_checkpointing_kwargs: typing.Union[dict, str, NoneType] = Noneinclude_inputs_for_metrics: bool = Falseinclude_for_metrics: typing.List[str] = <factory>eval_do_concat_batches: bool = Truefp16_backend: str = 'auto'evaluation_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = Nonepush_to_hub_model_id: typing.Optional[str] = Nonepush_to_hub_organization: typing.Optional[str] = Nonepush_to_hub_token: typing.Optional[str] = Nonemp_parameters: str = ''auto_find_batch_size: bool = Falsefull_determinism: bool = Falsetorchdynamo: typing.Optional[str] = Noneray_scope: typing.Optional[str] = 'last'ddp_timeout: typing.Optional[int] = 1800torch_compile: bool = Falsetorch_compile_backend: typing.Optional[str] = Nonetorch_compile_mode: typing.Optional[str] = Nonedispatch_batches: typing.Optional[bool] = Nonesplit_batches: typing.Optional[bool] = Noneinclude_tokens_per_second: typing.Optional[bool] = Falseinclude_num_input_tokens_seen: typing.Optional[bool] = Falseneftune_noise_alpha: typing.Optional[float] = Noneoptim_target_modules: typing.Union[NoneType, str, typing.List[str]] = Nonebatch_eval_metrics: bool = Falseeval_on_start: bool = Falseuse_liger_kernel: typing.Optional[bool] = Falseeval_use_gather_object: typing.Optional[bool] = Falseaverage_tokens_across_devices: typing.Optional[bool] = Falsemodel_init_kwargs: typing.Optional[dict] = Nonemax_prompt_length: typing.Optional[int] = 512num_generations: typing.Optional[int] = 8max_completion_length: typing.Optional[int] = 256ds3_gather_for_generation: bool = Truetemperature: float = 0.9top_p: float = 1.0top_k: typing.Optional[int] = 50min_p: typing.Optional[float] = Nonerepetition_penalty: float = 1.0use_vllm: typing.Optional[bool] = Falsevllm_device: typing.Optional[str] = 'auto'vllm_gpu_memory_utilization: float = 0.9vllm_dtype: typing.Optional[str] = 'auto'vllm_max_model_len: typing.Optional[int] = Nonevllm_enable_prefix_caching: typing.Optional[bool] = Truevllm_guided_decoding_regex: typing.Optional[str] = Nonebeta: float = 0.04num_iterations: int = 1epsilon: float = 0.2reward_weights: typing.Optional[list[float]] = Nonesync_ref_model: bool = Falseref_model_mixup_alpha: float = 0.6ref_model_sync_steps: int = 512log_completions: bool = False )

Parameters that control the model and reference model

model_init_kwargs (dict[str, Any] or None, optional, defaults to None) — Keyword arguments for from_pretrained, used when the model argument of the GRPOTrainer is provided as a string.
Parameters that control the data preprocessing

remove_unused_columns (bool, optional, defaults to False) — Whether to only keep the column "prompt" in the dataset. If you use a custom reward function that requires any column other than "prompts" and "completions", you should keep this to False.
max_prompt_length (int or None, optional, defaults to 512) — Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left.
num_generations (int or None, optional, defaults to 8) — Number of generations per prompt to sample. The global batch size (num_processes * per_device_batch_size) must be divisible by this value.
max_completion_length (int or None, optional, defaults to 256) — Maximum length of the generated completion.
ds3_gather_for_generation (bool, optional, defaults to True) — This setting applies to DeepSpeed ZeRO-3. If enabled, the policy model weights are gathered for generation, improving generation speed. However, disabling this option allows training models that exceed the VRAM capacity of a single GPU, albeit at the cost of slower generation. Disabling this option is not compatible with vLLM generation.
Parameters that control generation

temperature (float, defaults to 0.9) — Temperature for sampling. The higher the temperature, the more random the completions.
top_p (float, optional, defaults to 1.0) — Float that controls the cumulative probability of the top tokens to consider. Must be in (0, 1]. Set to 1.0 to consider all tokens.
top_k (int or None, optional, defaults to 50) — Number of highest probability vocabulary tokens to keep for top-k-filtering. If None, top-k-filtering is disabled.
min_p (float or None, optional, defaults to None) — Minimum token probability, which will be scaled by the probability of the most likely token. It must be a value between 0.0 and 1.0. Typical values are in the 0.01-0.2 range.
repetition_penalty (float, optional, defaults to 1.0) — Float that penalizes new tokens based on whether they appear in the prompt and the generated text so far. Values > 1.0 encourage the model to use new tokens, while values < 1.0 encourage the model to repeat tokens.
Parameters that control generation acceleration powered by vLLM

use_vllm (bool, optional, defaults to False) — Whether to use vLLM for generating completions. If set to True, ensure that a GPU is kept unused for training, as vLLM will require one for generation. vLLM must be installed (pip install vllm).
vllm_device (str, optional, defaults to "auto") — Device where vLLM generation will run, e.g. "cuda:1". If set to "auto" (default), the system will automatically select the next available GPU after the last one used for training. This assumes that training has not already occupied all available GPUs. If only one device is available, the device will be shared between both training and vLLM.
vllm_gpu_memory_utilization (float, optional, defaults to 0.9) — Ratio (between 0 and 1) of GPU memory to reserve for the model weights, activations, and KV cache on the device dedicated to generation powered by vLLM. Higher values will increase the KV cache size and thus improve the model’s throughput. However, if the value is too high, it may cause out-of-memory (OOM) errors during initialization.
vllm_dtype (str, optional, defaults to "auto") — Data type to use for vLLM generation. If set to "auto", the data type will be automatically determined based on the model configuration. Find the supported values in the vLLM documentation.
vllm_max_model_len (int or None, optional, defaults to None) — If set, the max_model_len to use for vLLM. This could be useful when running with reduced vllm_gpu_memory_utilization, leading to a reduced KV cache size. If not set, vLLM will use the model context size, which might be much larger than the KV cache, leading to inefficiencies.
vllm_enable_prefix_caching (bool, optional, defaults to True) — Whether to enable prefix caching in vLLM. If set to True (default), ensure that the model and the hardware support this feature.
vllm_guided_decoding_regex (str or None, optional, defaults to None) — Regex for vLLM guided decoding. If None (default), guided decoding is disabled.
Parameters that control the training

learning_rate (float, optional, defaults to 1e-6) — Initial learning rate for AdamW optimizer. The default value replaces that of TrainingArguments.
beta (float, optional, defaults to 0.04) — KL coefficient. If 0.0, the reference model is not loaded, reducing memory usage and improving training speed, but may be numerically unstable for long training runs.
num_iterations (int, optional, defaults to 1) — Number of iterations per batch (denoted as μ in the algorithm).
epsilon (float, optional, defaults to 0.2) — Epsilon value for clipping.
reward_weights (list[float] or None, optional, defaults to None) — Weights for each reward function. Must match the number of reward functions. If None, all rewards are weighted equally with weight 1.0.
sync_ref_model (bool, optional, defaults to False) — Whether to synchronize the reference model with the active model every ref_model_sync_steps steps, using the ref_model_mixup_alpha parameter. This synchronization originites from the TR-DPO paper.
ref_model_mixup_alpha (float, optional, defaults to 0.6) — α parameter from the TR-DPO paper, which controls the mix between the current policy and the previous reference policy during updates. The reference policy is updated according to the equation: π_ref = α * π_θ + (1 - α) * π_ref_prev. To use this parameter, you must set sync_ref_model=True.
ref_model_sync_steps (int, optional, defaults to 512) — τ parameter from the TR-DPO paper, which determines how frequently the current policy is synchronized with the reference policy. To use this parameter, you must set sync_ref_model=True.
Parameters that control the logging

log_completions (bool, optional, defaults to False) — Whether to log a sample of (prompt, completion) pairs every logging_steps steps. If rich is installed, it prints the sample. If wandb logging is enabled, it logs it to wandb.
Configuration class for the GRPOTrainer.

Only the parameters specific to GRPO training are listed here. For details on other parameters, refer to the TrainingArguments documentation.

Using HfArgumentParser we can turn this class into argparse arguments that can be specified on the command line.

<
>
Update on GitHub
←
GKD
KTO
→
GRPO Trainer
Overview
Quick start
Looking deeper into the GRPO method
Generating completions
Computing the advantage
Estimating the KL divergence
Computing the loss
Logged metrics
Customization
Speed up training with vLLM-powered generation
Using a custom reward function
Example 1: Reward longer completions
Example 2: Reward completions with specific format
Example 3: Reward completions based on a reference
Passing the reward function to the trainer
GRPOTrainer
GRPOConfig

