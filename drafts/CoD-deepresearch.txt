Hypothesis
We hypothesize that introducing semantically diverse intermediate token sampling into the Chain of Draft (CoD) reasoning process, coupled with reinforcement learning optimization, will lead to smarter and more efficient problem-solving by the model. In other words, if an LLM explores multiple concise reasoning drafts (instead of one deterministic draft) and learns from them via RL, it will achieve higher accuracy and better “intelligence” (solving more complex problems correctly) without a significant increase in token usage. This approach builds on prior findings that sampling diverse reasoning paths can improve answer correctness. By encouraging varied intermediate thoughts, the model can discover and converge on the most effective reasoning strategies. We expect this to yield accuracy on par with or above standard Chain-of-Thought (CoT) prompting, while preserving CoD’s hallmark efficiency (minimal tokens and low latency). The outcome should be a model that reasons both broadly (through exploring different approaches) and efficiently (through concise drafting), resulting in improved problem-solving capability. For instance, prior work has shown that exploring diverse reasoning paths and selecting a consistent answer can boost performance on math and commonsense tasks by a large margin. Similarly, CoD itself demonstrated that it can “matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens”​
ARXIV.ORG
. Our hypothesis is that combining these ideas – diverse drafts and RL-driven refinement – will further increase the model’s intelligence (accuracy on complex tasks) without sacrificing efficiency.Supporting Example: In one evaluation, CoD prompting reduced the tokens used from 189.4 to just 14.3 on a set of sports questions, while improving accuracy from 93.2% to 97.35%​
TECHXPLORE.COM
. This suggests that concise, diverse reasoning could even surpass verbose reasoning in quality. Thus, we expect an RL-trained, diversity-enhanced CoD strategy to consistently yield such improvements across various reasoning domains.
Experimental Setup
Reasoning Tasks
To thoroughly evaluate the modified CoD strategy, we will test it on a diverse suite of reasoning tasks that cover arithmetic, commonsense, symbolic logic, and coding. This ensures we gauge the method’s effectiveness across different domains of “intelligence.” The specific tasks (and example datasets) include:
Arithmetic Reasoning: Multi-step math word problems that require calculation and logic. Example: GSM8K, a benchmark of grade-school math problems, which is a standard test for CoT reasoning performance. These problems test the model’s ability to break down calculations step by step.
Commonsense Reasoning: Questions requiring everyday reasoning or understanding of basic facts. We will use tasks from BIG-Bench or other benchmarks, such as date understanding and sports understanding scenarios​
ARXIV.ORG
. For instance, a “sports understanding” task might ask the model to reason about a game outcome given certain conditions. These tasks ensure the model can apply world knowledge and common sense in its reasoning.
Symbolic/Logical Reasoning: Puzzles that involve logical manipulation or simulation of rules. A representative task is the coin-flip puzzle (a task introduced in prior reasoning benchmarks) where the model must track a sequence of coin flips or rule-based transformations. Such tasks test the ability to carry out stepwise logical inference. Another example could be sequence transformations or puzzles from the BIG-Bench hard tasks.
Coding Tasks: Problems that require reasoning about code or algorithms, such as writing a small function to meet a specification. We will use a dataset like HumanEval (a set of Python programming challenges) to evaluate coding capabilities. Each problem provides a prompt (docstring) and expects the model to generate correct code. This tests the model’s reasoning in a more structured, goal-oriented domain (simulating how a model plans a solution before coding) and checks if the reasoning strategy improves code correctness.
By selecting these tasks, we cover a spectrum from numerical reasoning to abstract logic and real-world judgment. Each category will have a dedicated evaluation set (e.g., a test split of GSM8K for arithmetic, a subset of BIG-Bench tasks for commonsense/symbolic, and HumanEval problems for coding). This variety helps determine whether the diverse token CoD + RL strategy generalizes across different kinds of reasoning challenges or if its benefits are domain-specific.
LLMs for Evaluation
We will evaluate the reasoning strategies on several Large Language Models to see how the approach scales and whether it benefits different model sizes or architectures. The chosen LLMs include:
GPT-4 (OpenAI, text-davinci- model, denoted GPT-4o): A state-of-the-art model with strong reasoning abilities. GPT-4 has shown high performance on reasoning tasks (e.g., ~95% on GSM8K with CoT prompting) and serves as a high-end baseline. Testing on GPT-4 will show the ceiling of performance and how much improvement is possible when it’s already near optimal.
Claude 3.5 (Anthropic, “Sonnet” version): An advanced model from Anthropic, slightly smaller or differently trained than GPT-4. Claude 3.5 was used in prior CoD research​
TECHXPLORE.COM
, making it a good candidate for replication and comparison. It will demonstrate how the strategy works on a model with strong reasoning but possibly different style (and ensure our results aren’t unique to GPT-4).
Smaller Open-Source LLM (e.g., LLaMA-2 13B or GPT-3.5 equivalent): A model with significantly fewer parameters, to test if the CoD+diversity+RL strategy can help less powerful models. Smaller models often struggle with complex reasoning, so an improvement here would indicate the method’s potential to “lift up” lower-tier models. We will fine-tune or prompt such a model in the same way for fair comparison.
Each model will be evaluated with identical prompts and conditions across the different reasoning strategies (Standard, CoT, CoD, etc.) to ensure fairness. By including a range of model sizes, we can observe whether the modified CoD strategy yields consistent gains (especially in accuracy) irrespective of scale, or whether large models benefit differently than smaller ones.
Baseline Prompting Methods
We will compare our Diverse-Token CoD + RL strategy against several baseline prompting strategies to measure improvements. The baselines are:
Standard Prompting (Direct Answer): The model is given the problem and asked to output the answer directly without any explicit reasoning prompt. This represents the simplest usage of the model and often results in decent answers for easy questions but fails on complex multi-step problems. It has minimal token usage (just the answer) but lower accuracy (for example, GPT-4 gets only ~53% on GSM8K with direct answering). This baseline sets a reference for performance without guided reasoning.
Chain of Thought (CoT) Prompting: The model is prompted to “think step by step” and produce a detailed, stepwise reasoning in natural language before giving the final answer. This tends to greatly improve accuracy on complex tasks by breaking them down. However, it comes at the cost of verbosity – many extra tokens are generated. For instance, CoT can boost a model like GPT-4o to ~95% accuracy on math problems, but with an output of ~205 tokens on average, incurring higher latency and cost. CoT will serve as a strong performance baseline (often near state-of-the-art accuracy), against which we judge if our method’s accuracy is competitive.
Standard Chain of Draft (CoD) Prompting: The model is instructed to produce concise intermediate drafts of reasoning rather than elaborate CoT-style explanations. This usually involves limiting each reasoning step to a few words or a short phrase capturing essential information. CoD’s goal is to maintain the logical steps of CoT but in a minimal form, thereby saving tokens. Prior work shows CoD can match or nearly match CoT accuracy while using a small fraction of tokens. For example, Claude 3.5 using CoD achieved almost the same accuracy as with CoT in several tasks, while using only ~7.6% of the words​
ARXIV.ORG
. CoD will be our efficiency baseline – it indicates how far brevity alone can go without special sampling or RL.
(Ours) Diverse Draft + RL (Proposed): This is the new strategy we are testing – essentially an enhanced CoD. The model is prompted similarly to CoD to use minimal drafts, but we also incorporate a diverse token sampling mechanism (described below) and further train the model with RL to optimize its use of these diverse drafts. At inference time, the model can still produce a single concise chain-of-draft, but it has been optimized to choose high-reward (accurate and brief) reasoning paths. We expect this method to outperform Standard and CoD in accuracy, and potentially rival CoT’s accuracy, all while keeping token counts low.
All methods will be given the same initial prompts or few-shot examples for fairness. For CoT and CoD, we may provide an example in the prompt demonstrating the format (detailed steps vs. drafted steps). For our method during evaluation, we will use the model after RL training and simply prompt it in the CoD style (the diversity is built into its training).By comparing these approaches, we can quantify the benefit of each component: CoT shows the value of stepwise reasoning, CoD shows the value of brevity, and our method should show the added value of diversity + RL optimization.

Figure: Token usage comparison for Standard prompting (blue), CoT (orange), and CoD (red) on various tasks. CoT is much more verbose (tall orange bars) than direct answers, while CoD dramatically cuts the number of tokens needed (short red bars) without significant accuracy loss. Our experiment tests whether adding diverse sampling + RL to CoD can retain these efficiency gains (low token count) while boosting accuracy further.
Diverse Token Sampling Strategy
To implement semantically diverse intermediate reasoning in CoD, we will employ a specialized token sampling strategy during the generation of each reasoning step. The idea is to prevent the model from always choosing the single most likely next token, and instead allow it to explore multiple plausible continuations for the reasoning. Key aspects of this strategy include:
Stochastic Decoding for Intermediate Steps: Instead of greedy decoding, we use a higher-entropy sampling method (such as temperature sampling or nucleus sampling) when generating each draft step. For example, we might set a temperature $T$ (e.g., 1.2) that encourages more randomness, so the model might say “compute sum” in one draft and “add them” in another, for the same step. By doing this, the content of the reasoning drafts can vary each time the model tackles a problem, producing semantically different tokens that still make sense in context.
Semantic Diversity Constraint: We ensure that the diversity is not just gibberish or irrelevant variance. The model is guided (via the prompt and later via RL rewards) to produce valid but varied reasoning. This can be facilitated by instructions like “think of another way if possible” or by simply relying on the randomness of sampling coupled with the model’s knowledge. We will monitor diversity by checking that intermediate tokens/phrases have different wording or approach (e.g., one draft might use an algebraic explanation, another a words-based logic). We may also enforce that duplicate sequences are disfavored during sampling (for instance, using nucleus sampling to cut off deterministic tail probabilities).
Multiple Draft Sampling: During training time (in the RL phase), the model will generate multiple reasoning drafts for each query by sampling different token sequences. For example, for a given math problem, we might sample 5 distinct draft-chains (each chain being a sequence of concise steps) by varying the random seed. Each of these chains leads to a final answer. This mimics the self-consistency approach (where multiple reasoning paths are explored) but in the CoD style. By obtaining a diverse set of attempted solutions, we can identify which reasoning trajectories tend to yield correct answers and use that signal for training (rewarding those paths).
Draft Length Control: We maintain CoD’s principle of brevity by limiting each intermediate reasoning step to, say, a maximum of 5 tokens (as done by the CoD authors to force conciseness​
TECHXPLORE.COM
). The sampling occurs within this constraint – the model must express a step in a few words, but which words it uses can vary. This prevents the diversity mechanism from inadvertently producing verbose CoT-like steps; all explorations remain minimal. Essentially, the model might try different minimal hints toward the next part of the solution.
In summary, during the experiment, whenever the model generates a reasoning chain (especially in training), it won’t deterministically follow one fixed path. Instead, it will explore a spectrum of concise reasoning paths. This diversity is crucial for the RL optimization phase: it provides a richer set of experiences from which the model can learn which reasoning tokens are more or less useful. By the end of training, we anticipate the model will internally weigh different reasoning token choices and favor those that lead to correct answers (thus effectively learning a policy over reasoning token selection).
Data Collection and Evaluation Metrics
Benchmark Datasets
For each category of reasoning task, we will use established benchmark datasets or challenge sets to evaluate performance. Using standard benchmarks ensures results are comparable to prior work (e.g., CoT or CoD studies) and that they cover a range of difficulties. The datasets include:
GSM8K (Arithmetic Word Problems): A widely-used benchmark of 8.5K grade school math problems requiring multi-step reasoning. We will use its public test set to measure accuracy in arithmetic reasoning. Each problem is a short story (e.g., “If there are 3 apples and you buy 5 more...”) with a numerical answer. This dataset is excellent for testing how well the model can break down calculations and whether our method improves correctness on complex math.
Big-Bench Commonsense Tasks: We include tasks like Date Understanding and Sports Understanding from the BIG-Bench (Beyond the Imitation Game) suite​
ARXIV.ORG
. In date understanding, models must perform operations like adding days to dates or interpreting dates, which tests logical reasoning and a bit of world knowledge. Sports understanding involves reasoning about game outcomes or standings (for example, inferring a team’s win/loss given conditions). These tasks check the model’s ability to use common sense and simple world facts in a reasoning process. We will use the predefined queries from BIG-Bench for these tasks and evaluate accuracy of answers.
Symbolic Reasoning Puzzles: We will use the coin flip puzzle task (as mentioned in prior CoT research) and similar logical puzzles. In a coin flip task, the model might be told “A coin is flipped 3 times, you see X, what is probability/what is outcome…” or “Flip a coin, then do X if heads… what is the final result?” requiring tracking of possibilities or states. These tasks often appear in reasoning benchmarks to test systematic logical computation. We might use a small custom set or one from literature (for instance, the coin-flip problems used by Wei et al. in their CoT paper). Accuracy here means correctly simulating the logic or providing the right logical conclusion.
HumanEval (Code Generation Tasks): A set of programming problems where the model must produce a correct function given a description. We will take a subset of HumanEval (for example, 50 problems covering various algorithms). Each problem has test cases to determine if the generated code is correct. We will use pass@1 accuracy (i.e., the model’s first attempt is correct) as the metric for coding, as is standard​
REDDIT.COM
. This tests whether reasoning strategies improve the model’s ability to plan and produce correct code. Because coding tasks are sensitive to logical planning (e.g., considering edge cases, stepwise solving), a reasoning strategy can help the model outline the solution before writing code. We will ensure the model is allowed to “think” in pseudo-code or comments (for CoT/CoD) but only the final answer (the function code) is evaluated for correctness.
For each dataset, we will compile a test set (if not already standard) and possibly a validation set to tune any prompt details. The number of queries in each might be: GSM8K test (~1319 problems), Big-Bench tasks (~hundreds each or as available), coin-flip puzzles (~20-50 puzzles), HumanEval problems (~164 total, we might use all or most). This gives a robust sample to measure performance metrics. During data collection, we will record the model’s responses under each strategy for each query, along with metadata like tokens used and time taken.
Evaluation Metrics
We will evaluate the performance of each reasoning strategy using a comprehensive set of metrics that capture not only accuracy but also efficiency and quality of reasoning. The key metrics include:
Accuracy (Task Success Rate): The primary metric is the percentage of queries answered correctly. For QA or math tasks, an answer is “correct” if it exactly matches the ground-truth answer (or falls within an acceptable numerical tolerance for arithmetic). For coding tasks, accuracy is measured by functional correctness (did the generated code pass all test cases). We will compute accuracy per dataset (e.g., model got 90% of GSM8K correct) and also an overall average if meaningful. This metric directly reflects the model’s problem-solving capability or “intelligence.” We will compare accuracy across prompting methods to see which yields the best results.
Token Efficiency (Tokens Used per Solution): We measure the average number of tokens the model generates in its output for each query, including both reasoning and the final answer. Since one of CoD’s goals is to reduce verbosity, this metric is crucial. We will report the mean output token count for each strategy on each task. Lower is better for efficiency, as it implies less cost and faster responses. For example, we expect CoT to have high token count (often hundreds) and CoD much lower (maybe tens). Our new method aims to keep token count close to CoD levels. If our diversity approach requires multiple attempts, we will also note total tokens consumed (if doing multiple drafts per query in inference, though ideally our final method will produce one draft per query). This metric ties to latency as well.
Reasoning Diversity Score: To quantify the diversity of reasoning, we will introduce a metric that captures how varied the intermediate reasoning steps are. One approach is to run the model on the same query multiple times (especially for our method which is nondeterministic) and measure the diversity among the generated reasoning traces. We can use metrics like distinct n-grams in the reasoning steps or the average cosine distance between embedding representations of different reasoning sequences. A higher diversity score means the model is exploring substantially different phrasing or approaches in its reasoning. We expect Standard and CoT (greedy) to have near-zero diversity (they produce the same reasoning every time), whereas our method should score higher due to stochastic sampling. We will use this to verify that the “semantically diverse sampling” is indeed happening. Additionally, we’ll qualitatively verify diversity by examining examples of reasoning outputs.
Latency: The time taken to produce an answer, measured to assess practicality. This can be measured in seconds per query on a fixed hardware setup, or in relative terms like tokens per second. Latency is influenced by the number of tokens and the number of forward passes. For single-pass strategies (Standard, CoT, CoD, our final RL-tuned model), latency is roughly proportional to token count. CoT will have higher latency due to lengthy output (e.g., nearly 4.2 seconds vs 1.0 second for CoD in one setting). We aim for our method to remain low-latency like CoD. If our method involves multiple samples at inference (optional), that could increase latency linearly with number of samples – we will report if that trade-off is needed or if the RL training allows us to avoid multiple runtime samples. Ultimately, we will compare latency numbers to ensure the method is practical (e.g., ideally not significantly slower than standard CoD).
RL Reward and Convergence: Although not a user-facing metric, we will monitor the reward achieved during RL training as a measure of learning progress. The reward function (defined below) encapsulates correctness and brevity; we will track the average reward per episode over training iterations. A rising reward indicates the model is improving its policy. At evaluation, we won’t directly use reward, but this helps ensure our RL has effectively optimized what we care about. Also, we might report the final policy’s average reward on a validation set to illustrate how well it balances accuracy and token usage.
Draft Steps Count: As a supplementary metric, we will count the average number of reasoning steps (drafts) used by each strategy. CoT might use many small steps, CoD uses fewer (since it tries to compress reasoning). Our method might find an optimal number of steps – possibly similar to CoD. This metric is related to token count but distinct if some strategies use longer but fewer steps or vice versa. It can shed light on how the model organizes its thinking (e.g., does it break the problem into 3 big draft steps or 6 tiny ones, etc.).
Error Analysis Metrics: Post-hoc, we will also categorize errors (where the answer was wrong) by type: logical errors, arithmetic mistakes, off-by-one, etc., to see if our method reduces certain types of errors compared to CoT or Standard. While not a single number metric, this analysis helps qualitatively understand why the new strategy may be better (e.g., “diverse drafts caught an ambiguity that a single draft missed”).
All these metrics will be collected for each model and each prompting strategy. The evaluation will produce a set of tables and graphs: for example, a table of accuracy & token count for each method per task, and perhaps plots showing the accuracy vs token trade-off. We will also log model outputs to enable manual inspection of reasoning diversity and correctness. By considering both accuracy and efficiency together, we can quantify “intelligence” improvements in a balanced way. A successful outcome would show high accuracy and low token count for our method, shifting the Pareto frontier of what LLMs can achieve.RL Reward Function: It’s worth noting how we design the reward for the RL training, as it directly ties to these metrics. We define a reward $R$ for each query (episode) as follows: $R = 1.0$ for a correct final answer (0 for incorrect) minus a small penalty for each token generated in the reasoning. For example, $R = 1 - 0.001\times (\text{# of output tokens})$ could be a starting formula. This means a perfect score of 1 is achievable with a correct answer and virtually no extra tokens; a correct answer with 50 tokens yields $R \approx 0.95$, and an incorrect answer yields 0 regardless of tokens. This reward setup explicitly encourages the model to be correct and concise, aligning with our metrics of accuracy and efficiency. During training, the RL algorithm will seek to maximize this reward, thus implicitly optimizing those metrics. We might adjust the token penalty weight (0.001 in this example) to tune the trade-off between accuracy and brevity (based on validation performance). The reward function essentially encodes our hypothesis: that terse, accurate reasoning is the goal.
Implementation of Reinforcement Learning
RL Framework & Algorithm: We frame the reasoning task as a sequential decision-making process (a Markov Decision Process, MDP) where the agent is the LLM generating tokens. Each state in this MDP consists of the current conversation context: initially the problem question (and any prompt instructions), and later the question plus any draft reasoning tokens the model has already produced. At each time step, the model chooses an action – which in our case is generating the next token (or potentially the next few tokens up to a delimiter or until a step is complete). The process continues until the model outputs a special end-of-reasoning token or produces a final answer token. Once the final answer is given, the environment provides a reward (as defined above) based on the answer’s correctness and the length of the sequence.Given the enormous action space (the vocabulary of possible tokens) and the need for stable training, we will use Proximal Policy Optimization (PPO) as our reinforcement learning algorithm. PPO is a policy-gradient method well-suited for fine-tuning language models, known for its stability and reliability in large action spaces. In practice, PPO (with clipping) has been used in Reinforcement Learning from Human Feedback (RLHF) to train models like ChatGPT, demonstrating its effectiveness on text-generation tasks. We will implement PPO such that the LLM’s policy $\pi_\theta(token|state)$ (with parameters $\theta$ initialized from a pre-trained model) is updated to maximize the expected reward.Training Process: We will initialize the model with a supervised fine-tuning baseline (possibly the model already fine-tuned to follow instructions or even fine-tuned on a few CoT examples). This helps to have a reasonable starting policy. Then we conduct iterative training episodes:
Episode Generation (Rollout): We present a problem (from our training set of tasks) to the model. The model generates a reasoning draft step by step, using the diverse token sampling strategy – effectively, the policy is stochastic, so each rollout may be different. The episode ends when the model outputs an answer. We record the sequence of states, actions (tokens), and the final reward (computed by checking the answer).
Policy Update: Using PPO, we adjust the model’s parameters to increase the probability of token actions that led to higher rewards and decrease the probability of those leading to poor outcomes. PPO does this carefully to avoid too-large updates (ensuring the new policy doesn’t deviate too far from the old one in one step, which prevents instability). We use a batch of episodes (across different problems) for each update. Over time, this optimization should make correct, concise reasoning sequences more likely under the model’s policy.
Exploration vs. Exploitation: To ensure the model doesn’t prematurely converge to a suboptimal way of reasoning, we maintain exploration during training. The stochastic sampling inherently provides exploration (especially with higher temperature early on). Additionally, we include an entropy bonus in the PPO objective – a term that encourages the policy to maintain uncertainty (i.e., not put all probability on one token). This prevents the model from becoming too deterministic too quickly, and fosters continued generation of diverse reasoning tokens. By doing so, we combat the known issue that “during RL training models fail to explore significantly beyond solutions already produced by supervised models”. Our approach explicitly injects more exploration via diverse token choices, helping the model discover new reasoning patterns that a greedy policy might never try.
Value Function & Reward Shaping: PPO typically uses an auxiliary value function to predict expected reward from a state, which helps reduce variance in training. We will train a value head alongside the policy to estimate the reward of a given partial solution. We keep the reward mostly sparse (only at the end), but we have shaped it slightly with the token penalty. We avoid giving direct intermediate step rewards to not bias the model toward any single known solution path; we want it to freely explore (except for the eventual correctness signal). If needed, we could experiment with a small intermediate reward for matching certain necessary sub-steps (like getting an intermediate arithmetic result correct), but primarily the reward is the final outcome.
Algorithm Hyperparameters: We will run PPO for a certain number of iterations or until convergence. We’ll choose a learning rate and clipping parameter appropriate for large models (small learning rate to avoid destabilizing the pre-trained knowledge). Batch sizes might be on the order of dozens of episodes per update. We might train for several thousand episodes, sampling across our task training set. Early stopping or model checkpointing will be used to avoid overfitting (especially since we have a fixed set of training puzzles; although many reasoning tasks can be generated or augmented if needed to supply enough training data).
Throughout training, we will evaluate on a validation set of problems (distinct from training) to monitor how accuracy and token counts are trending. We expect to see the average reward per episode go up, the validation accuracy approach or exceed the CoT baseline, and the average tokens per solution remain low or even decrease (as the model learns to avoid unnecessary drafts). If the model starts to exploit the token penalty too much (e.g., by giving very short but perhaps cryptic answers), we might adjust the penalty weight to ensure correctness isn’t sacrificed. PPO’s iterative nature should balance this: if the model gets too terse and makes errors, the reward drops, pushing it back toward more accurate reasoning.Integration of Diverse Tokens in RL: The diverse token sampling plays a critical role during training. Because we sample actions stochastically, the training data for PPO will include a variety of trajectories for the same problem. Some trajectories will yield the correct answer (reward ~1) and others will not (reward 0). The RL algorithm will thus get a signal that, for the same state (problem), certain token choices lead to success, others to failure. This encourages the model to assign higher probability to those successful token sequences in the future. In effect, the model learns from its own exploratory mistakes and successes. Over many problems, it may learn general heuristics (e.g., “it’s often useful to first simplify the math problem” or “if a certain approach wording led to dead-end, try a different approach”). The diverse sampling ensures the model isn’t just reinforcing one path – it considers alternatives and learns the best one.Concretely, suppose the task is a math problem. One draft reasoning (trajectory A) the model tries is to set up an equation, which leads to a correct answer (reward 1). Another draft (trajectory B) it tries on another iteration is to do a guess-and-check, which leads to a wrong answer (reward 0). The policy update will strengthen the tokens in trajectory A (so next time it’s more likely to start with the equation approach) and weaken tokens in B (making it less likely to guess-and-check in the future for similar problems). Without diversity, the model might never have tried approach A if B was more obvious from pre-training; with our method, it had the opportunity to try both and identify the better strategy. Over thousands of such instances, the model should internalize better reasoning patterns.We will also examine the trained model’s behavior to ensure it hasn’t collapsed to trivial solutions. The entropy bonus and diversity aim to maintain some creativity. After training, at inference, we can run the model in a deterministic mode (temperature 0) to see its “optimal” reasoning policy, or still with a bit of temperature if we want it to occasionally produce variations. The final policy should ideally solve tasks with minimal but sufficient steps – effectively an improved CoD.If time permits, we may also experiment with a value-based RL approach as a comparison, such as Q-learning or an Actor-Critic without PPO’s specific formulation. However, those are less common for text generation. Another variant could be Expert Iteration, where we use a reference (like the CoT solution) as expert guidance. The paper by Havrilla et al. (2024) suggested Expert Iteration performed well in reasoning tasks. In our context, an “expert” could be a high-quality CoT or a self-consistency result. This is an optional extension: we could guide the model with an expert policy initially and then refine with PPO. The main implementation, however, will focus on PPO with on-policy exploration as described.In summary, the RL implementation will transform the LLM’s reasoning process into a trainable policy, using diverse sampled drafts as the exploration data. The expectation is that this yields a policy that generates concise, accurate reasoning by design.
Analysis and Expected Outcomes
Comparative Performance
After running the experiments, we will compile results comparing all prompting strategies (Standard, CoT, CoD, and our Diverse CoD + RL) on each task and model. We anticipate clear differences in both accuracy and efficiency. Below is an example of the kind of result we expect (for illustrative purposes, consider GPT-4 on the GSM8K math task):
Method	Accuracy (GSM8K)	Avg. Output Tokens
Standard (Direct Answer)	53.3%	1.1
Chain-of-Thought (CoT)	95.4%	205.1
Chain-of-Draft (CoD)	91.1%	43.9
Diverse CoD + RL (Ours)	≈95% (expected)	≈50 (expected)
Table 1: Baseline vs. expected performance on a math benchmark (GPT-4, GSM8K). Baseline figures from Xu et al. (2025). Our method is projected to retain CoD’s efficiency (very low token count) while closing the accuracy gap to CoT.Across tasks, we expect our Diverse CoD + RL approach to achieve the highest accuracy or close to it. In math problems (like GSM8K), CoT is very strong; our aim is to match that ~95% accuracy without the huge verbosity. It’s plausible our method even exceeds CoT slightly if the model learns to avoid certain pitfalls via exploration (similar to how self-consistency found correct answers more often than a single CoT chain). Commonsense tasks (like Big-Bench questions) often have CoT and CoD already near perfect accuracy (some were 100%). In those cases, we expect our method to also hit ~100% or no worse than baseline, but with no extra cost. For symbolic puzzles, if CoT struggled, our method might improve more noticeably by trying multiple reasoning angles. Coding tasks are interesting: CoT sometimes helps with coding, but can also introduce irrelevant commentary. We anticipate our concise reasoning approach might guide the model to write correct code more consistently (possibly boosting pass@1 on HumanEval by a few points compared to no reasoning or even verbose reasoning).In terms of token usage, our method should be very efficient. Each reasoning chain it produces is short (like CoD). The only potential overhead is if we were to use multiple samples at inference to ensure correctness (like an ensemble). However, one goal of RL training is to bake the self-consistency into the model, so that a single sample is usually enough. Thus, we expect the average tokens per answer to be closer to CoD than to CoT. For example, maybe ~50 tokens on GSM8K (versus 205 for CoT, a 75% reduction), and similarly large savings on other tasks. In prior work, CoD cut token usage by 68–92% while maintaining accuracy; we aim to inherit that benefit. This translates to faster response times – likely on the order of 1 second or less per query for our method on GPT-4, compared to 4+ seconds for CoT (as seen in Table 1 and other results). We will present charts for each task showing the accuracy vs. token count trade-off. We expect our method’s point on those charts to be an upper-right position (high accuracy, low tokens), essentially dominating the other strategies.Another expected outcome is an improvement in reasoning robustness. By robustness, we mean the model’s ability to get the answer correct even if one particular approach doesn’t work, because it has considered alternatives. We might observe that certain tricky questions which CoT got wrong (perhaps due to a single line of reasoning that went astray) could be solved by our method, because during RL training the model might have explored a different angle that succeeds. This could manifest as a higher accuracy particularly on the more challenging subset of questions. We will identify if there are specific problem types where our method has a significant edge. For instance, if a question has a trap or requires a creative insight, a single CoT chain might miss it, but multiple drafts increase the chance of hitting the insight.To ensure these differences are meaningful, we will conduct statistical significance tests. For accuracy improvements, since each model’s answer is either correct or not per question, we can use a McNemar’s test between our method and a baseline (treating each question as a paired trial) to see if one method significantly outperforms the other. Alternatively, we can perform bootstrap resampling of the test set results to get confidence intervals for accuracy differences. We anticipate that on large test sets (like GSM8K with 1319 questions), even a few percentage points improvement (e.g., 91% to 95%) is statistically significant (p < 0.01). We will report such significance where applicable (e.g., “our method outperforms standard CoD with p = 0.005 on GSM8K”). For token counts, differences are so large (multiple times fewer tokens) that significance is less of a question; instead, it’s about the practical impact. We’ll still compute, for example, the standard deviation of token counts and use t-tests to confirm that reductions are consistent.We will also analyze the reasoning diversity in the outputs. We expect to find that the reasoning traces produced by our RL-trained model are not identical each time (if we sample it multiple times), yet all lead to correct answers. This can be seen as the model having a rich repertoire of problem-solving methods. We might quantify that, say, on a particular question, the model can solve it via two distinct short drafts. In contrast, CoT would have essentially one verbose method (or if you sampled CoT, many would be similar). We’ll report the diversity metric values: e.g., “Under our strategy, the average self-consistency score (entropy-based) was X, compared to near 0 for deterministic CoT.” If possible, we will include a small table of diversity metrics or a figure illustrating different reasoning paths for a single example. An example analysis might be: for a question about a trick riddle, CoT might follow the obvious but wrong assumption (leading to a wrong answer), whereas our method tried three different interpretations (drafts) during training and discovered the correct interpretation, which it then uses at test time.
Sample Case Analysis
To illustrate the qualitative differences, consider a sample problem and how each method handles it:
Problem: “Alice and Bob each have some candies. Alice says: if Bob gives me 3 candies, we’ll have the same number. Bob says: if Alice gives me 3 candies, I’ll have double what she has left. How many candies do Alice and Bob have?”
Standard: The model often guesses, yielding an incorrect answer due to no reasoning.
CoT: The model writes a long explanation: defines variables, sets up equations step by step, solves them, and arrives at the correct answer (say, Alice has 9, Bob 3). It uses ~150 tokens explaining each detail (like “Alice has A, Bob has B. Alice’s statement: B-3 = A+3…” etc.). Correct but verbose.
CoD: The model writes a minimal draft of reasoning, for example: “Let A, B be counts. Equation1: B-3 = A+3. Equation2: A-3 = (B+3)/2. Solve: A=9, B=3.” (plus final answer). This uses perhaps 20 tokens. It’s concise and correct​
TECHXPLORE.COM
. If the model is well prompted, it will drop any redundant words. Suppose CoD gets it right in this case.
Our Method (Diverse CoD+RL): After training, the model might solve it similarly concisely. But what if initially it was confused? During RL training, for such a puzzle, it might have tried an alternate path like trial and error: “Try values… Not equal, adjust…” which maybe failed (reward 0), and the equation method which succeeded (reward 1). The model learns the equation method is reliable. So at test time it does something like: “Set up eq: B-3 = A+3; A-3 = (B+3)/2 ⇒ A=9,B=3. Answer: Alice 9, Bob 3.” – also ~20 tokens. In essence, it converges to the best concise method. If there was a trick, the diversity ensured the model considered it during training.
This kind of case study will be included in the analysis to show how the reasoning differs. We expect to see that our method’s answers are accompanied by just a few crucial reasoning steps that are often the more direct or clever steps, as opposed to CoT which lists every single thought (some of which might be unnecessary).
Future Implications
If our hypothesis is confirmed, the findings would have notable implications for future LLM optimization:
Enhanced Training Paradigms: The success would indicate that prompting strategies (like CoD) can be effectively integrated into the training of the model, not just the prompting at inference. It showcases a path to train a model to think more efficiently. This blurs the line between prompting and model capability – the model internalizes a reasoning strategy via RL. Future models might be trained with similar techniques to embed sophisticated reasoning heuristics (like diverse exploration, self-checking, etc.) directly into the model weights. This could reduce reliance on complicated prompting at runtime.
Efficiency Without Loss of Accuracy: A major takeaway would be that we can have the best of both worlds: reasoning accuracy and speed. Previously, one had to choose between fast but shallow reasoning (direct answer) and slow but accurate reasoning (CoT). Our approach could set a new standard where models solve complex tasks quickly. This can enable deployment of advanced reasoning models in real-time applications (like assistants on mobile devices or time-sensitive decision systems) where long latencies are unacceptable. Also, fewer tokens mean lower API costs and memory usage, which is economically and technologically beneficial.
Applicability to Smaller Models: If the experiment shows that even a 13B parameter model improved significantly with this method, it suggests a way to make less-capable models more competitive. This is important for open-source and private models that can’t match GPT-4 in raw scale. Through training on diverse reasoning, a smaller model might punch above its weight on complex tasks. Future research could explore scaling laws: e.g., does a 13B model trained with this method match a 30B model with standard training? If so, that’s a huge win in efficiency.
Integration with RLHF and Alignment: Our method used a straightforward automated reward (accuracy). It could be combined with human feedback rewards to ensure the reasoning is not only correct but also aligned with human preferences (e.g., not producing reasoning that is offensive or not understandable). One could imagine first using our method to achieve high task performance, then applying RLHF to fine-tune the style or safety. Since our method already shortens outputs, it might also reduce the chance of the model generating undesired content in long explanations. Future work can examine if diverse CoD + RL is complementary with RLHF pipelines or if any conflicts arise.
Generalized Diversity Strategies: While we applied diversity at the token level in reasoning, this concept might extend to other areas. For example, Tree-of-Thoughts is a strategy where the model explores a tree of reasoning. Our results could inspire training a model to do tree search internally. Similarly, for multi-modal models, one could train with diverse chains of visual reasoning (for image understanding tasks). Essentially, encouraging exploration and then optimizing the model’s policy could be a general recipe for boosting model “cognitive” abilities.
Understanding and Controllability: An interesting side effect is that by having the model produce drafts, we maintain some transparency into its process (like CoT). If the model is making a mistake, we could inspect the drafts. Moreover, because the model learned to be concise, those drafts are easier to analyze than a long rambling CoT. This could help in debugging model decisions or in fields like AI safety where understanding the model’s chain of thought is important. Future researchers might prefer such RL-trained concise reasoners as they are easier to interpret step-by-step.
We will include in the report a discussion section elaborating on these points, using our data to back them. For instance, if smaller models benefited greatly, we’ll highlight that as evidence that this training approach is a promising equalizer. If certain tasks still required more verbosity, we’ll note that perhaps some problems inherently need more explanation (and thus future work might allow a mix of strategies). We will also mention any limitations observed: e.g., did the model ever exploit the reward in weird ways (like giving correct answers with no reasoning but not generalizing well)? Did diversity ever confuse the model or lead to inconsistent outputs? Addressing such questions will nuance the conclusions.In conclusion, we expect the experiment to demonstrate a viable method to train LLMs to “think” more like humans: exploring various concise ideas and learning from them. The final report will present evidence that diverse token sampling in CoD, combined with RL optimization, can significantly improve reasoning accuracy while keeping responses efficient. We will provide clear tables of results, graphs of the accuracy-token trade-offs, and statistical validations to support these claims. The success of this approach would mark a step toward more intelligent and cost-effective AI systems, and open up new avenues for research into hybrid prompting-training strategies.