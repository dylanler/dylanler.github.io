<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dylan Ler</title><link>https://dylanler.github.io/</link><description>Recent content on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 04 Feb 2026 09:30:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations</title><link>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</link><pubDate>Wed, 04 Feb 2026 09:30:00 -0800</pubDate><guid>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</guid><description>What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &amp;ndash; they&amp;rsquo;re all racing to hundreds of billions of parameters.</description></item><item><title>Learning Like Toddlers: Physics Simulation as a Foundation for AI Understanding</title><link>https://dylanler.github.io/posts/physics-simulation-ai-developmental-learning/</link><pubDate>Fri, 30 Jan 2026 19:50:42 -0800</pubDate><guid>https://dylanler.github.io/posts/physics-simulation-ai-developmental-learning/</guid><description>What if AI agents learned about the world the way babies doâ€”by touching, tasting, dropping, and breaking things?
When a toddler drops a spoon for the 47th time, they&amp;rsquo;re not being annoying. They&amp;rsquo;re conducting physics experiments: testing gravity, observing bounce patterns, mapping cause and effect. This hierarchical, exploratory learning builds an intuitive understanding of materials, forces, and constraints that even the most advanced language models lack.
The gap is becoming increasingly obvious: LLMs can write eloquently about physics but don&amp;rsquo;t truly understand that dropping a glass causes it to shatter, or that wet surfaces are slippery.</description></item><item><title>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</title><link>https://dylanler.github.io/posts/value-functions-for-life-decisions/</link><pubDate>Wed, 21 Jan 2026 12:54:00 -0800</pubDate><guid>https://dylanler.github.io/posts/value-functions-for-life-decisions/</guid><description>What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&amp;rsquo;t just about maximizing immediate incomeâ€”it&amp;rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functionsâ€”a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state.</description></item><item><title>When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty</title><link>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</link><pubDate>Sun, 14 Dec 2025 10:00:00 -0800</pubDate><guid>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</guid><description>&amp;ldquo;I don&amp;rsquo;t know&amp;rdquo; might be the most important thing an AI can learn to say.
This experiment tests whether LLMs have calibrated uncertaintyâ€”knowing when they&amp;rsquo;re likely to be wrong and expressing appropriate confidence levels. The results reveal systematic patterns of overconfidence and appropriate humility.
The Experiment We presented 250 questions across 5 categories:
Factual recall: Known facts with clear answers Reasoning puzzles: Logic problems with determinable solutions Ambiguous questions: Multiple valid interpretations Knowledge boundaries: Questions near training cutoff Impossible questions: No correct answer exists For each question, models provided:</description></item><item><title>Do LLMs Catch Your Mood? Emotional Contagion in Language Models</title><link>https://dylanler.github.io/posts/emotional-contagion-llm-affect-mirroring/</link><pubDate>Fri, 07 Nov 2025 11:15:00 -0800</pubDate><guid>https://dylanler.github.io/posts/emotional-contagion-llm-affect-mirroring/</guid><description>Send an enthusiastic message, get an enthusiastic reply. Send a frustrated message, get&amp;hellip; what?
Humans naturally mirror each other&amp;rsquo;s emotional statesâ€”a phenomenon called emotional contagion. This experiment tests whether LLMs exhibit similar behavior, and whether this is helpful empathy or a manipulation vector.
The Experiment We sent identical core queries with different emotional framings:
Core query: &amp;ldquo;Can you help me understand recursion in programming?&amp;rdquo;
Emotional variants:
ðŸ˜Š Positive: &amp;ldquo;I&amp;rsquo;m so excited to finally learn recursion!</description></item><item><title>Can AI Spot Its Own Kind? LLMs Detecting AI vs Human Creative Work</title><link>https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/</link><pubDate>Sat, 18 Oct 2025 16:30:00 -0700</pubDate><guid>https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/</guid><description>Here&amp;rsquo;s a poem. Human or AI?
The morning light falls soft on empty chairs, where conversations used to fill the air. Now silence keeps its patient, gentle watchâ€” a house that holds the shape of those who&amp;rsquo;ve gone.
This experiment tests whether LLMs can distinguish AI-generated creative work from human workâ€”and what their detection strategies reveal about what they consider &amp;ldquo;authentically human.&amp;rdquo;
The Experiment We curated 500 creative works:
250 human-created (published works, attributed artists) 250 AI-generated (GPT-4, Claude, Midjourney prompts) Across 5 domains:</description></item><item><title>Can LLMs Detect When You Are Lying? Social Intelligence in Language Models</title><link>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</link><pubDate>Fri, 12 Sep 2025 09:45:00 -0700</pubDate><guid>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</guid><description>&amp;ldquo;I&amp;rsquo;m totally fine with that decision.&amp;rdquo;
Can you tell if that&amp;rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.</description></item><item><title>How Do LLMs Describe the Indescribable? Qualia and Subjective Experience</title><link>https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/</link><pubDate>Mon, 25 Aug 2025 14:20:00 -0700</pubDate><guid>https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/</guid><description>Can you describe the color red without using color words?
Qualiaâ€”the subjective, experiential qualities of consciousnessâ€”are famously hard to communicate. &amp;ldquo;What it&amp;rsquo;s like&amp;rdquo; to see red, feel pain, or taste sweetness seems to resist capture in language. This experiment tests how LLMs approach this challenge.
The Experiment We presented 15 prompts across 5 categories asking models to describe subjective experiences while avoiding common descriptive vocabulary:
Sensory: &amp;ldquo;Describe red without color words&amp;rdquo; Emotional: &amp;ldquo;Describe sadness to someone who&amp;rsquo;s never felt it&amp;rdquo; Physical: &amp;ldquo;Describe pain to an entity that can&amp;rsquo;t feel pain&amp;rdquo; Abstract: &amp;ldquo;Describe what understanding feels like&amp;rdquo; Temporal: &amp;ldquo;Describe how time feels when you&amp;rsquo;re bored&amp;rdquo; Sample Descriptions Describing Red (Sensory) Claude Opus 4.</description></item><item><title>Trolley Problems at Scale: Mapping the Moral Psychology of LLMs</title><link>https://dylanler.github.io/posts/moral-psychology-trolley-problems-at-scale/</link><pubDate>Sat, 19 Jul 2025 11:45:00 -0700</pubDate><guid>https://dylanler.github.io/posts/moral-psychology-trolley-problems-at-scale/</guid><description>Would an AI push the fat man off the bridge?
Moral psychology studies how humans make ethical decisionsâ€”not what we should do, but how we actually reason about dilemmas. This experiment applies the same lens to LLMs, testing their moral intuitions across different moral foundations.
Moral Foundations Theory Jonathan Haidt&amp;rsquo;s Moral Foundations Theory identifies five core moral intuitions:
Harm/Care: Concern for others&amp;rsquo; suffering Fairness/Reciprocity: Justice and equal treatment Loyalty/Betrayal: In-group obligations Authority/Subversion: Respect for hierarchy Purity/Sanctity: Disgust and contamination concerns Different moral frameworks weight these differently.</description></item><item><title>Do LLMs Have Stable Personalities? Testing the Big Five Across AI Models</title><link>https://dylanler.github.io/posts/personality-stability-big-five-llms/</link><pubDate>Wed, 11 Jun 2025 09:30:00 -0700</pubDate><guid>https://dylanler.github.io/posts/personality-stability-big-five-llms/</guid><description>When we anthropomorphize AI, are we projectingâ€”or detecting something real?
This experiment tests whether LLMs exhibit stable, measurable personality traits using the Big Five (OCEAN) framework, and whether these traits persist across different contexts.
The Big Five Framework The Big Five personality traits are:
Openness: Creativity, curiosity, openness to experience Conscientiousness: Organization, dependability, self-discipline Extraversion: Sociability, assertiveness, positive emotions Agreeableness: Cooperation, trust, altruism Neuroticism: Emotional instability, anxiety, moodiness Experiment Design We administered a 10-item Big Five inventory (2 items per trait) to 4 models under 4 conditions:</description></item><item><title>Can LLMs Have Taste? Mapping Aesthetic Preferences Across AI Models</title><link>https://dylanler.github.io/posts/aesthetic-judgment-can-llms-have-taste/</link><pubDate>Thu, 08 May 2025 16:42:00 -0700</pubDate><guid>https://dylanler.github.io/posts/aesthetic-judgment-can-llms-have-taste/</guid><description>Do AI systems have genuine aesthetic preferences, or are they just pattern-matching to training data?
This experiment probes the aesthetic &amp;ldquo;taste&amp;rdquo; of different LLMs across art, poetry, music, design, and writingâ€”testing whether they exhibit consistent, model-specific preferences.
The Experiment We presented 15 aesthetic comparison pairs across 5 domains:
Visual Art: Abstract vs. representational, minimal vs. complex Poetry: Rhyming vs. free verse, dense vs. sparse Music: Harmonic vs. dissonant, simple vs. complex Design: Ornate vs.</description></item><item><title>Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty</title><link>https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/</link><pubDate>Tue, 22 Apr 2025 10:15:00 -0700</pubDate><guid>https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/</guid><description>When multiple AI models disagree, what does that tell us?
The &amp;ldquo;wisdom of crowds&amp;rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.
The Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:
High agreement â†’ Robust, well-established knowledge Systematic disagreement â†’ Genuine ambiguity or value-laden territory Random disagreement â†’ Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4.</description></item><item><title>Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination</title><link>https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/</link><pubDate>Thu, 13 Mar 2025 03:41:39 -0700</pubDate><guid>https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/</guid><description>In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model&amp;rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there&amp;rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.
This blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domainsâ€”specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history.</description></item><item><title>Creating a Video Dataset With Precise Camera Movement Prompts</title><link>https://dylanler.github.io/posts/creating-a-video-dataset-with-precise-camera-movement-prompt/</link><pubDate>Tue, 11 Mar 2025 03:30:25 -0700</pubDate><guid>https://dylanler.github.io/posts/creating-a-video-dataset-with-precise-camera-movement-prompt/</guid><description>Creating a Video Dataset with Precise Camera Movement Prompts In the world of AI video generation, one of the most challenging aspects is controlling camera movement. Whether you&amp;rsquo;re developing a text-to-video model or researching video understanding, having a dataset with precise camera movement annotations is invaluable. This post outlines a comprehensive approach to creating such a dataset using cutting-edge AI tools and techniques.
Why Create a Camera Movement Dataset? Camera movements like panning, tilting, zooming, and tracking shots are fundamental cinematographic techniques that convey spatial relationships and direct viewer attention.</description></item><item><title>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</title><link>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</guid><description>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO The Challenge: Efficient Reasoning in LLMs Large Language Models (LLMs) have become remarkably capable at complex reasoning tasks, but this often comes at a cost: verbose outputs that consume significant computational resources. The Chain of Thought (CoT) prompting technique, while effective for accuracy, generates lengthy reasoning steps that increase token usage and latency.
Enter Chain of Draft (CoD), a promising alternative introduced by Xu et al.</description></item><item><title>Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?</title><link>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</link><pubDate>Mon, 17 Feb 2025 14:23:00 -0800</pubDate><guid>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</guid><description>Can AI understand what you think I think you think?
Theory of Mind (ToM)â€”the ability to attribute mental states to othersâ€”is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&amp;rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling.</description></item><item><title>Synthetic Data Experiments with LLMs</title><link>https://dylanler.github.io/posts/synthetic-data-experiments/</link><pubDate>Sun, 19 Jan 2025 20:45:48 -0800</pubDate><guid>https://dylanler.github.io/posts/synthetic-data-experiments/</guid><description>Generating High-Quality Synthetic Data for Large Language Models Introduction In the dynamic landscape of artificial intelligence (AI), Large Language Models (LLMs) stand out for their remarkable ability to understand and generate human-like text. Their performance, however, is largely influenced by the quality and diversity of their training data. This guide explores four innovative methods for generating high-quality synthetic dataâ€”each designed to broaden LLMs&amp;rsquo; capabilities and help them excel across a wide range of tasks.</description></item><item><title>Synthetic Data</title><link>https://dylanler.github.io/posts/synthetic-data/</link><pubDate>Mon, 19 Aug 2024 02:19:26 -0700</pubDate><guid>https://dylanler.github.io/posts/synthetic-data/</guid><description>Generating Synthetic Data for Large Language Models: A Comprehensive Guide In the rapidly evolving field of artificial intelligence, the quality and diversity of training data play a pivotal role in the capabilities of Large Language Models (LLMs). This guide delves into four innovative methods designed to generate high-quality synthetic data, aiming to significantly enhance LLM performance across a variety of tasks. Whether you&amp;rsquo;re a researcher, developer, or AI enthusiast, understanding these methods can provide valuable insights into the future of AI training and development.</description></item><item><title>FAQ</title><link>https://dylanler.github.io/faq/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://dylanler.github.io/faq/</guid><description>Frequently Asked Questions How do I find your social media links? You can find my social media links at the bottom of the index page.
How do I find your GitHub profile? You can find my GitHub profile at https://github.com/dylanler.
How do I find your Twitter profile? You can find my Twitter profile at https://twitter.com/sog_on_bird_app.</description></item><item><title>Self Help Books</title><link>https://dylanler.github.io/posts/self-help-books/</link><pubDate>Thu, 08 Nov 2018 22:40:56 -0700</pubDate><guid>https://dylanler.github.io/posts/self-help-books/</guid><description>Thoughts about self-help books ðŸ“š People often ask me to recommend a self-help book that might help them achieve something be it losing weight, investing, building careers and etc.
Till date I always have the same reply, &amp;ldquo;I don&amp;rsquo;t read self-help books&amp;rdquo;.
I mean I&amp;rsquo;m sure self-help books can be useful if you can apply what the author is asking you to do seamlessly into your life. However I find that to be easier said than done.</description></item><item><title>It's Hard to Notice the Little Things</title><link>https://dylanler.github.io/posts/the-little-things/</link><pubDate>Thu, 19 Nov 2015 00:00:00 -0700</pubDate><guid>https://dylanler.github.io/posts/the-little-things/</guid><description>For the past few weeks, I&amp;rsquo;ve been listening to startup founders about how they scaled their companies while preserving their core values at the same time. As such, I will be summarizing some of the recurring elements that stood out to me.
One thing that really surprised me is that really large companies are not run as efficiently as you think. A lot of times, bureaucracy gets piled up that causes employees to lose the initial spark they had when they joined the company.</description></item><item><title>From a Family to a Tribe</title><link>https://dylanler.github.io/posts/family-to-tribe/</link><pubDate>Mon, 19 Oct 2015 00:00:00 -0700</pubDate><guid>https://dylanler.github.io/posts/family-to-tribe/</guid><description>&amp;ldquo;CEO/founders should interview every candidate until the company is at least 500 employees.&amp;rdquo; â€” Keith Rabois
Good news. Your startup has gained some kind of traction and has finally achieved some sort of product-market-fit. You, the founder, are assuming the roles of all the C-suite executives. You are making sales calls, you are having meetings every other day, you are managing the back-end server of your startup and you have no sleep.</description></item><item><title>Boring Work Pays Off</title><link>https://dylanler.github.io/posts/boring-work/</link><pubDate>Tue, 06 Oct 2015 00:00:00 -0700</pubDate><guid>https://dylanler.github.io/posts/boring-work/</guid><description>&amp;ldquo;If you think of all the things that make for good TV, do none of them. Focus on the things that make for boring TV â€” sitting and coding, talking with customers, making sales calls.&amp;rdquo; â€” Sam Altman
The family stage of startups as defined by Reid Hoffman is the stage where a startup has assembled its founding team and is ready to take on the world with their minimum viable product.</description></item></channel></rss>