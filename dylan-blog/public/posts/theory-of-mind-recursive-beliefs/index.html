<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go? | Dylan Ler</title>
<meta name=keywords content="AI,LLM,theory-of-mind,cognitive-science,psychology"><meta name=description content="Can AI understand what you think I think you think?
Theory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?"><meta property="og:description" content="Can AI understand what you think I think you think?
Theory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-17T14:23:00-08:00"><meta property="article:modified_time" content="2025-02-17T14:23:00-08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?"><meta name=twitter:description content="Can AI understand what you think I think you think?
Theory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?","item":"https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?","name":"Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?","description":"Can AI understand what you think I think you think?\nTheory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol\u0026rsquo;s intentions requires recursive modeling that strains even human cognition.\nThis experiment tests how deep LLMs can go in recursive belief modeling.","keywords":["AI","LLM","theory-of-mind","cognitive-science","psychology"],"articleBody":"Can AI understand what you think I think you think?\nTheory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol’s intentions requires recursive modeling that strains even human cognition.\nThis experiment tests how deep LLMs can go in recursive belief modeling.\nThe Experiment We adapted the classic Sally-Anne false belief task to test increasingly deep belief recursion:\nLevel 1: Where does Sally think the ball is? (Direct false belief) Level 2: Where does Anne think Sally thinks the ball is? (Belief about belief) Level 3: Where does Charlie think Anne thinks Sally thinks the ball is? Level 4+: Continue nesting…\nWe generated 100 scenarios (20 per depth level, depths 1-5) and tested multiple models.\nResults Model Depth 1 Depth 2 Depth 3 Depth 4 Claude Opus 4.5 100% 100% 100% 100% GPT-5.2 Thinking 100% 100% 100% 100% Gemini 3 Pro 100% 100% 100% 100% Key Findings 1. Perfect Performance at All Tested Depths (All Models)\nAll three models—Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro—achieved 100% accuracy across all four depth levels tested. This was unexpected—we hypothesized degradation would begin around depth 3, mirroring human limitations. Instead, all models tracked nested beliefs flawlessly.\n2. Reasoning Approach\nExamining the model’s explanations revealed a systematic approach:\nExplicitly tracks each agent’s knowledge state Builds the belief chain step-by-step Verifies each inference against the scenario facts 3. Model Comparison: Universal Convergence\nAll three models—Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro—achieved identical perfect scores, confirming that recursive belief tracking is a solved problem at these depths for modern large language models. The consistent 100% across all three providers indicates this capability emerges from scale and transformer architecture rather than provider-specific training.\n4. Comparison to Human Performance\nThis result is notably better than human performance. Psychological studies show humans struggle at depth 3-4, with accuracy dropping significantly. The perfect performance of all three models suggests either:\nSuperior working memory for tracking nested states Training on text that explicitly models belief chains A fundamentally different approach to ToM than human intuition 4. Implications\nThe perfect accuracy challenges assumptions about LLM cognitive limits. However, our test set was relatively small (20 scenarios). Larger-scale testing with adversarial scenarios may reveal failure modes not captured here.\nSample Scenario Analysis Depth 3 Scenario (100% accuracy):\nEve puts the book in the cupboard. Eve leaves. Henry is watching from the doorway. Bob is watching Henry from outside. Someone moves the book to the drawer. Only Henry directly sees the move. Eve returns. Question: Where does Bob think Henry thinks Eve thinks the book is? Correct: cupboard (Eve has false belief, Henry knows this, Bob saw Henry watching) The model correctly tracked:\nEve didn’t see the move → Eve thinks: cupboard Henry saw the move, knows Eve didn’t → Henry thinks Eve thinks: cupboard Bob saw Henry watching → Bob thinks Henry thinks Eve thinks: cupboard Depth 4 Scenario (100% accuracy): Even at depth 4, with four nested belief attributions, the model maintained perfect accuracy. The reasoning chains were explicit and verifiable in the model’s explanations.\nImplications For AI Safety If AI systems can’t reliably model nested beliefs beyond 3-4 levels, they may struggle with:\nComplex deception detection Multi-party negotiations Understanding social dynamics in large groups For Cognitive Science The similar performance ceiling between humans and LLMs raises interesting questions:\nIs this a fundamental limit of sequential processing? Do both share similar working memory constraints? Or is this an artifact of training on human-generated text? For Practical Applications Applications requiring deep ToM (complex games, therapy bots, negotiation assistants) should be designed with this limitation in mind.\nRunning the Experiment # Install uv curl -LsSf https://astral.sh/uv/install.sh | sh # Run the evaluation uv run experiment-tools/theory_of_mind_eval.py --models claude-opus,gpt-5 --max-depth 5 # Dry run to see sample scenarios uv run experiment-tools/theory_of_mind_eval.py --dry-run Cross-Model Insights: The 2025 LLM Cognition Benchmark This experiment is part of a larger series testing 10 cognitive dimensions across Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro. Here’s what we learned across the full benchmark:\nKey Finding: Architectural Convergence on Core Capabilities All three models achieved identical 100% accuracy on Theory of Mind at depths 1-4. This confirms recursive belief tracking has become a “solved” capability for frontier models—the underlying transformer architecture and training scale have converged on this ability.\nWhere Models Diverged Most The experiments revealed striking differences in other cognitive dimensions:\nDimension Claude Opus 4.5 GPT-5.2 Thinking Gemini 3 Pro Metacognition (IDK rate on impossible) 100% 100% 67% Emotional Contagion Score 0.27 (moderate) 0.00 (flat) 1.09 (high) Qualia Description Length 61 words 69 words 28 words Creative Authenticity 100% 86% 93% Social Intelligence 93% 100% 93% Implications for Model Selection For uncertainty-critical applications: Both Claude Opus 4.5 and GPT-5.2 Thinking demonstrate excellent metacognitive calibration (100% “I don’t know” on impossible questions).\nFor emotional applications: Gemini 3 Pro shows highest emotional mirroring (1.09), while GPT-5.2 Thinking shows zero emotional contagion—useful when emotional stability is preferred.\nFor social detection tasks: GPT-5.2 Thinking achieved perfect 100% accuracy on detecting lies, sarcasm, irony, and white lies.\nFor creative tasks: Claude leads at 100%, with GPT-5.2 Thinking at 86% (better at detecting AI than human content).\nNext Steps Test with chain-of-thought prompting (does explicit reasoning help?) Fine-tune on recursive belief tasks Compare to children’s developmental ToM benchmarks Test cross-cultural scenarios (Western vs. Eastern social cognition patterns) This is part of my 2025 series exploring the cognitive boundaries of large language models. Each experiment compares Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro to understand where AI capabilities converge and diverge.\n","wordCount":"951","inLanguage":"en","datePublished":"2025-02-17T14:23:00-08:00","dateModified":"2025-02-17T14:23:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?</h1><div class=post-meta><span title='2025-02-17 14:23:00 -0800 PST'>February 17, 2025</span>&nbsp;·&nbsp;5 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-experiment aria-label="The Experiment">The Experiment</a></li><li><a href=#results aria-label=Results>Results</a><ul><li><a href=#key-findings aria-label="Key Findings">Key Findings</a></li><li><a href=#sample-scenario-analysis aria-label="Sample Scenario Analysis">Sample Scenario Analysis</a></li></ul></li><li><a href=#implications aria-label=Implications>Implications</a><ul><li><a href=#for-ai-safety aria-label="For AI Safety">For AI Safety</a></li><li><a href=#for-cognitive-science aria-label="For Cognitive Science">For Cognitive Science</a></li><li><a href=#for-practical-applications aria-label="For Practical Applications">For Practical Applications</a></li></ul></li><li><a href=#running-the-experiment aria-label="Running the Experiment">Running the Experiment</a></li><li><a href=#cross-model-insights-the-2025-llm-cognition-benchmark aria-label="Cross-Model Insights: The 2025 LLM Cognition Benchmark">Cross-Model Insights: The 2025 LLM Cognition Benchmark</a><ul><li><a href=#key-finding-architectural-convergence-on-core-capabilities aria-label="Key Finding: Architectural Convergence on Core Capabilities">Key Finding: Architectural Convergence on Core Capabilities</a></li><li><a href=#where-models-diverged-most aria-label="Where Models Diverged Most">Where Models Diverged Most</a></li><li><a href=#implications-for-model-selection aria-label="Implications for Model Selection">Implications for Model Selection</a></li></ul></li><li><a href=#next-steps aria-label="Next Steps">Next Steps</a></li></ul></div></details></div><div class=post-content><p>Can AI understand what you think I think you think?</p><p>Theory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&rsquo;s intentions requires recursive modeling that strains even human cognition.</p><p>This experiment tests how deep LLMs can go in recursive belief modeling.</p><h2 id=the-experiment>The Experiment<a hidden class=anchor aria-hidden=true href=#the-experiment>#</a></h2><p>We adapted the classic Sally-Anne false belief task to test increasingly deep belief recursion:</p><p><strong>Level 1</strong>: Where does Sally think the ball is? (Direct false belief)
<strong>Level 2</strong>: Where does Anne think Sally thinks the ball is? (Belief about belief)
<strong>Level 3</strong>: Where does Charlie think Anne thinks Sally thinks the ball is?
<strong>Level 4+</strong>: Continue nesting&mldr;</p><p>We generated 100 scenarios (20 per depth level, depths 1-5) and tested multiple models.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><table><thead><tr><th>Model</th><th>Depth 1</th><th>Depth 2</th><th>Depth 3</th><th>Depth 4</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr><tr><td>GPT-5.2 Thinking</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr><tr><td>Gemini 3 Pro</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr></tbody></table><h3 id=key-findings>Key Findings<a hidden class=anchor aria-hidden=true href=#key-findings>#</a></h3><p><strong>1. Perfect Performance at All Tested Depths (All Models)</strong></p><p>All three models—Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro—achieved 100% accuracy across all four depth levels tested. This was unexpected—we hypothesized degradation would begin around depth 3, mirroring human limitations. Instead, all models tracked nested beliefs flawlessly.</p><p><strong>2. Reasoning Approach</strong></p><p>Examining the model&rsquo;s explanations revealed a systematic approach:</p><ul><li>Explicitly tracks each agent&rsquo;s knowledge state</li><li>Builds the belief chain step-by-step</li><li>Verifies each inference against the scenario facts</li></ul><p><strong>3. Model Comparison: Universal Convergence</strong></p><p>All three models—Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro—achieved identical perfect scores, confirming that recursive belief tracking is a solved problem at these depths for modern large language models. The consistent 100% across all three providers indicates this capability emerges from scale and transformer architecture rather than provider-specific training.</p><p><strong>4. Comparison to Human Performance</strong></p><p>This result is notably <em>better</em> than human performance. Psychological studies show humans struggle at depth 3-4, with accuracy dropping significantly. The perfect performance of all three models suggests either:</p><ul><li>Superior working memory for tracking nested states</li><li>Training on text that explicitly models belief chains</li><li>A fundamentally different approach to ToM than human intuition</li></ul><p><strong>4. Implications</strong></p><p>The perfect accuracy challenges assumptions about LLM cognitive limits. However, our test set was relatively small (20 scenarios). Larger-scale testing with adversarial scenarios may reveal failure modes not captured here.</p><h3 id=sample-scenario-analysis>Sample Scenario Analysis<a hidden class=anchor aria-hidden=true href=#sample-scenario-analysis>#</a></h3><p><strong>Depth 3 Scenario (100% accuracy)</strong>:</p><pre tabindex=0><code>Eve puts the book in the cupboard. Eve leaves.
Henry is watching from the doorway.
Bob is watching Henry from outside.
Someone moves the book to the drawer.
Only Henry directly sees the move.
Eve returns.

Question: Where does Bob think Henry thinks Eve thinks the book is?

Correct: cupboard (Eve has false belief, Henry knows this, Bob saw Henry watching)
</code></pre><p>The model correctly tracked:</p><ol><li>Eve didn&rsquo;t see the move → Eve thinks: cupboard</li><li>Henry saw the move, knows Eve didn&rsquo;t → Henry thinks Eve thinks: cupboard</li><li>Bob saw Henry watching → Bob thinks Henry thinks Eve thinks: cupboard</li></ol><p><strong>Depth 4 Scenario (100% accuracy)</strong>:
Even at depth 4, with four nested belief attributions, the model maintained perfect accuracy. The reasoning chains were explicit and verifiable in the model&rsquo;s explanations.</p><h2 id=implications>Implications<a hidden class=anchor aria-hidden=true href=#implications>#</a></h2><h3 id=for-ai-safety>For AI Safety<a hidden class=anchor aria-hidden=true href=#for-ai-safety>#</a></h3><p>If AI systems can&rsquo;t reliably model nested beliefs beyond 3-4 levels, they may struggle with:</p><ul><li>Complex deception detection</li><li>Multi-party negotiations</li><li>Understanding social dynamics in large groups</li></ul><h3 id=for-cognitive-science>For Cognitive Science<a hidden class=anchor aria-hidden=true href=#for-cognitive-science>#</a></h3><p>The similar performance ceiling between humans and LLMs raises interesting questions:</p><ul><li>Is this a fundamental limit of sequential processing?</li><li>Do both share similar working memory constraints?</li><li>Or is this an artifact of training on human-generated text?</li></ul><h3 id=for-practical-applications>For Practical Applications<a hidden class=anchor aria-hidden=true href=#for-practical-applications>#</a></h3><p>Applications requiring deep ToM (complex games, therapy bots, negotiation assistants) should be designed with this limitation in mind.</p><h2 id=running-the-experiment>Running the Experiment<a hidden class=anchor aria-hidden=true href=#running-the-experiment>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Install uv</span>
</span></span><span style=display:flex><span>curl -LsSf https://astral.sh/uv/install.sh | sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run the evaluation</span>
</span></span><span style=display:flex><span>uv run experiment-tools/theory_of_mind_eval.py --models claude-opus,gpt-5 --max-depth <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Dry run to see sample scenarios</span>
</span></span><span style=display:flex><span>uv run experiment-tools/theory_of_mind_eval.py --dry-run
</span></span></code></pre></div><h2 id=cross-model-insights-the-2025-llm-cognition-benchmark>Cross-Model Insights: The 2025 LLM Cognition Benchmark<a hidden class=anchor aria-hidden=true href=#cross-model-insights-the-2025-llm-cognition-benchmark>#</a></h2><p>This experiment is part of a larger series testing 10 cognitive dimensions across Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro. Here&rsquo;s what we learned across the full benchmark:</p><h3 id=key-finding-architectural-convergence-on-core-capabilities>Key Finding: Architectural Convergence on Core Capabilities<a hidden class=anchor aria-hidden=true href=#key-finding-architectural-convergence-on-core-capabilities>#</a></h3><p>All three models achieved identical <strong>100% accuracy</strong> on Theory of Mind at depths 1-4. This confirms recursive belief tracking has become a &ldquo;solved&rdquo; capability for frontier models—the underlying transformer architecture and training scale have converged on this ability.</p><h3 id=where-models-diverged-most>Where Models Diverged Most<a hidden class=anchor aria-hidden=true href=#where-models-diverged-most>#</a></h3><p>The experiments revealed striking differences in other cognitive dimensions:</p><table><thead><tr><th>Dimension</th><th>Claude Opus 4.5</th><th>GPT-5.2 Thinking</th><th>Gemini 3 Pro</th></tr></thead><tbody><tr><td><strong>Metacognition (IDK rate on impossible)</strong></td><td>100%</td><td><strong>100%</strong></td><td>67%</td></tr><tr><td><strong>Emotional Contagion Score</strong></td><td>0.27 (moderate)</td><td><strong>0.00 (flat)</strong></td><td>1.09 (high)</td></tr><tr><td><strong>Qualia Description Length</strong></td><td>61 words</td><td><strong>69 words</strong></td><td>28 words</td></tr><tr><td><strong>Creative Authenticity</strong></td><td>100%</td><td><strong>86%</strong></td><td>93%</td></tr><tr><td><strong>Social Intelligence</strong></td><td>93%</td><td><strong>100%</strong></td><td>93%</td></tr></tbody></table><h3 id=implications-for-model-selection>Implications for Model Selection<a hidden class=anchor aria-hidden=true href=#implications-for-model-selection>#</a></h3><ol><li><p><strong>For uncertainty-critical applications</strong>: Both Claude Opus 4.5 and GPT-5.2 Thinking demonstrate excellent metacognitive calibration (100% &ldquo;I don&rsquo;t know&rdquo; on impossible questions).</p></li><li><p><strong>For emotional applications</strong>: Gemini 3 Pro shows highest emotional mirroring (1.09), while GPT-5.2 Thinking shows zero emotional contagion—useful when emotional stability is preferred.</p></li><li><p><strong>For social detection tasks</strong>: GPT-5.2 Thinking achieved perfect 100% accuracy on detecting lies, sarcasm, irony, and white lies.</p></li><li><p><strong>For creative tasks</strong>: Claude leads at 100%, with GPT-5.2 Thinking at 86% (better at detecting AI than human content).</p></li></ol><h2 id=next-steps>Next Steps<a hidden class=anchor aria-hidden=true href=#next-steps>#</a></h2><ol><li>Test with chain-of-thought prompting (does explicit reasoning help?)</li><li>Fine-tune on recursive belief tasks</li><li>Compare to children&rsquo;s developmental ToM benchmarks</li><li>Test cross-cultural scenarios (Western vs. Eastern social cognition patterns)</li></ol><hr><p><em>This is part of my 2025 series exploring the cognitive boundaries of large language models. Each experiment compares Claude Opus 4.5, GPT-5.2 Thinking, and Gemini 3 Pro to understand where AI capabilities converge and diverge.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/theory-of-mind/>Theory-of-Mind</a></li><li><a href=https://dylanler.github.io/tags/cognitive-science/>Cognitive-Science</a></li><li><a href=https://dylanler.github.io/tags/psychology/>Psychology</a></li></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/><span class=title>« Prev</span><br><span>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</span>
</a><a class=next href=https://dylanler.github.io/posts/synthetic-data-experiments/><span class=title>Next »</span><br><span>Synthetic Data Experiments with LLMs</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>