<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO | Dylan Ler</title>
<meta name=keywords content="AI,LLM,Chain of Thought,Chain of Draft,Reinforcement Learning"><meta name=description content="A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)"><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO"><meta property="og:description" content="A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)"><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO"><meta name=twitter:description content="A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO","item":"https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO","name":"Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO","description":"A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)","keywords":["AI","LLM","Chain of Thought","Chain of Draft","Reinforcement Learning"],"articleBody":"Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO The Challenge: Efficient Reasoning in LLMs Large Language Models (LLMs) have become remarkably capable at complex reasoning tasks, but this often comes at a cost: verbose outputs that consume significant computational resources. The Chain of Thought (CoT) prompting technique, while effective for accuracy, generates lengthy reasoning steps that increase token usage and latency.\nEnter Chain of Draft (CoD), a promising alternative introduced by Xu et al. (2025) that encourages LLMs to produce concise, minimalistic reasoning steps. CoD has shown impressive results, matching or exceeding CoT accuracy while using as little as 7.6% of the tokens.\nBut could we make this approach even better?\nOur Hypothesis We hypothesize that by introducing semantically diverse token sampling into the CoD process and optimizing it through reinforcement learning (RL), we could create a reasoning system that:\nMaintains the token efficiency of CoD Matches or exceeds the accuracy of CoT Explores multiple reasoning paths to find optimal solutions In other words: Can we make LLMs think both broadly (exploring different approaches) and efficiently (through concise drafting)?\nProposed Experimental Design flowchart TD\rA[Problem Statement] --\u003e B[Baseline Methods]\rB --\u003e C1[Standard Prompting]\rB --\u003e C2[Chain of Thought]\rB --\u003e C3[Chain of Draft]\rB --\u003e C4[Our Method: Diverse CoD + GRPO]\rA --\u003e D[Evaluation Tasks]\rD --\u003e E1[Arithmetic Reasoning]\rD --\u003e E2[Commonsense Reasoning]\rD --\u003e E3[Symbolic/Logical Reasoning]\rD --\u003e E4[Coding Tasks]\rA --\u003e F[Models to Test]\rF --\u003e G1[Qwen2.5-0.5B]\rF --\u003e G2[Qwen2.5-1.5B]\rF --\u003e G3[Qwen2.5-7B]\rF --\u003e G4[Qwen2.5-72B]\rA --\u003e H[Metrics]\rH --\u003e I1[Accuracy]\rH --\u003e I2[Token Efficiency]\rH --\u003e I3[Reasoning Diversity]\rH --\u003e I4[Latency] Baseline Methods We plan to compare four different prompting strategies:\nStandard Prompting: Direct answer without explicit reasoning Chain of Thought (CoT): Detailed step-by-step reasoning Chain of Draft (CoD): Concise intermediate reasoning steps Our Method (Diverse CoD + GRPO): Enhanced CoD with diverse token sampling and GRPO optimization Reasoning Tasks To thoroughly evaluate our approach, we’ll test it on diverse reasoning tasks:\nArithmetic Reasoning: GSM8K math word problems Commonsense Reasoning: Date understanding and sports understanding from BIG-Bench Symbolic/Logical Reasoning: Coin-flip puzzles and logical transformations Coding Tasks: HumanEval programming challenges Models to Evaluate We’ll focus our evaluation exclusively on Qwen models to provide a consistent benchmark:\nQwen2.5-0.5B Qwen2.5-1.5B Qwen2.5-7B Qwen2.5-72B The Proposed Approach: Diverse Token Sampling + GRPO The core innovation of our approach combines two key elements:\n1. Semantically Diverse Token Sampling Code Example: Implementing Diverse Token Sampling The following code demonstrates how we implement the token diversity module shown in the diagram above:\ndef generate_diverse_drafts(model, tokenizer, prompt, num_drafts=3, max_tokens=100): \"\"\" Generate multiple diverse reasoning drafts using different sampling strategies. Args: model: The language model tokenizer: The tokenizer for the model prompt: The problem statement num_drafts: Number of diverse drafts to generate max_tokens: Maximum tokens to generate per draft Returns: A list of diverse reasoning drafts \"\"\" drafts = [] # Prepare input inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) # Strategy 1: High Temperature Sampling # This encourages exploration of less likely tokens outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=1.2, # Higher temperature = more randomness top_k=50, repetition_penalty=1.0, pad_token_id=tokenizer.eos_token_id ) draft1 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft1)) # Strategy 2: Nucleus (Top-p) Sampling # This samples from the smallest set of tokens whose cumulative probability exceeds p outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.8, top_p=0.92, # Only consider tokens in the top 92% of probability mass repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id ) draft2 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft2)) # Strategy 3: Repetition Penalty Enforcement # This discourages the model from repeating the same patterns outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.9, top_k=40, top_p=0.95, repetition_penalty=1.5, # Strongly penalize repetition pad_token_id=tokenizer.eos_token_id ) draft3 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft3)) # If more drafts are requested, generate with random combinations of parameters for i in range(3, num_drafts): # Randomly select parameters within reasonable ranges temp = 0.7 + 0.8 * torch.rand(1).item() # Temperature between 0.7 and 1.5 p = 0.85 + 0.14 * torch.rand(1).item() # Top-p between 0.85 and 0.99 rep_penalty = 1.0 + 0.8 * torch.rand(1).item() # Rep penalty between 1.0 and 1.8 outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=temp, top_p=p, repetition_penalty=rep_penalty, pad_token_id=tokenizer.eos_token_id ) draft = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft)) return drafts def enforce_conciseness(draft, max_tokens_per_step=5): \"\"\" Ensure each reasoning step is concise by limiting tokens per line. Args: draft: The generated reasoning draft max_tokens_per_step: Maximum tokens allowed per reasoning step Returns: A concise version of the draft \"\"\" lines = draft.split('\\n') concise_lines = [] for line in lines: line = line.strip() if not line: continue # Tokenize the line (simple whitespace tokenization for illustration) tokens = line.split() # If the line is too long, truncate it if len(tokens) \u003e max_tokens_per_step: tokens = tokens[:max_tokens_per_step] concise_lines.append(' '.join(tokens)) return '\\n'.join(concise_lines) def select_best_draft(drafts, model, tokenizer, problem, reference_answer): \"\"\" Select the best draft based on a combination of correctness and conciseness. This function would typically be replaced by the GRPO reward mechanism during training. For inference, we can use this to select the most promising draft. Args: drafts: List of generated drafts model: The language model tokenizer: The tokenizer problem: The original problem reference_answer: The correct answer (if available) Returns: The best draft based on our heuristics \"\"\" best_score = -float('inf') best_draft = None for draft in drafts: # 1. Check if the draft leads to a correct answer # (In practice, you would use the model to generate an answer from the draft) # 2. Calculate conciseness score lines = [line for line in draft.split('\\n') if line.strip()] total_tokens = sum(len(line.split()) for line in lines) avg_tokens_per_line = total_tokens / max(1, len(lines)) # Lower average tokens per line is better (more concise) conciseness_score = 5 - min(5, avg_tokens_per_line) # 3. Calculate diversity score (simplified) # In practice, you would use embeddings or more sophisticated methods unique_words = set() for line in lines: unique_words.update(line.split()) diversity_score = min(5, len(unique_words) / 5) # 4. Combine scores (weights would be tuned in practice) score = conciseness_score + diversity_score if score \u003e best_score: best_score = score best_draft = draft return best_draft ### 2. Reinforcement Learning with GRPO We'll frame the reasoning task as a sequential decision-making process and use Group Relative Policy Optimization (GRPO) to train the model to maximize a reward function that balances: - **Accuracy**: Correctness of the final answer - **Token Efficiency**: Minimizing the number of tokens used - **Semantic Diversity**: Encouraging varied reasoning approaches The GRPO algorithm works by: 1. Sampling a group of reasoning paths for the same problem 2. Evaluating each path with our reward function 3. Calculating the advantage for each path by comparing its performance to the group average 4. Updating the policy to favor high-reward paths while maintaining KL divergence constraints The proposed reward function is: R = 1.0 (for correct answer) - 0.001 × (number of tokens used)\nThis encourages the model to find the most efficient path to the correct answer while the group comparison mechanism of GRPO reduces variance and leads to more stable training.\r## Implementation Plan\r```mermaid\rsequenceDiagram\rparticipant P as Problem\rparticipant M as Model\rparticipant R as GRPO Environment\rP-\u003e\u003eM: Present problem\rloop Training Episodes\rM-\u003e\u003eM: Generate diverse drafts\rM-\u003e\u003eR: Submit drafts \u0026 answers\rR-\u003e\u003eR: Evaluate correctness\rR-\u003e\u003eR: Calculate reward\rR-\u003e\u003eM: Update policy\rend\rP-\u003e\u003eM: Test problem\rM-\u003e\u003eP: Optimized concise reasoning Initial Setup: We’ll start with a model fine-tuned to follow instructions.\nTraining Process:\nEpisode Generation: The model will generate multiple reasoning drafts for each problem using diverse token sampling. Reward Calculation: We’ll compute rewards based on answer correctness and token usage. Policy Update: Using GRPO, we’ll adjust the model’s parameters to increase the probability of token actions that lead to higher rewards compared to the group average, while maintaining a KL divergence constraint to prevent drastic changes. Group Comparison: GRPO’s group sampling approach naturally balances exploration vs. exploitation by comparing multiple reasoning paths against each other, reducing variance in updates and preventing premature convergence to suboptimal strategies.\nExpected Outcomes Based on prior research on CoD and diverse sampling techniques, we anticipate the following outcomes:\nMethod Expected Accuracy Expected Tokens Standard Prompting 50-60% 1-5 Chain of Thought 90-95% 150-250 Chain of Draft 85-90% 30-60 Diverse CoD + GRPO 90-95% 30-60 Anticipated Findings Accuracy Improvement: We expect our method to achieve accuracy comparable to CoT while maintaining the token efficiency of CoD.\nToken Efficiency: We anticipate using approximately 75% fewer tokens than CoT, resulting in significantly lower latency and computational cost.\nReasoning Diversity: The model should demonstrate the ability to explore multiple reasoning paths, helping it avoid common pitfalls and find more robust solutions.\nGeneralization: We expect the approach to work well across different reasoning tasks and model sizes, with particularly notable improvements for smaller models.\nVisual Representation of Expected Results Method Accuracy (%) Tokens Used Standard Prompting 55 3 Chain of Thought 93 200 Chain of Draft 88 45 Diverse CoD + GRPO 93 45 Example: How Different Methods Might Tackle the Same Problem Problem: “Alice and Bob each have some candies. Alice says: if Bob gives me 3 candies, we’ll have the same number. Bob says: if Alice gives me 3 candies, I’ll have double what she has left. How many candies do Alice and Bob have?”\nStandard (Expected):\n“Alice has 9 candies and Bob has 3 candies.” (Might be correct or incorrect, with no visible reasoning)\nChain of Thought (Expected):\n• Let’s denote Alice’s candies as A and Bob’s candies as B.\n• From Alice’s statement: If Bob gives Alice 3 candies, they’ll have the same number.\n• This means: A + 3 = B - 3\n• Simplifying: A + 6 = B\n• From Bob’s statement: If Alice gives Bob 3 candies, Bob will have double what Alice has left.\n• This means: B + 3 = 2 * (A - 3)\n• Simplifying: B + 3 = 2A - 6\n• B = 2A - 9\n• Now we have two equations:\n• A + 6 = B\n• B = 2A - 9\n• Substituting the second equation into the first:\n• A + 6 = 2A - 9\n• 6 + 9 = 2A - A\n• 15 = A\n• So A = 15, and B = A + 6 = 15 + 6 = 21.\n• Wait, let me double-check:\n• If Bob gives Alice 3 candies: Alice would have 15 + 3 = 18, and Bob would have 21 - 3 = 18. ✓\n• If Alice gives Bob 3 candies: Alice would have 15 - 3 = 12, and Bob would have 21 + 3 = 24. Is this double? 12 * 2 = 24. ✓\n• Therefore, Alice has 15 candies and Bob has 21 candies.\n• Actually, I made a mistake. Let me recalculate… [continues with more steps]\n• The answer is: Alice has 9 candies and Bob has 3 candies.\nChain of Draft (Expected):\n• A=Alice, B=Bob\n• Eq1: A+3=B-3\n• Eq2: B+3=2(A-3)\n• Solve: A=9, B=3\n• Answer: Alice 9, Bob 3\nOur Method (Diverse CoD + GRPO) (Expected):\n• Eq1: A+3=B-3 → A+6=B\n• Eq2: B+3=2(A-3) → B+3=2A-6\n• Solve: A=9, B=3\n• Alice has 9, Bob has 3.\nThe key difference we expect to see is that our method will learn to focus on the most critical reasoning steps through exploration of diverse drafts during training.\nPotential Implications If our hypothesis is confirmed, the findings would have several important implications:\nEnhanced Training Paradigms: Reasoning strategies like CoD could be effectively integrated into model training, not just prompting.\nEfficiency Without Accuracy Loss: We could have both high accuracy and low token usage, enabling real-time applications.\nSmaller Model Competitiveness: This approach could help smaller models perform reasoning tasks more effectively, reducing the need for massive parameter counts.\nGeneralized Diversity Strategies: The concept of diverse exploration followed by RL optimization could extend to other areas of LLM development.\nConclusion This proposed experiment aims to demonstrate that combining semantically diverse token sampling with Group Relative Policy Optimization (GRPO) can significantly enhance the Chain of Draft approach. If successful, the result would be a reasoning system that achieves the accuracy of verbose methods like Chain of Thought while maintaining the efficiency of concise drafting.\nThis approach represents a potential step toward more intelligent and cost-effective AI systems that can reason both broadly and efficiently—thinking faster by writing less, but exploring more.\nThis research builds upon “Chain of Draft: Thinking Faster by Writing Less” by Silei Xu et al. (2025) and extends it with concepts from Group Relative Policy Optimization (GRPO) and diverse sampling techniques.\nPractical Implementation: Training Qwen2.5-0.5B with GRPO To demonstrate how our approach would be implemented in practice, here’s a complete training script using the Hugging Face TRL (Transformer Reinforcement Learning) library, which provides a convenient implementation of GRPO.\nTraining Script (train_diverse_cod_grpo.py) \"\"\" Train Qwen2.5-0.5B with GRPO for Chain of Draft with Diverse Thinking Tokens This script demonstrates how to train a Qwen2.5-0.5B model using Group Relative Policy Optimization to generate concise, diverse reasoning drafts that maintain high accuracy. \"\"\" import re import torch from datasets import load_dataset, Dataset from transformers import AutoTokenizer, AutoModelForCausalLM from peft import LoraConfig from trl import GRPOConfig, GRPOTrainer # Define the Chain of Draft format with XML tags for clear structure SYSTEM_PROMPT = \"\"\" You are a problem-solving assistant that thinks efficiently. Respond in the following format: [Write concise reasoning steps, each ≤5 tokens] [Your final answer] \"\"\" XML_COD_FORMAT = \"\"\"\\ {draft} {answer} \"\"\" # Helper functions for extracting answers and evaluating responses def extract_draft(text: str) -\u003e str: \"\"\"Extract the draft reasoning from XML tags.\"\"\" if \"\" not in text or \"\" not in text: return \"\" draft = text.split(\"\")[-1] draft = draft.split(\"\")[0] return draft.strip() def extract_answer(text: str) -\u003e str: \"\"\"Extract the final answer from XML tags.\"\"\" if \"\" not in text or \"\" not in text: return \"\" answer = text.split(\"\")[-1] answer = answer.split(\"\")[0] return answer.strip() def extract_gsm8k_answer(text: str) -\u003e str | None: \"\"\"Extract the answer from GSM8K format.\"\"\" if \"####\" not in text: return None return text.split(\"####\")[1].strip().replace(\",\", \"\").replace(\"$\", \"\") # Functions for generating diverse drafts def generate_diverse_drafts(model, tokenizer, prompt, num_drafts=3, max_tokens=100): \"\"\" Generate multiple diverse reasoning drafts using different sampling strategies. Args: model: The language model tokenizer: The tokenizer for the model prompt: The problem statement num_drafts: Number of diverse drafts to generate max_tokens: Maximum tokens to generate per draft Returns: A list of diverse reasoning drafts \"\"\" drafts = [] # Prepare input inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) # Strategy 1: High Temperature Sampling # This encourages exploration of less likely tokens outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=1.2, # Higher temperature = more randomness top_k=50, repetition_penalty=1.0, pad_token_id=tokenizer.eos_token_id ) draft1 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft1)) # Strategy 2: Nucleus (Top-p) Sampling # This samples from the smallest set of tokens whose cumulative probability exceeds p outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.8, top_p=0.92, # Only consider tokens in the top 92% of probability mass repetition_penalty=1.1, pad_token_id=tokenizer.eos_token_id ) draft2 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft2)) # Strategy 3: Repetition Penalty Enforcement # This discourages the model from repeating the same patterns outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=0.9, top_k=40, top_p=0.95, repetition_penalty=1.5, # Strongly penalize repetition pad_token_id=tokenizer.eos_token_id ) draft3 = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft3)) # If more drafts are requested, generate with random combinations of parameters for i in range(3, num_drafts): # Randomly select parameters within reasonable ranges temp = 0.7 + 0.8 * torch.rand(1).item() # Temperature between 0.7 and 1.5 p = 0.85 + 0.14 * torch.rand(1).item() # Top-p between 0.85 and 0.99 rep_penalty = 1.0 + 0.8 * torch.rand(1).item() # Rep penalty between 1.0 and 1.8 outputs = model.generate( inputs.input_ids, max_new_tokens=max_tokens, do_sample=True, temperature=temp, top_p=p, repetition_penalty=rep_penalty, pad_token_id=tokenizer.eos_token_id ) draft = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) drafts.append(enforce_conciseness(draft)) return drafts def enforce_conciseness(draft, max_tokens_per_step=5): \"\"\" Ensure each reasoning step is concise by limiting tokens per line. Args: draft: The generated reasoning draft max_tokens_per_step: Maximum tokens allowed per reasoning step Returns: A concise version of the draft \"\"\" lines = draft.split('\\n') concise_lines = [] for line in lines: line = line.strip() if not line: continue # Tokenize the line (simple whitespace tokenization for illustration) tokens = line.split() # If the line is too long, truncate it if len(tokens) \u003e max_tokens_per_step: tokens = tokens[:max_tokens_per_step] concise_lines.append(' '.join(tokens)) return '\\n'.join(concise_lines) def select_best_draft(drafts, model, tokenizer, problem, reference_answer=None): \"\"\" Select the best draft based on a combination of correctness and conciseness. This function would typically be replaced by the GRPO reward mechanism during training. For inference, we can use this to select the most promising draft. Args: drafts: List of generated drafts model: The language model tokenizer: The tokenizer problem: The original problem reference_answer: The correct answer (if available) Returns: The best draft based on our heuristics \"\"\" best_score = -float('inf') best_draft = None for draft in drafts: # 1. Check if the draft leads to a correct answer # (In practice, you would use the model to generate an answer from the draft) # 2. Calculate conciseness score lines = [line for line in draft.split('\\n') if line.strip()] total_tokens = sum(len(line.split()) for line in lines) avg_tokens_per_line = total_tokens / max(1, len(lines)) # Lower average tokens per line is better (more concise) conciseness_score = 5 - min(5, avg_tokens_per_line) # 3. Calculate diversity score (simplified) # In practice, you would use embeddings or more sophisticated methods unique_words = set() for line in lines: unique_words.update(line.split()) diversity_score = min(5, len(unique_words) / 5) # 4. Combine scores (weights would be tuned in practice) score = conciseness_score + diversity_score if score \u003e best_score: best_score = score best_draft = draft return best_draft # Prepare the GSM8K dataset with Chain of Draft format def get_gsm8k_questions(split=\"train\") -\u003e Dataset: \"\"\"Load and preprocess the GSM8K dataset for Chain of Draft training.\"\"\" data = load_dataset('openai/gsm8k', 'main')[split] data = data.map(lambda x: { 'prompt': [ {'role': 'system', 'content': SYSTEM_PROMPT}, {'role': 'user', 'content': x['question']} ], 'answer': extract_gsm8k_answer(x['answer']) }) return data # Custom GRPO trainer that uses diverse draft generation class DiverseCoDGRPOTrainer(GRPOTrainer): \"\"\"Custom GRPO trainer that uses diverse draft generation strategies.\"\"\" def generate_completions(self, prompts, **kwargs): \"\"\"Override the default generation method to use diverse drafts.\"\"\" batch_size = len(prompts) num_generations = self.args.num_generations all_completions = [] for i in range(batch_size): prompt = self.tokenizer.apply_chat_template(prompts[i], tokenize=False) # Generate diverse drafts drafts = generate_diverse_drafts( self.model, self.tokenizer, prompt, num_drafts=num_generations, max_tokens=self.args.max_completion_length ) # Format each draft with XML tags completions = [] for draft in drafts: # Extract answer using the model (simplified here) answer_prompt = f\"{prompt}\\n\\n{draft}\\n\\n\" answer_inputs = self.tokenizer(answer_prompt, return_tensors=\"pt\").to(self.model.device) answer_outputs = self.model.generate( answer_inputs.input_ids, max_new_tokens=50, do_sample=False, pad_token_id=self.tokenizer.eos_token_id ) answer_text = self.tokenizer.decode( answer_outputs[0, answer_inputs.input_ids.shape[1]:], skip_special_tokens=True ).split(\"\")[0].strip() # Format the complete response formatted_completion = XML_COD_FORMAT.format(draft=draft, answer=answer_text) completions.append([{\"role\": \"assistant\", \"content\": formatted_completion}]) all_completions.append(completions) return all_completions # Define reward functions for GRPO training def combined_reward(prompts, completions, answer, **kwargs) -\u003e list[float]: \"\"\"Combined reward function that balances correctness, conciseness, and diversity.\"\"\" responses = [completion[0]['content'] for completion in completions] extracted_answers = [extract_answer(r) for r in responses] extracted_drafts = [extract_draft(r) for r in responses] rewards = [] for i, (resp, ans, draft) in enumerate(zip(responses, extracted_answers, extracted_drafts)): # 1. Correctness reward (1.0 for correct answers) correctness = 1.0 if ans == answer[i] else 0.0 # 2. Token efficiency reward # Count tokens in the draft lines = [line.strip() for line in draft.split('\\n') if line.strip()] total_tokens = sum(len(line.split()) for line in lines) token_penalty = 0.001 * total_tokens # Small penalty for each token used # 3. Conciseness reward concise_lines = 0 total_lines = max(1, len(lines)) for line in lines: tokens = line.split() if len(tokens) \u003c= 5: concise_lines += 1 conciseness_bonus = 0.2 * (concise_lines / total_lines) # 4. Format adherence reward format_bonus = 0.1 if (\"\" in resp and \"\" in resp and \"\" in resp and \"\" in resp) else 0.0 # Combine all rewards # R = 1.0 (for correct answer) - 0.001 × (number of tokens used) + bonuses total_reward = correctness - token_penalty + conciseness_bonus + format_bonus rewards.append(total_reward) # For debugging if i == 0: print('-'*20) print(f\"Correctness: {correctness}\") print(f\"Token penalty: {token_penalty}\") print(f\"Conciseness bonus: {conciseness_bonus}\") print(f\"Format bonus: {format_bonus}\") print(f\"Total reward: {total_reward}\") return rewards # Main training script def main(): # Configuration model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" output_dir = \"outputs/Qwen-0.5B-DiverseCoD-GRPO\" run_name = \"Qwen-0.5B-DiverseCoD-GRPO-gsm8k\" # Load dataset dataset = get_gsm8k_questions() print(f\"Loaded {len(dataset)} examples from GSM8K\") # GRPO training configuration training_args = GRPOConfig( output_dir=output_dir, run_name=run_name, learning_rate=5e-6, adam_beta1=0.9, adam_beta2=0.99, weight_decay=0.1, warmup_ratio=0.1, lr_scheduler_type='cosine', logging_steps=1, bf16=True, per_device_train_batch_size=1, gradient_accumulation_steps=4, num_generations=5, # Number of diverse drafts per problem max_prompt_length=256, max_completion_length=512, num_train_epochs=1, save_steps=100, max_grad_norm=0.1, report_to=\"wandb\", log_on_each_node=False, ) # LoRA configuration for parameter-efficient fine-tuning peft_config = LoraConfig( r=16, lora_alpha=64, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"], task_type=\"CAUSAL_LM\", lora_dropout=0.05, ) # Load model model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\", device_map=\"auto\" ) # Load tokenizer tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token = tokenizer.eos_token # Initialize custom GRPO trainer with combined reward function trainer = DiverseCoDGRPOTrainer( model=model, processing_class=tokenizer, reward_funcs=[combined_reward], # Use our combined reward function args=training_args, train_dataset=dataset, peft_config=peft_config ) # Train the model trainer.train() # Save the final model trainer.save_model(output_dir) print(f\"Training complete. Model saved to {output_dir}\") if __name__ == \"__main__\": main() Running the Training To train the model, you would run:\npython train_diverse_cod_grpo.py This script will:\nLoad the GSM8K dataset for math reasoning tasks Format the problems using a Chain of Draft structure with XML tags Initialize a Qwen2.5-0.5B model for GRPO training Apply LoRA for parameter-efficient fine-tuning Generate diverse drafts using the strategies defined in generate_diverse_drafts Train the model using a combined reward function that balances: Correctness of the final answer (1.0 for correct answers) Token efficiency (-0.001 per token used) Conciseness of reasoning steps (bonus for steps ≤5 tokens) Proper formatting (bonus for adhering to XML structure) Save checkpoints and the final model Key Components of the Implementation The implementation above includes several key components that make our approach work:\nCustom GRPO Trainer: We’ve created a DiverseCoDGRPOTrainer class that overrides the default generation method to use our generate_diverse_drafts function.\nDiverse Draft Generation: The generate_diverse_drafts function implements three specific sampling strategies plus additional random combinations to explore different reasoning paths.\nConciseness Enforcement: The enforce_conciseness function ensures that each reasoning step is limited to a maximum of 5 tokens, maintaining the efficiency goal of Chain of Draft.\nCombined Reward Function: Instead of separate reward functions, we’ve unified them into a single combined_reward function that implements our proposed reward formula:\nR = 1.0 (for correct answer) - 0.001 × (number of tokens used) + bonuses XML-Structured Format: Using XML tags ( and ) provides a clear structure for the model to follow, making it easier to extract and evaluate the reasoning and answer.\nInference with the Trained Model After training, you can use the model for inference:\nimport torch from transformers import AutoModelForCausalLM, AutoTokenizer # Load the trained model model_path = \"outputs/Qwen-0.5B-DiverseCoD-GRPO\" model = AutoModelForCausalLM.from_pretrained(model_path) tokenizer = AutoTokenizer.from_pretrained(model_path) def solve_problem(problem): \"\"\"Solve a problem using the trained Diverse CoD model.\"\"\" messages = [ {\"role\": \"system\", \"content\": \"\"\"You are a problem-solving assistant that thinks efficiently. Respond in the following format: [Write concise reasoning steps, each ≤5 tokens] [Your final answer] \"\"\"}, {\"role\": \"user\", \"content\": problem} ] # Format the input for the model prompt = tokenizer.apply_chat_template(messages, tokenize=False) # Generate multiple diverse drafts drafts = generate_diverse_drafts(model, tokenizer, prompt, num_drafts=5, max_tokens=100) # Select the best draft best_draft = select_best_draft(drafts, model, tokenizer, problem) # Generate final answer based on the best draft answer_prompt = f\"{prompt}\\n\\n{best_draft}\\n\\n\" inputs = tokenizer(answer_prompt, return_tensors=\"pt\").to(model.device) outputs = model.generate( inputs.input_ids, max_new_tokens=50, do_sample=False, pad_token_id=tokenizer.eos_token_id ) answer = tokenizer.decode(outputs[0, inputs.input_ids.shape[1]:], skip_special_tokens=True) answer = answer.split(\"\")[0].strip() return best_draft, answer # Example usage problem = \"Alice and Bob each have some candies. Alice says: if Bob gives me 3 candies, we'll have the same number. Bob says: if Alice gives me 3 candies, I'll have double what she has left. How many candies do Alice and Bob have?\" draft, answer = solve_problem(problem) print(\"Reasoning Draft:\") print(draft) print(\"\\nFinal Answer:\") print(answer) # Expected output: # Reasoning Draft: # A=Alice, B=Bob # Eq1: A+3=B-3 # Eq2: B+3=2(A-3) # Solve: A=9, B=3 # # Final Answer: # Alice has 9 candies and Bob has 3 candies. This implementation demonstrates how our approach can be practically applied to train a small language model (Qwen2.5-0.5B) to generate concise, diverse reasoning drafts that maintain high accuracy.\n","wordCount":"3966","inLanguage":"en","datePublished":"2025-03-05T00:00:00Z","dateModified":"2025-03-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</h1><div class=post-description>A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)</div><div class=post-meta><span title='2025-03-05 00:00:00 +0000 UTC'>March 5, 2025</span>&nbsp;·&nbsp;19 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#enhancing-llm-reasoning-chain-of-draft-with-semantically-diverse-thinking-tokens-using-grpo aria-label="Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO">Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</a><ul><li><a href=#the-challenge-efficient-reasoning-in-llms aria-label="The Challenge: Efficient Reasoning in LLMs">The Challenge: Efficient Reasoning in LLMs</a></li><li><a href=#our-hypothesis aria-label="Our Hypothesis">Our Hypothesis</a></li><li><a href=#proposed-experimental-design aria-label="Proposed Experimental Design">Proposed Experimental Design</a><ul><li><a href=#baseline-methods aria-label="Baseline Methods">Baseline Methods</a></li><li><a href=#reasoning-tasks aria-label="Reasoning Tasks">Reasoning Tasks</a></li><li><a href=#models-to-evaluate aria-label="Models to Evaluate">Models to Evaluate</a></li></ul></li><li><a href=#the-proposed-approach-diverse-token-sampling--grpo aria-label="The Proposed Approach: Diverse Token Sampling + GRPO">The Proposed Approach: Diverse Token Sampling + GRPO</a><ul><li><a href=#1-semantically-diverse-token-sampling aria-label="1. Semantically Diverse Token Sampling">1. Semantically Diverse Token Sampling</a><ul><li><a href=#code-example-implementing-diverse-token-sampling aria-label="Code Example: Implementing Diverse Token Sampling">Code Example: Implementing Diverse Token Sampling</a></li></ul></li></ul></li><li><a href=#expected-outcomes aria-label="Expected Outcomes">Expected Outcomes</a><ul><li><a href=#anticipated-findings aria-label="Anticipated Findings">Anticipated Findings</a></li><li><a href=#visual-representation-of-expected-results aria-label="Visual Representation of Expected Results">Visual Representation of Expected Results</a></li></ul></li><li><a href=#example-how-different-methods-might-tackle-the-same-problem aria-label="Example: How Different Methods Might Tackle the Same Problem">Example: How Different Methods Might Tackle the Same Problem</a></li><li><a href=#potential-implications aria-label="Potential Implications">Potential Implications</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#practical-implementation-training-qwen25-05b-with-grpo aria-label="Practical Implementation: Training Qwen2.5-0.5B with GRPO">Practical Implementation: Training Qwen2.5-0.5B with GRPO</a><ul><li><a href=#training-script-train_diverse_cod_grpopy aria-label="Training Script (train_diverse_cod_grpo.py)">Training Script (train_diverse_cod_grpo.py)</a></li><li><a href=#running-the-training aria-label="Running the Training">Running the Training</a></li><li><a href=#key-components-of-the-implementation aria-label="Key Components of the Implementation">Key Components of the Implementation</a></li><li><a href=#inference-with-the-trained-model aria-label="Inference with the Trained Model">Inference with the Trained Model</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=enhancing-llm-reasoning-chain-of-draft-with-semantically-diverse-thinking-tokens-using-grpo>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO<a hidden class=anchor aria-hidden=true href=#enhancing-llm-reasoning-chain-of-draft-with-semantically-diverse-thinking-tokens-using-grpo>#</a></h1><h2 id=the-challenge-efficient-reasoning-in-llms>The Challenge: Efficient Reasoning in LLMs<a hidden class=anchor aria-hidden=true href=#the-challenge-efficient-reasoning-in-llms>#</a></h2><p>Large Language Models (LLMs) have become remarkably capable at complex reasoning tasks, but this often comes at a cost: verbose outputs that consume significant computational resources. The Chain of Thought (CoT) prompting technique, while effective for accuracy, generates lengthy reasoning steps that increase token usage and latency.</p><p>Enter Chain of Draft (CoD), a promising alternative introduced by Xu et al. (2025) that encourages LLMs to produce concise, minimalistic reasoning steps. CoD has shown impressive results, matching or exceeding CoT accuracy while using as little as 7.6% of the tokens.</p><p>But could we make this approach even better?</p><h2 id=our-hypothesis>Our Hypothesis<a hidden class=anchor aria-hidden=true href=#our-hypothesis>#</a></h2><p>We hypothesize that by introducing <strong>semantically diverse token sampling</strong> into the CoD process and optimizing it through <strong>reinforcement learning (RL)</strong>, we could create a reasoning system that:</p><ol><li>Maintains the token efficiency of CoD</li><li>Matches or exceeds the accuracy of CoT</li><li>Explores multiple reasoning paths to find optimal solutions</li></ol><p>In other words: Can we make LLMs think both broadly (exploring different approaches) and efficiently (through concise drafting)?</p><h2 id=proposed-experimental-design>Proposed Experimental Design<a hidden class=anchor aria-hidden=true href=#proposed-experimental-design>#</a></h2><pre tabindex=0><code class=language-mermaid data-lang=mermaid>flowchart TD
    A[Problem Statement] --&gt; B[Baseline Methods]
    B --&gt; C1[Standard Prompting]
    B --&gt; C2[Chain of Thought]
    B --&gt; C3[Chain of Draft]
    B --&gt; C4[Our Method: Diverse CoD + GRPO]
    
    A --&gt; D[Evaluation Tasks]
    D --&gt; E1[Arithmetic Reasoning]
    D --&gt; E2[Commonsense Reasoning]
    D --&gt; E3[Symbolic/Logical Reasoning]
    D --&gt; E4[Coding Tasks]
    
    A --&gt; F[Models to Test]
    F --&gt; G1[Qwen2.5-0.5B]
    F --&gt; G2[Qwen2.5-1.5B]
    F --&gt; G3[Qwen2.5-7B]
    F --&gt; G4[Qwen2.5-72B]
    
    A --&gt; H[Metrics]
    H --&gt; I1[Accuracy]
    H --&gt; I2[Token Efficiency]
    H --&gt; I3[Reasoning Diversity]
    H --&gt; I4[Latency]
</code></pre><h3 id=baseline-methods>Baseline Methods<a hidden class=anchor aria-hidden=true href=#baseline-methods>#</a></h3><p>We plan to compare four different prompting strategies:</p><ol><li><strong>Standard Prompting</strong>: Direct answer without explicit reasoning</li><li><strong>Chain of Thought (CoT)</strong>: Detailed step-by-step reasoning</li><li><strong>Chain of Draft (CoD)</strong>: Concise intermediate reasoning steps</li><li><strong>Our Method (Diverse CoD + GRPO)</strong>: Enhanced CoD with diverse token sampling and GRPO optimization</li></ol><h3 id=reasoning-tasks>Reasoning Tasks<a hidden class=anchor aria-hidden=true href=#reasoning-tasks>#</a></h3><p>To thoroughly evaluate our approach, we&rsquo;ll test it on diverse reasoning tasks:</p><ul><li><strong>Arithmetic Reasoning</strong>: GSM8K math word problems</li><li><strong>Commonsense Reasoning</strong>: Date understanding and sports understanding from BIG-Bench</li><li><strong>Symbolic/Logical Reasoning</strong>: Coin-flip puzzles and logical transformations</li><li><strong>Coding Tasks</strong>: HumanEval programming challenges</li></ul><h3 id=models-to-evaluate>Models to Evaluate<a hidden class=anchor aria-hidden=true href=#models-to-evaluate>#</a></h3><p>We&rsquo;ll focus our evaluation exclusively on Qwen models to provide a consistent benchmark:</p><ul><li>Qwen2.5-0.5B</li><li>Qwen2.5-1.5B</li><li>Qwen2.5-7B</li><li>Qwen2.5-72B</li></ul><h2 id=the-proposed-approach-diverse-token-sampling--grpo>The Proposed Approach: Diverse Token Sampling + GRPO<a hidden class=anchor aria-hidden=true href=#the-proposed-approach-diverse-token-sampling--grpo>#</a></h2><p>The core innovation of our approach combines two key elements:</p><h3 id=1-semantically-diverse-token-sampling>1. Semantically Diverse Token Sampling<a hidden class=anchor aria-hidden=true href=#1-semantically-diverse-token-sampling>#</a></h3><p><img loading=lazy src=/images/sampling-mermaid.png alt="Diverse Token Sampling Process"></p><h4 id=code-example-implementing-diverse-token-sampling>Code Example: Implementing Diverse Token Sampling<a hidden class=anchor aria-hidden=true href=#code-example-implementing-diverse-token-sampling>#</a></h4><p>The following code demonstrates how we implement the token diversity module shown in the diagram above:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_diverse_drafts</span>(model, tokenizer, prompt, num_drafts<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generate multiple diverse reasoning drafts using different sampling strategies.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tokenizer: The tokenizer for the model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prompt: The problem statement
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        num_drafts: Number of diverse drafts to generate
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_tokens: Maximum tokens to generate per draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A list of diverse reasoning drafts
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    drafts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Prepare input</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>to(model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 1: High Temperature Sampling</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This encourages exploration of less likely tokens</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>1.2</span>,  <span style=color:#75715e># Higher temperature = more randomness</span>
</span></span><span style=display:flex><span>        top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft1 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft1))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 2: Nucleus (Top-p) Sampling</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This samples from the smallest set of tokens whose cumulative probability exceeds p</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>,
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.92</span>,  <span style=color:#75715e># Only consider tokens in the top 92% of probability mass</span>
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.1</span>,
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft2 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft2))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 3: Repetition Penalty Enforcement</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This discourages the model from repeating the same patterns</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,
</span></span><span style=display:flex><span>        top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span>,  <span style=color:#75715e># Strongly penalize repetition</span>
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft3 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft3))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># If more drafts are requested, generate with random combinations of parameters</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>3</span>, num_drafts):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Randomly select parameters within reasonable ranges</span>
</span></span><span style=display:flex><span>        temp <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.7</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># Temperature between 0.7 and 1.5</span>
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.85</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.14</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()   <span style=color:#75715e># Top-p between 0.85 and 0.99</span>
</span></span><span style=display:flex><span>        rep_penalty <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># Rep penalty between 1.0 and 1.8</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span>temp,
</span></span><span style=display:flex><span>            top_p<span style=color:#f92672>=</span>p,
</span></span><span style=display:flex><span>            repetition_penalty<span style=color:#f92672>=</span>rep_penalty,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        draft <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> drafts
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>enforce_conciseness</span>(draft, max_tokens_per_step<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Ensure each reasoning step is concise by limiting tokens per line.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        draft: The generated reasoning draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_tokens_per_step: Maximum tokens allowed per reasoning step
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A concise version of the draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    lines <span style=color:#f92672>=</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    concise_lines <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines:
</span></span><span style=display:flex><span>        line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize the line (simple whitespace tokenization for illustration)</span>
</span></span><span style=display:flex><span>        tokens <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># If the line is too long, truncate it</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(tokens) <span style=color:#f92672>&gt;</span> max_tokens_per_step:
</span></span><span style=display:flex><span>            tokens <span style=color:#f92672>=</span> tokens[:max_tokens_per_step]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        concise_lines<span style=color:#f92672>.</span>append(<span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(tokens))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>join(concise_lines)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_best_draft</span>(drafts, model, tokenizer, problem, reference_answer):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Select the best draft based on a combination of correctness and conciseness.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    This function would typically be replaced by the GRPO reward mechanism during training.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    For inference, we can use this to select the most promising draft.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        drafts: List of generated drafts
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tokenizer: The tokenizer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        problem: The original problem
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        reference_answer: The correct answer (if available)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        The best draft based on our heuristics
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    best_score <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>float(<span style=color:#e6db74>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>    best_draft <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> draft <span style=color:#f92672>in</span> drafts:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1. Check if the draft leads to a correct answer</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># (In practice, you would use the model to generate an answer from the draft)</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. Calculate conciseness score</span>
</span></span><span style=display:flex><span>        lines <span style=color:#f92672>=</span> [line <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>if</span> line<span style=color:#f92672>.</span>strip()]
</span></span><span style=display:flex><span>        total_tokens <span style=color:#f92672>=</span> sum(len(line<span style=color:#f92672>.</span>split()) <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines)
</span></span><span style=display:flex><span>        avg_tokens_per_line <span style=color:#f92672>=</span> total_tokens <span style=color:#f92672>/</span> max(<span style=color:#ae81ff>1</span>, len(lines))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Lower average tokens per line is better (more concise)</span>
</span></span><span style=display:flex><span>        conciseness_score <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>-</span> min(<span style=color:#ae81ff>5</span>, avg_tokens_per_line)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. Calculate diversity score (simplified)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># In practice, you would use embeddings or more sophisticated methods</span>
</span></span><span style=display:flex><span>        unique_words <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines:
</span></span><span style=display:flex><span>            unique_words<span style=color:#f92672>.</span>update(line<span style=color:#f92672>.</span>split())
</span></span><span style=display:flex><span>        diversity_score <span style=color:#f92672>=</span> min(<span style=color:#ae81ff>5</span>, len(unique_words) <span style=color:#f92672>/</span> <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4. Combine scores (weights would be tuned in practice)</span>
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> conciseness_score <span style=color:#f92672>+</span> diversity_score
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_score <span style=color:#f92672>=</span> score
</span></span><span style=display:flex><span>            best_draft <span style=color:#f92672>=</span> draft
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_draft
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 2. Reinforcement Learning with GRPO</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>We<span style=color:#e6db74>&#39;ll frame the reasoning task as a sequential decision-making process and use Group Relative Policy Optimization (GRPO) to train the model to maximize a reward function that balances:</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#f92672>**</span>Accuracy<span style=color:#f92672>**</span>: Correctness of the final answer
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#f92672>**</span>Token Efficiency<span style=color:#f92672>**</span>: Minimizing the number of tokens used
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> <span style=color:#f92672>**</span>Semantic Diversity<span style=color:#f92672>**</span>: Encouraging varied reasoning approaches
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The GRPO algorithm works by:
</span></span><span style=display:flex><span><span style=color:#ae81ff>1.</span> Sampling a group of reasoning paths <span style=color:#66d9ef>for</span> the same problem
</span></span><span style=display:flex><span><span style=color:#ae81ff>2.</span> Evaluating each path <span style=color:#66d9ef>with</span> our reward function
</span></span><span style=display:flex><span><span style=color:#ae81ff>3.</span> Calculating the advantage <span style=color:#66d9ef>for</span> each path by comparing its performance to the group average
</span></span><span style=display:flex><span><span style=color:#ae81ff>4.</span> Updating the policy to favor high<span style=color:#f92672>-</span>reward paths <span style=color:#66d9ef>while</span> maintaining KL divergence constraints
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The proposed reward function <span style=color:#f92672>is</span>:
</span></span></code></pre></div><p>R = 1.0 (for correct answer) - 0.001 × (number of tokens used)</p><pre tabindex=0><code>
This encourages the model to find the most efficient path to the correct answer while the group comparison mechanism of GRPO reduces variance and leads to more stable training.

## Implementation Plan

```mermaid
sequenceDiagram
    participant P as Problem
    participant M as Model
    participant R as GRPO Environment
    
    P-&gt;&gt;M: Present problem
    loop Training Episodes
        M-&gt;&gt;M: Generate diverse drafts
        M-&gt;&gt;R: Submit drafts &amp; answers
        R-&gt;&gt;R: Evaluate correctness
        R-&gt;&gt;R: Calculate reward
        R-&gt;&gt;M: Update policy
    end
    P-&gt;&gt;M: Test problem
    M-&gt;&gt;P: Optimized concise reasoning
</code></pre><ol><li><p><strong>Initial Setup</strong>: We&rsquo;ll start with a model fine-tuned to follow instructions.</p></li><li><p><strong>Training Process</strong>:</p><ul><li><strong>Episode Generation</strong>: The model will generate multiple reasoning drafts for each problem using diverse token sampling.</li><li><strong>Reward Calculation</strong>: We&rsquo;ll compute rewards based on answer correctness and token usage.</li><li><strong>Policy Update</strong>: Using GRPO, we&rsquo;ll adjust the model&rsquo;s parameters to increase the probability of token actions that lead to higher rewards compared to the group average, while maintaining a KL divergence constraint to prevent drastic changes.</li></ul></li><li><p><strong>Group Comparison</strong>: GRPO&rsquo;s group sampling approach naturally balances exploration vs. exploitation by comparing multiple reasoning paths against each other, reducing variance in updates and preventing premature convergence to suboptimal strategies.</p></li></ol><h2 id=expected-outcomes>Expected Outcomes<a hidden class=anchor aria-hidden=true href=#expected-outcomes>#</a></h2><p>Based on prior research on CoD and diverse sampling techniques, we anticipate the following outcomes:</p><table><thead><tr><th>Method</th><th>Expected Accuracy</th><th>Expected Tokens</th></tr></thead><tbody><tr><td>Standard Prompting</td><td>50-60%</td><td>1-5</td></tr><tr><td>Chain of Thought</td><td>90-95%</td><td>150-250</td></tr><tr><td>Chain of Draft</td><td>85-90%</td><td>30-60</td></tr><tr><td>Diverse CoD + GRPO</td><td>90-95%</td><td>30-60</td></tr></tbody></table><h3 id=anticipated-findings>Anticipated Findings<a hidden class=anchor aria-hidden=true href=#anticipated-findings>#</a></h3><ol><li><p><strong>Accuracy Improvement</strong>: We expect our method to achieve accuracy comparable to CoT while maintaining the token efficiency of CoD.</p></li><li><p><strong>Token Efficiency</strong>: We anticipate using approximately 75% fewer tokens than CoT, resulting in significantly lower latency and computational cost.</p></li><li><p><strong>Reasoning Diversity</strong>: The model should demonstrate the ability to explore multiple reasoning paths, helping it avoid common pitfalls and find more robust solutions.</p></li><li><p><strong>Generalization</strong>: We expect the approach to work well across different reasoning tasks and model sizes, with particularly notable improvements for smaller models.</p></li></ol><h3 id=visual-representation-of-expected-results>Visual Representation of Expected Results<a hidden class=anchor aria-hidden=true href=#visual-representation-of-expected-results>#</a></h3><table><thead><tr><th>Method</th><th>Accuracy (%)</th><th>Tokens Used</th></tr></thead><tbody><tr><td>Standard Prompting</td><td>55</td><td>3</td></tr><tr><td>Chain of Thought</td><td>93</td><td>200</td></tr><tr><td>Chain of Draft</td><td>88</td><td>45</td></tr><tr><td>Diverse CoD + GRPO</td><td>93</td><td>45</td></tr></tbody></table><h2 id=example-how-different-methods-might-tackle-the-same-problem>Example: How Different Methods Might Tackle the Same Problem<a hidden class=anchor aria-hidden=true href=#example-how-different-methods-might-tackle-the-same-problem>#</a></h2><p><strong>Problem</strong>: &ldquo;Alice and Bob each have some candies. Alice says: if Bob gives me 3 candies, we&rsquo;ll have the same number. Bob says: if Alice gives me 3 candies, I&rsquo;ll have double what she has left. How many candies do Alice and Bob have?&rdquo;</p><p><strong>Standard (Expected)</strong>:<br>&ldquo;Alice has 9 candies and Bob has 3 candies.&rdquo; (Might be correct or incorrect, with no visible reasoning)</p><p><strong>Chain of Thought (Expected)</strong>:<br>• Let&rsquo;s denote Alice&rsquo;s candies as A and Bob&rsquo;s candies as B.</p><p>• From Alice&rsquo;s statement: If Bob gives Alice 3 candies, they&rsquo;ll have the same number.<br>• This means: A + 3 = B - 3<br>• Simplifying: A + 6 = B</p><p>• From Bob&rsquo;s statement: If Alice gives Bob 3 candies, Bob will have double what Alice has left.<br>• This means: B + 3 = 2 * (A - 3)<br>• Simplifying: B + 3 = 2A - 6<br>• B = 2A - 9</p><p>• Now we have two equations:<br>• A + 6 = B<br>• B = 2A - 9</p><p>• Substituting the second equation into the first:<br>• A + 6 = 2A - 9<br>• 6 + 9 = 2A - A<br>• 15 = A</p><p>• So A = 15, and B = A + 6 = 15 + 6 = 21.</p><p>• Wait, let me double-check:<br>• If Bob gives Alice 3 candies: Alice would have 15 + 3 = 18, and Bob would have 21 - 3 = 18. ✓<br>• If Alice gives Bob 3 candies: Alice would have 15 - 3 = 12, and Bob would have 21 + 3 = 24. Is this double? 12 * 2 = 24. ✓</p><p>• Therefore, Alice has 15 candies and Bob has 21 candies.</p><p>• Actually, I made a mistake. Let me recalculate&mldr;
[continues with more steps]</p><p>• The answer is: Alice has 9 candies and Bob has 3 candies.</p><p><strong>Chain of Draft (Expected)</strong>:<br>• A=Alice, B=Bob<br>• Eq1: A+3=B-3<br>• Eq2: B+3=2(A-3)<br>• Solve: A=9, B=3<br>• Answer: Alice 9, Bob 3</p><p><strong>Our Method (Diverse CoD + GRPO) (Expected)</strong>:<br>• Eq1: A+3=B-3 → A+6=B<br>• Eq2: B+3=2(A-3) → B+3=2A-6<br>• Solve: A=9, B=3<br>• Alice has 9, Bob has 3.</p><p>The key difference we expect to see is that our method will learn to focus on the most critical reasoning steps through exploration of diverse drafts during training.</p><h2 id=potential-implications>Potential Implications<a hidden class=anchor aria-hidden=true href=#potential-implications>#</a></h2><p>If our hypothesis is confirmed, the findings would have several important implications:</p><ol><li><p><strong>Enhanced Training Paradigms</strong>: Reasoning strategies like CoD could be effectively integrated into model training, not just prompting.</p></li><li><p><strong>Efficiency Without Accuracy Loss</strong>: We could have both high accuracy and low token usage, enabling real-time applications.</p></li><li><p><strong>Smaller Model Competitiveness</strong>: This approach could help smaller models perform reasoning tasks more effectively, reducing the need for massive parameter counts.</p></li><li><p><strong>Generalized Diversity Strategies</strong>: The concept of diverse exploration followed by RL optimization could extend to other areas of LLM development.</p></li></ol><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This proposed experiment aims to demonstrate that combining semantically diverse token sampling with Group Relative Policy Optimization (GRPO) can significantly enhance the Chain of Draft approach. If successful, the result would be a reasoning system that achieves the accuracy of verbose methods like Chain of Thought while maintaining the efficiency of concise drafting.</p><p>This approach represents a potential step toward more intelligent and cost-effective AI systems that can reason both broadly and efficiently—thinking faster by writing less, but exploring more.</p><hr><p><em>This research builds upon &ldquo;Chain of Draft: Thinking Faster by Writing Less&rdquo; by Silei Xu et al. (2025) and extends it with concepts from Group Relative Policy Optimization (GRPO) and diverse sampling techniques.</em></p><h2 id=practical-implementation-training-qwen25-05b-with-grpo>Practical Implementation: Training Qwen2.5-0.5B with GRPO<a hidden class=anchor aria-hidden=true href=#practical-implementation-training-qwen25-05b-with-grpo>#</a></h2><p>To demonstrate how our approach would be implemented in practice, here&rsquo;s a complete training script using the Hugging Face TRL (Transformer Reinforcement Learning) library, which provides a convenient implementation of GRPO.</p><h3 id=training-script-train_diverse_cod_grpopy>Training Script (train_diverse_cod_grpo.py)<a hidden class=anchor aria-hidden=true href=#training-script-train_diverse_cod_grpopy>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Train Qwen2.5-0.5B with GRPO for Chain of Draft with Diverse Thinking Tokens
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>This script demonstrates how to train a Qwen2.5-0.5B model using Group Relative Policy Optimization
</span></span></span><span style=display:flex><span><span style=color:#e6db74>to generate concise, diverse reasoning drafts that maintain high accuracy.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset, Dataset
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer, AutoModelForCausalLM
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> peft <span style=color:#f92672>import</span> LoraConfig
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> trl <span style=color:#f92672>import</span> GRPOConfig, GRPOTrainer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define the Chain of Draft format with XML tags for clear structure</span>
</span></span><span style=display:flex><span>SYSTEM_PROMPT <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>You are a problem-solving assistant that thinks efficiently.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Respond in the following format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[Write concise reasoning steps, each ≤5 tokens]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;answer&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[Your final answer]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/answer&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>XML_COD_FORMAT <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;&#34;</span><span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span><span style=color:#e6db74>&lt;draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#e6db74>{draft}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;answer&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74></span><span style=color:#e6db74>{answer}</span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/answer&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Helper functions for extracting answers and evaluating responses</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_draft</span>(text: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Extract the draft reasoning from XML tags.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;&lt;draft&gt;&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> text <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;&lt;/draft&gt;&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> text:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    draft <span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;draft&gt;&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    draft <span style=color:#f92672>=</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;/draft&gt;&#34;</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> draft<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_answer</span>(text: str) <span style=color:#f92672>-&gt;</span> str:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Extract the final answer from XML tags.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;&lt;answer&gt;&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> text <span style=color:#f92672>or</span> <span style=color:#e6db74>&#34;&lt;/answer&gt;&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> text:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    answer <span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;answer&gt;&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    answer <span style=color:#f92672>=</span> answer<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;/answer&gt;&#34;</span>)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> answer<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_gsm8k_answer</span>(text: str) <span style=color:#f92672>-&gt;</span> str <span style=color:#f92672>|</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Extract the answer from GSM8K format.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;####&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> text:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;####&#34;</span>)[<span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>strip()<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;,&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#34;$&#34;</span>, <span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Functions for generating diverse drafts</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_diverse_drafts</span>(model, tokenizer, prompt, num_drafts<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generate multiple diverse reasoning drafts using different sampling strategies.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tokenizer: The tokenizer for the model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        prompt: The problem statement
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        num_drafts: Number of diverse drafts to generate
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_tokens: Maximum tokens to generate per draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A list of diverse reasoning drafts
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    drafts <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Prepare input</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tokenizer(prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>to(model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 1: High Temperature Sampling</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This encourages exploration of less likely tokens</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>1.2</span>,  <span style=color:#75715e># Higher temperature = more randomness</span>
</span></span><span style=display:flex><span>        top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>,
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft1 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft1))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 2: Nucleus (Top-p) Sampling</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This samples from the smallest set of tokens whose cumulative probability exceeds p</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>,
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.92</span>,  <span style=color:#75715e># Only consider tokens in the top 92% of probability mass</span>
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.1</span>,
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft2 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft2))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Strategy 3: Repetition Penalty Enforcement</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># This discourages the model from repeating the same patterns</span>
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        temperature<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,
</span></span><span style=display:flex><span>        top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>,
</span></span><span style=display:flex><span>        top_p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>        repetition_penalty<span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span>,  <span style=color:#75715e># Strongly penalize repetition</span>
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    draft3 <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft3))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># If more drafts are requested, generate with random combinations of parameters</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>3</span>, num_drafts):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Randomly select parameters within reasonable ranges</span>
</span></span><span style=display:flex><span>        temp <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.7</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># Temperature between 0.7 and 1.5</span>
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.85</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.14</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()   <span style=color:#75715e># Top-p between 0.85 and 0.99</span>
</span></span><span style=display:flex><span>        rep_penalty <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> torch<span style=color:#f92672>.</span>rand(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># Rep penalty between 1.0 and 1.8</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>            inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>            max_new_tokens<span style=color:#f92672>=</span>max_tokens,
</span></span><span style=display:flex><span>            do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>            temperature<span style=color:#f92672>=</span>temp,
</span></span><span style=display:flex><span>            top_p<span style=color:#f92672>=</span>p,
</span></span><span style=display:flex><span>            repetition_penalty<span style=color:#f92672>=</span>rep_penalty,
</span></span><span style=display:flex><span>            pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        draft <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        drafts<span style=color:#f92672>.</span>append(enforce_conciseness(draft))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> drafts
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>enforce_conciseness</span>(draft, max_tokens_per_step<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Ensure each reasoning step is concise by limiting tokens per line.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        draft: The generated reasoning draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        max_tokens_per_step: Maximum tokens allowed per reasoning step
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        A concise version of the draft
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    lines <span style=color:#f92672>=</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>    concise_lines <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines:
</span></span><span style=display:flex><span>        line <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> line:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#75715e># Tokenize the line (simple whitespace tokenization for illustration)</span>
</span></span><span style=display:flex><span>        tokens <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># If the line is too long, truncate it</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(tokens) <span style=color:#f92672>&gt;</span> max_tokens_per_step:
</span></span><span style=display:flex><span>            tokens <span style=color:#f92672>=</span> tokens[:max_tokens_per_step]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        concise_lines<span style=color:#f92672>.</span>append(<span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(tokens))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>.</span>join(concise_lines)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>select_best_draft</span>(drafts, model, tokenizer, problem, reference_answer<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Select the best draft based on a combination of correctness and conciseness.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    This function would typically be replaced by the GRPO reward mechanism during training.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    For inference, we can use this to select the most promising draft.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        drafts: List of generated drafts
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model: The language model
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tokenizer: The tokenizer
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        problem: The original problem
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        reference_answer: The correct answer (if available)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        The best draft based on our heuristics
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    best_score <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>float(<span style=color:#e6db74>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>    best_draft <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> draft <span style=color:#f92672>in</span> drafts:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1. Check if the draft leads to a correct answer</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># (In practice, you would use the model to generate an answer from the draft)</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. Calculate conciseness score</span>
</span></span><span style=display:flex><span>        lines <span style=color:#f92672>=</span> [line <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>if</span> line<span style=color:#f92672>.</span>strip()]
</span></span><span style=display:flex><span>        total_tokens <span style=color:#f92672>=</span> sum(len(line<span style=color:#f92672>.</span>split()) <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines)
</span></span><span style=display:flex><span>        avg_tokens_per_line <span style=color:#f92672>=</span> total_tokens <span style=color:#f92672>/</span> max(<span style=color:#ae81ff>1</span>, len(lines))
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Lower average tokens per line is better (more concise)</span>
</span></span><span style=display:flex><span>        conciseness_score <span style=color:#f92672>=</span> <span style=color:#ae81ff>5</span> <span style=color:#f92672>-</span> min(<span style=color:#ae81ff>5</span>, avg_tokens_per_line)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. Calculate diversity score (simplified)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># In practice, you would use embeddings or more sophisticated methods</span>
</span></span><span style=display:flex><span>        unique_words <span style=color:#f92672>=</span> set()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines:
</span></span><span style=display:flex><span>            unique_words<span style=color:#f92672>.</span>update(line<span style=color:#f92672>.</span>split())
</span></span><span style=display:flex><span>        diversity_score <span style=color:#f92672>=</span> min(<span style=color:#ae81ff>5</span>, len(unique_words) <span style=color:#f92672>/</span> <span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4. Combine scores (weights would be tuned in practice)</span>
</span></span><span style=display:flex><span>        score <span style=color:#f92672>=</span> conciseness_score <span style=color:#f92672>+</span> diversity_score
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> score <span style=color:#f92672>&gt;</span> best_score:
</span></span><span style=display:flex><span>            best_score <span style=color:#f92672>=</span> score
</span></span><span style=display:flex><span>            best_draft <span style=color:#f92672>=</span> draft
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_draft
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Prepare the GSM8K dataset with Chain of Draft format</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_gsm8k_questions</span>(split<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;train&#34;</span>) <span style=color:#f92672>-&gt;</span> Dataset:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Load and preprocess the GSM8K dataset for Chain of Draft training.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#39;openai/gsm8k&#39;</span>, <span style=color:#e6db74>&#39;main&#39;</span>)[split]
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;prompt&#39;</span>: [
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;system&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: SYSTEM_PROMPT},
</span></span><span style=display:flex><span>            {<span style=color:#e6db74>&#39;role&#39;</span>: <span style=color:#e6db74>&#39;user&#39;</span>, <span style=color:#e6db74>&#39;content&#39;</span>: x[<span style=color:#e6db74>&#39;question&#39;</span>]}
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;answer&#39;</span>: extract_gsm8k_answer(x[<span style=color:#e6db74>&#39;answer&#39;</span>])
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Custom GRPO trainer that uses diverse draft generation</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DiverseCoDGRPOTrainer</span>(GRPOTrainer):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Custom GRPO trainer that uses diverse draft generation strategies.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_completions</span>(self, prompts, <span style=color:#f92672>**</span>kwargs):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Override the default generation method to use diverse drafts.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        batch_size <span style=color:#f92672>=</span> len(prompts)
</span></span><span style=display:flex><span>        num_generations <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>args<span style=color:#f92672>.</span>num_generations
</span></span><span style=display:flex><span>        all_completions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(batch_size):
</span></span><span style=display:flex><span>            prompt <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>apply_chat_template(prompts[i], tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Generate diverse drafts</span>
</span></span><span style=display:flex><span>            drafts <span style=color:#f92672>=</span> generate_diverse_drafts(
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>model, 
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>tokenizer, 
</span></span><span style=display:flex><span>                prompt, 
</span></span><span style=display:flex><span>                num_drafts<span style=color:#f92672>=</span>num_generations,
</span></span><span style=display:flex><span>                max_tokens<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>args<span style=color:#f92672>.</span>max_completion_length
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Format each draft with XML tags</span>
</span></span><span style=display:flex><span>            completions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> draft <span style=color:#f92672>in</span> drafts:
</span></span><span style=display:flex><span>                <span style=color:#75715e># Extract answer using the model (simplified here)</span>
</span></span><span style=display:flex><span>                answer_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;draft&gt;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>draft<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;/draft&gt;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;answer&gt;&#34;</span>
</span></span><span style=display:flex><span>                answer_inputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>tokenizer(answer_prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>to(self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>                answer_outputs <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>                    answer_inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>                    max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>                    do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                    pad_token_id<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>                )
</span></span><span style=display:flex><span>                answer_text <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>decode(
</span></span><span style=display:flex><span>                    answer_outputs[<span style=color:#ae81ff>0</span>, answer_inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], 
</span></span><span style=display:flex><span>                    skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>                )<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;/answer&gt;&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Format the complete response</span>
</span></span><span style=display:flex><span>                formatted_completion <span style=color:#f92672>=</span> XML_COD_FORMAT<span style=color:#f92672>.</span>format(draft<span style=color:#f92672>=</span>draft, answer<span style=color:#f92672>=</span>answer_text)
</span></span><span style=display:flex><span>                completions<span style=color:#f92672>.</span>append([{<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;assistant&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: formatted_completion}])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            all_completions<span style=color:#f92672>.</span>append(completions)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> all_completions
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define reward functions for GRPO training</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>combined_reward</span>(prompts, completions, answer, <span style=color:#f92672>**</span>kwargs) <span style=color:#f92672>-&gt;</span> list[float]:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Combined reward function that balances correctness, conciseness, and diversity.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    responses <span style=color:#f92672>=</span> [completion[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;content&#39;</span>] <span style=color:#66d9ef>for</span> completion <span style=color:#f92672>in</span> completions]
</span></span><span style=display:flex><span>    extracted_answers <span style=color:#f92672>=</span> [extract_answer(r) <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> responses]
</span></span><span style=display:flex><span>    extracted_drafts <span style=color:#f92672>=</span> [extract_draft(r) <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> responses]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    rewards <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (resp, ans, draft) <span style=color:#f92672>in</span> enumerate(zip(responses, extracted_answers, extracted_drafts)):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 1. Correctness reward (1.0 for correct answers)</span>
</span></span><span style=display:flex><span>        correctness <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.0</span> <span style=color:#66d9ef>if</span> ans <span style=color:#f92672>==</span> answer[i] <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 2. Token efficiency reward</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Count tokens in the draft</span>
</span></span><span style=display:flex><span>        lines <span style=color:#f92672>=</span> [line<span style=color:#f92672>.</span>strip() <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> draft<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>if</span> line<span style=color:#f92672>.</span>strip()]
</span></span><span style=display:flex><span>        total_tokens <span style=color:#f92672>=</span> sum(len(line<span style=color:#f92672>.</span>split()) <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines)
</span></span><span style=display:flex><span>        token_penalty <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.001</span> <span style=color:#f92672>*</span> total_tokens  <span style=color:#75715e># Small penalty for each token used</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 3. Conciseness reward</span>
</span></span><span style=display:flex><span>        concise_lines <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        total_lines <span style=color:#f92672>=</span> max(<span style=color:#ae81ff>1</span>, len(lines))
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> line <span style=color:#f92672>in</span> lines:
</span></span><span style=display:flex><span>            tokens <span style=color:#f92672>=</span> line<span style=color:#f92672>.</span>split()
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(tokens) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>5</span>:
</span></span><span style=display:flex><span>                concise_lines <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        conciseness_bonus <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.2</span> <span style=color:#f92672>*</span> (concise_lines <span style=color:#f92672>/</span> total_lines)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># 4. Format adherence reward</span>
</span></span><span style=display:flex><span>        format_bonus <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span> <span style=color:#66d9ef>if</span> (<span style=color:#e6db74>&#34;&lt;draft&gt;&#34;</span> <span style=color:#f92672>in</span> resp <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;&lt;/draft&gt;&#34;</span> <span style=color:#f92672>in</span> resp <span style=color:#f92672>and</span> 
</span></span><span style=display:flex><span>                               <span style=color:#e6db74>&#34;&lt;answer&gt;&#34;</span> <span style=color:#f92672>in</span> resp <span style=color:#f92672>and</span> <span style=color:#e6db74>&#34;&lt;/answer&gt;&#34;</span> <span style=color:#f92672>in</span> resp) <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Combine all rewards</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># R = 1.0 (for correct answer) - 0.001 × (number of tokens used) + bonuses</span>
</span></span><span style=display:flex><span>        total_reward <span style=color:#f92672>=</span> correctness <span style=color:#f92672>-</span> token_penalty <span style=color:#f92672>+</span> conciseness_bonus <span style=color:#f92672>+</span> format_bonus
</span></span><span style=display:flex><span>        rewards<span style=color:#f92672>.</span>append(total_reward)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># For debugging</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> i <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#39;-&#39;</span><span style=color:#f92672>*</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Correctness: </span><span style=color:#e6db74>{</span>correctness<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Token penalty: </span><span style=color:#e6db74>{</span>token_penalty<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Conciseness bonus: </span><span style=color:#e6db74>{</span>conciseness_bonus<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Format bonus: </span><span style=color:#e6db74>{</span>format_bonus<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Total reward: </span><span style=color:#e6db74>{</span>total_reward<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> rewards
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Main training script</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># Configuration</span>
</span></span><span style=display:flex><span>    model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Qwen/Qwen2.5-0.5B-Instruct&#34;</span>
</span></span><span style=display:flex><span>    output_dir <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;outputs/Qwen-0.5B-DiverseCoD-GRPO&#34;</span>
</span></span><span style=display:flex><span>    run_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Qwen-0.5B-DiverseCoD-GRPO-gsm8k&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load dataset</span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> get_gsm8k_questions()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loaded </span><span style=color:#e6db74>{</span>len(dataset)<span style=color:#e6db74>}</span><span style=color:#e6db74> examples from GSM8K&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># GRPO training configuration</span>
</span></span><span style=display:flex><span>    training_args <span style=color:#f92672>=</span> GRPOConfig(
</span></span><span style=display:flex><span>        output_dir<span style=color:#f92672>=</span>output_dir,
</span></span><span style=display:flex><span>        run_name<span style=color:#f92672>=</span>run_name,
</span></span><span style=display:flex><span>        learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>5e-6</span>,
</span></span><span style=display:flex><span>        adam_beta1<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>,
</span></span><span style=display:flex><span>        adam_beta2<span style=color:#f92672>=</span><span style=color:#ae81ff>0.99</span>,
</span></span><span style=display:flex><span>        weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>        warmup_ratio<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>        lr_scheduler_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;cosine&#39;</span>,
</span></span><span style=display:flex><span>        logging_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        bf16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        per_device_train_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        gradient_accumulation_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        num_generations<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>,  <span style=color:#75715e># Number of diverse drafts per problem</span>
</span></span><span style=display:flex><span>        max_prompt_length<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>,
</span></span><span style=display:flex><span>        max_completion_length<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>,
</span></span><span style=display:flex><span>        num_train_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        save_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>,
</span></span><span style=display:flex><span>        max_grad_norm<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>,
</span></span><span style=display:flex><span>        report_to<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;wandb&#34;</span>,
</span></span><span style=display:flex><span>        log_on_each_node<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># LoRA configuration for parameter-efficient fine-tuning</span>
</span></span><span style=display:flex><span>    peft_config <span style=color:#f92672>=</span> LoraConfig(
</span></span><span style=display:flex><span>        r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>        lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>,
</span></span><span style=display:flex><span>        target_modules<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;o_proj&#34;</span>, <span style=color:#e6db74>&#34;up_proj&#34;</span>, <span style=color:#e6db74>&#34;down_proj&#34;</span>, <span style=color:#e6db74>&#34;gate_proj&#34;</span>],
</span></span><span style=display:flex><span>        task_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;CAUSAL_LM&#34;</span>,
</span></span><span style=display:flex><span>        lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0.05</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load model</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>        model_name,
</span></span><span style=display:flex><span>        torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>bfloat16,
</span></span><span style=display:flex><span>        attn_implementation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;flash_attention_2&#34;</span>,
</span></span><span style=display:flex><span>        device_map<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load tokenizer</span>
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>.</span>pad_token <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>eos_token
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Initialize custom GRPO trainer with combined reward function</span>
</span></span><span style=display:flex><span>    trainer <span style=color:#f92672>=</span> DiverseCoDGRPOTrainer(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>        processing_class<span style=color:#f92672>=</span>tokenizer,
</span></span><span style=display:flex><span>        reward_funcs<span style=color:#f92672>=</span>[combined_reward],  <span style=color:#75715e># Use our combined reward function</span>
</span></span><span style=display:flex><span>        args<span style=color:#f92672>=</span>training_args,
</span></span><span style=display:flex><span>        train_dataset<span style=color:#f92672>=</span>dataset,
</span></span><span style=display:flex><span>        peft_config<span style=color:#f92672>=</span>peft_config
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Train the model</span>
</span></span><span style=display:flex><span>    trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Save the final model</span>
</span></span><span style=display:flex><span>    trainer<span style=color:#f92672>.</span>save_model(output_dir)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Training complete. Model saved to </span><span style=color:#e6db74>{</span>output_dir<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h3 id=running-the-training>Running the Training<a hidden class=anchor aria-hidden=true href=#running-the-training>#</a></h3><p>To train the model, you would run:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python train_diverse_cod_grpo.py
</span></span></code></pre></div><p>This script will:</p><ol><li>Load the GSM8K dataset for math reasoning tasks</li><li>Format the problems using a Chain of Draft structure with XML tags</li><li>Initialize a Qwen2.5-0.5B model for GRPO training</li><li>Apply LoRA for parameter-efficient fine-tuning</li><li>Generate diverse drafts using the strategies defined in <code>generate_diverse_drafts</code></li><li>Train the model using a combined reward function that balances:<ul><li>Correctness of the final answer (1.0 for correct answers)</li><li>Token efficiency (-0.001 per token used)</li><li>Conciseness of reasoning steps (bonus for steps ≤5 tokens)</li><li>Proper formatting (bonus for adhering to XML structure)</li></ul></li><li>Save checkpoints and the final model</li></ol><h3 id=key-components-of-the-implementation>Key Components of the Implementation<a hidden class=anchor aria-hidden=true href=#key-components-of-the-implementation>#</a></h3><p>The implementation above includes several key components that make our approach work:</p><ol><li><p><strong>Custom GRPO Trainer</strong>: We&rsquo;ve created a <code>DiverseCoDGRPOTrainer</code> class that overrides the default generation method to use our <code>generate_diverse_drafts</code> function.</p></li><li><p><strong>Diverse Draft Generation</strong>: The <code>generate_diverse_drafts</code> function implements three specific sampling strategies plus additional random combinations to explore different reasoning paths.</p></li><li><p><strong>Conciseness Enforcement</strong>: The <code>enforce_conciseness</code> function ensures that each reasoning step is limited to a maximum of 5 tokens, maintaining the efficiency goal of Chain of Draft.</p></li><li><p><strong>Combined Reward Function</strong>: Instead of separate reward functions, we&rsquo;ve unified them into a single <code>combined_reward</code> function that implements our proposed reward formula:</p><pre tabindex=0><code>R = 1.0 (for correct answer) - 0.001 × (number of tokens used) + bonuses
</code></pre></li><li><p><strong>XML-Structured Format</strong>: Using XML tags (<code>&lt;draft></code> and <code>&lt;answer></code>) provides a clear structure for the model to follow, making it easier to extract and evaluate the reasoning and answer.</p></li></ol><h3 id=inference-with-the-trained-model>Inference with the Trained Model<a hidden class=anchor aria-hidden=true href=#inference-with-the-trained-model>#</a></h3><p>After training, you can use the model for inference:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the trained model</span>
</span></span><span style=display:flex><span>model_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;outputs/Qwen-0.5B-DiverseCoD-GRPO&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForCausalLM<span style=color:#f92672>.</span>from_pretrained(model_path)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>solve_problem</span>(problem):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Solve a problem using the trained Diverse CoD model.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    messages <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;system&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: <span style=color:#e6db74>&#34;&#34;&#34;You are a problem-solving assistant that thinks efficiently.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>Respond in the following format:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[Write concise reasoning steps, each ≤5 tokens]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/draft&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;answer&gt;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[Your final answer]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>&lt;/answer&gt;&#34;&#34;&#34;</span>},
</span></span><span style=display:flex><span>        {<span style=color:#e6db74>&#34;role&#34;</span>: <span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;content&#34;</span>: problem}
</span></span><span style=display:flex><span>    ]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Format the input for the model</span>
</span></span><span style=display:flex><span>    prompt <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>apply_chat_template(messages, tokenize<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate multiple diverse drafts</span>
</span></span><span style=display:flex><span>    drafts <span style=color:#f92672>=</span> generate_diverse_drafts(model, tokenizer, prompt, num_drafts<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, max_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Select the best draft</span>
</span></span><span style=display:flex><span>    best_draft <span style=color:#f92672>=</span> select_best_draft(drafts, model, tokenizer, problem)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Generate final answer based on the best draft</span>
</span></span><span style=display:flex><span>    answer_prompt <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>prompt<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;draft&gt;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>best_draft<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;/draft&gt;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&lt;answer&gt;&#34;</span>
</span></span><span style=display:flex><span>    inputs <span style=color:#f92672>=</span> tokenizer(answer_prompt, return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;pt&#34;</span>)<span style=color:#f92672>.</span>to(model<span style=color:#f92672>.</span>device)
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>generate(
</span></span><span style=display:flex><span>        inputs<span style=color:#f92672>.</span>input_ids,
</span></span><span style=display:flex><span>        max_new_tokens<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>        do_sample<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        pad_token_id<span style=color:#f92672>=</span>tokenizer<span style=color:#f92672>.</span>eos_token_id
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    answer <span style=color:#f92672>=</span> tokenizer<span style=color:#f92672>.</span>decode(outputs[<span style=color:#ae81ff>0</span>, inputs<span style=color:#f92672>.</span>input_ids<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]:], skip_special_tokens<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    answer <span style=color:#f92672>=</span> answer<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;&lt;/answer&gt;&#34;</span>)[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> best_draft, answer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Example usage</span>
</span></span><span style=display:flex><span>problem <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Alice and Bob each have some candies. Alice says: if Bob gives me 3 candies, we&#39;ll have the same number. Bob says: if Alice gives me 3 candies, I&#39;ll have double what she has left. How many candies do Alice and Bob have?&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>draft, answer <span style=color:#f92672>=</span> solve_problem(problem)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;Reasoning Draft:&#34;</span>)
</span></span><span style=display:flex><span>print(draft)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Final Answer:&#34;</span>)
</span></span><span style=display:flex><span>print(answer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Expected output:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Reasoning Draft:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># A=Alice, B=Bob</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Eq1: A+3=B-3</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Eq2: B+3=2(A-3)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Solve: A=9, B=3</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Final Answer:</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Alice has 9 candies and Bob has 3 candies.</span>
</span></span></code></pre></div><p>This implementation demonstrates how our approach can be practically applied to train a small language model (Qwen2.5-0.5B) to generate concise, diverse reasoning drafts that maintain high accuracy.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/chain-of-thought/>Chain of Thought</a></li><li><a href=https://dylanler.github.io/tags/chain-of-draft/>Chain of Draft</a></li><li><a href=https://dylanler.github.io/tags/reinforcement-learning/>Reinforcement-Learning</a></li></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/creating-a-video-dataset-with-precise-camera-movement-prompt/><span class=title>« Prev</span><br><span>Creating a Video Dataset With Precise Camera Movement Prompts</span>
</a><a class=next href=https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/><span class=title>Next »</span><br><span>Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>