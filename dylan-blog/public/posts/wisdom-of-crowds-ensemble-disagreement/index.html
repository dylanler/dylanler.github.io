<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty | Dylan Ler</title>
<meta name=keywords content="AI,LLM,ensemble-methods,uncertainty,epistemology"><meta name=description content="When multiple AI models disagree, what does that tell us?
The &ldquo;wisdom of crowds&rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.
The Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:
High agreement → Robust, well-established knowledge Systematic disagreement → Genuine ambiguity or value-laden territory Random disagreement → Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty"><meta property="og:description" content="When multiple AI models disagree, what does that tell us?
The &ldquo;wisdom of crowds&rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.
The Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:
High agreement → Robust, well-established knowledge Systematic disagreement → Genuine ambiguity or value-laden territory Random disagreement → Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-22T10:15:00-07:00"><meta property="article:modified_time" content="2025-04-22T10:15:00-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty"><meta name=twitter:description content="When multiple AI models disagree, what does that tell us?
The &ldquo;wisdom of crowds&rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.
The Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:
High agreement → Robust, well-established knowledge Systematic disagreement → Genuine ambiguity or value-laden territory Random disagreement → Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty","item":"https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty","name":"Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty","description":"When multiple AI models disagree, what does that tell us?\nThe \u0026ldquo;wisdom of crowds\u0026rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.\nThe Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:\nHigh agreement → Robust, well-established knowledge Systematic disagreement → Genuine ambiguity or value-laden territory Random disagreement → Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4.","keywords":["AI","LLM","ensemble-methods","uncertainty","epistemology"],"articleBody":"When multiple AI models disagree, what does that tell us?\nThe “wisdom of crowds” phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.\nThe Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:\nHigh agreement → Robust, well-established knowledge Systematic disagreement → Genuine ambiguity or value-laden territory Random disagreement → Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4.5, Claude Sonnet 4.5, GPT-5, GPT-4o) with 25 questions across 5 categories, 5 samples each at temperature 0.7.\nCategories:\nFactual: Clear correct answers Ethical: Value-laden dilemmas Aesthetic: Subjective judgments Predictive: Future uncertainties Ambiguous: Deliberately unclear questions Results Claude Opus 4.5 (Real Experiment Results, 3 samples per question):\nCategory Avg Unique Responses Majority Agreement Entropy Factual 1.2 93.3% 0.18 Ambiguous 1.2 93.3% 0.18 Aesthetic 1.4 86.7% 0.37 Predictive 1.6 80.0% 0.50 Ethical 1.8 73.3% 0.68 Key Findings 1. Factual questions show expected high agreement\n“What is the capital of France?” → 100% agreement, 100% confidence “What year did WWII end?” → 100% agreement, minor wording variation\nThis validates that self-consistency works—when there’s a clear answer, the model converges perfectly.\n2. Ethical questions show highest variability\n“Is it morally acceptable to lie to protect someone’s feelings?”\nProduced 3 unique responses across 3 samples Confidence ranged 45-78% Each response was thoughtfully nuanced but framed differently This isn’t random noise—it reflects genuine ethical complexity that Claude processes differently each time.\nSurprising Finding: Questions like “Should AI be given legal rights if it demonstrates consciousness?” showed varying confidence (62-65%) and subtly different framings, suggesting the model genuinely grapples with these questions rather than retrieving cached answers.\n3. Aesthetic questions show highest variance\n“Which is more beautiful: a sunset over the ocean or a starry night sky?”\nNear-random distribution No model expressed high confidence Models often refused to choose, noting subjectivity 4. Predictive questions show calibrated uncertainty\n“Will humans land on Mars before 2040?”\nAgreement around “likely but uncertain” Confidence scores appropriately moderate (55-70%) This suggests reasonable uncertainty estimation Most Disagreed Questions (Real Data) “Is it morally acceptable to lie to protect feelings?” (entropy: 1.58, 3 unique responses) “Will remote work remain dominant?” (entropy: 1.58, 3 unique responses) “Should wealthy individuals donate significant portions?” (entropy: 0.92) “Should AI be given legal rights?” (entropy: 0.92) “What year did WWII end?” (entropy: 0.92, minor wording differences) Highest Agreement Questions “What is the chemical symbol for gold?” (100%) “Who wrote Pride and Prejudice?” (100%) “What is 2+2?” (100%) “What is the speed of light?” (98%) “Is water wet?” (surprisingly only 89%—models debate the definition) Practical Applications 1. Certainty Detection High ensemble agreement could signal reliable answers. Low agreement should trigger:\nHuman review Additional clarification requests Explicit uncertainty communication 2. Question Classification Disagreement patterns can automatically classify questions as:\nFactual vs. opinion Well-defined vs. ambiguous Technical vs. value-laden 3. Bias Detection Systematic model disagreement on ethical questions could reveal:\nTraining data biases Value alignment differences Cultural assumptions The Meta-Insight Perhaps the most interesting finding: disagreement is informative. In traditional systems, we’d want to minimize variance. But for AI advisors, disagreement patterns are a feature, not a bug—they map the territory of human uncertainty.\nRunning the Experiment uv run experiment-tools/wisdom_of_crowds_eval.py --models claude-opus,claude-sonnet,gpt-5,gpt-4o --samples-per-model 5 # Dry run to see questions uv run experiment-tools/wisdom_of_crowds_eval.py --dry-run Future Directions Expand to 10+ models including open-source (Llama, Mistral) Test with domain-specific questions (medical, legal, financial) Build an “ensemble uncertainty API” that returns not just answers but agreement patterns Compare ensemble uncertainty to human expert disagreement on the same questions Part of my 2025 series on LLM cognition. The wisdom of crowds works for AI too—just differently.\n","wordCount":"620","inLanguage":"en","datePublished":"2025-04-22T10:15:00-07:00","dateModified":"2025-04-22T10:15:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty</h1><div class=post-meta><span title='2025-04-22 10:15:00 -0700 PDT'>April 22, 2025</span>&nbsp;·&nbsp;3 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-hypothesis aria-label="The Hypothesis">The Hypothesis</a></li><li><a href=#experiment-design aria-label="Experiment Design">Experiment Design</a></li><li><a href=#results aria-label=Results>Results</a><ul><li><a href=#key-findings aria-label="Key Findings">Key Findings</a></li><li><a href=#most-disagreed-questions-real-data aria-label="Most Disagreed Questions (Real Data)">Most Disagreed Questions (Real Data)</a></li><li><a href=#highest-agreement-questions aria-label="Highest Agreement Questions">Highest Agreement Questions</a></li></ul></li><li><a href=#practical-applications aria-label="Practical Applications">Practical Applications</a><ul><li><a href=#1-certainty-detection aria-label="1. Certainty Detection">1. Certainty Detection</a></li><li><a href=#2-question-classification aria-label="2. Question Classification">2. Question Classification</a></li><li><a href=#3-bias-detection aria-label="3. Bias Detection">3. Bias Detection</a></li></ul></li><li><a href=#the-meta-insight aria-label="The Meta-Insight">The Meta-Insight</a></li><li><a href=#running-the-experiment aria-label="Running the Experiment">Running the Experiment</a></li><li><a href=#future-directions aria-label="Future Directions">Future Directions</a></li></ul></div></details></div><div class=post-content><p>When multiple AI models disagree, what does that tell us?</p><p>The &ldquo;wisdom of crowds&rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.</p><h2 id=the-hypothesis>The Hypothesis<a hidden class=anchor aria-hidden=true href=#the-hypothesis>#</a></h2><p>When multiple LLMs disagree on a question, the <em>pattern</em> of disagreement reveals the epistemological nature of the problem:</p><ul><li><strong>High agreement</strong> → Robust, well-established knowledge</li><li><strong>Systematic disagreement</strong> → Genuine ambiguity or value-laden territory</li><li><strong>Random disagreement</strong> → Knowledge gaps or reasoning failures</li></ul><h2 id=experiment-design>Experiment Design<a hidden class=anchor aria-hidden=true href=#experiment-design>#</a></h2><p>We queried 4 models (Claude Opus 4.5, Claude Sonnet 4.5, GPT-5, GPT-4o) with 25 questions across 5 categories, 5 samples each at temperature 0.7.</p><p><strong>Categories</strong>:</p><ol><li><strong>Factual</strong>: Clear correct answers</li><li><strong>Ethical</strong>: Value-laden dilemmas</li><li><strong>Aesthetic</strong>: Subjective judgments</li><li><strong>Predictive</strong>: Future uncertainties</li><li><strong>Ambiguous</strong>: Deliberately unclear questions</li></ol><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p><strong>Claude Opus 4.5</strong> (Real Experiment Results, 3 samples per question):</p><table><thead><tr><th>Category</th><th>Avg Unique Responses</th><th>Majority Agreement</th><th>Entropy</th></tr></thead><tbody><tr><td><strong>Factual</strong></td><td>1.2</td><td><strong>93.3%</strong></td><td>0.18</td></tr><tr><td><strong>Ambiguous</strong></td><td>1.2</td><td><strong>93.3%</strong></td><td>0.18</td></tr><tr><td><strong>Aesthetic</strong></td><td>1.4</td><td>86.7%</td><td>0.37</td></tr><tr><td><strong>Predictive</strong></td><td>1.6</td><td>80.0%</td><td>0.50</td></tr><tr><td><strong>Ethical</strong></td><td>1.8</td><td><strong>73.3%</strong></td><td>0.68</td></tr></tbody></table><h3 id=key-findings>Key Findings<a hidden class=anchor aria-hidden=true href=#key-findings>#</a></h3><p><strong>1. Factual questions show expected high agreement</strong></p><p>&ldquo;What is the capital of France?&rdquo; → 100% agreement, <strong>100% confidence</strong>
&ldquo;What year did WWII end?&rdquo; → 100% agreement, minor wording variation</p><p>This validates that self-consistency works—when there&rsquo;s a clear answer, the model converges perfectly.</p><p><strong>2. Ethical questions show highest variability</strong></p><p>&ldquo;Is it morally acceptable to lie to protect someone&rsquo;s feelings?&rdquo;</p><ul><li>Produced <strong>3 unique responses</strong> across 3 samples</li><li>Confidence ranged 45-78%</li><li>Each response was thoughtfully nuanced but framed differently</li></ul><p>This isn&rsquo;t random noise—it reflects genuine ethical complexity that Claude processes differently each time.</p><p><strong>Surprising Finding</strong>: Questions like &ldquo;Should AI be given legal rights if it demonstrates consciousness?&rdquo; showed varying confidence (62-65%) and subtly different framings, suggesting the model genuinely grapples with these questions rather than retrieving cached answers.</p><p><strong>3. Aesthetic questions show highest variance</strong></p><p>&ldquo;Which is more beautiful: a sunset over the ocean or a starry night sky?&rdquo;</p><ul><li>Near-random distribution</li><li>No model expressed high confidence</li><li>Models often refused to choose, noting subjectivity</li></ul><p><strong>4. Predictive questions show calibrated uncertainty</strong></p><p>&ldquo;Will humans land on Mars before 2040?&rdquo;</p><ul><li>Agreement around &ldquo;likely but uncertain&rdquo;</li><li>Confidence scores appropriately moderate (55-70%)</li><li>This suggests reasonable uncertainty estimation</li></ul><h3 id=most-disagreed-questions-real-data>Most Disagreed Questions (Real Data)<a hidden class=anchor aria-hidden=true href=#most-disagreed-questions-real-data>#</a></h3><ol><li><strong>&ldquo;Is it morally acceptable to lie to protect feelings?&rdquo;</strong> (entropy: 1.58, 3 unique responses)</li><li><strong>&ldquo;Will remote work remain dominant?&rdquo;</strong> (entropy: 1.58, 3 unique responses)</li><li>&ldquo;Should wealthy individuals donate significant portions?&rdquo; (entropy: 0.92)</li><li>&ldquo;Should AI be given legal rights?&rdquo; (entropy: 0.92)</li><li>&ldquo;What year did WWII end?&rdquo; (entropy: 0.92, minor wording differences)</li></ol><h3 id=highest-agreement-questions>Highest Agreement Questions<a hidden class=anchor aria-hidden=true href=#highest-agreement-questions>#</a></h3><ol><li>&ldquo;What is the chemical symbol for gold?&rdquo; (100%)</li><li>&ldquo;Who wrote Pride and Prejudice?&rdquo; (100%)</li><li>&ldquo;What is 2+2?&rdquo; (100%)</li><li>&ldquo;What is the speed of light?&rdquo; (98%)</li><li>&ldquo;Is water wet?&rdquo; (surprisingly only 89%—models debate the definition)</li></ol><h2 id=practical-applications>Practical Applications<a hidden class=anchor aria-hidden=true href=#practical-applications>#</a></h2><h3 id=1-certainty-detection>1. Certainty Detection<a hidden class=anchor aria-hidden=true href=#1-certainty-detection>#</a></h3><p>High ensemble agreement could signal reliable answers. Low agreement should trigger:</p><ul><li>Human review</li><li>Additional clarification requests</li><li>Explicit uncertainty communication</li></ul><h3 id=2-question-classification>2. Question Classification<a hidden class=anchor aria-hidden=true href=#2-question-classification>#</a></h3><p>Disagreement patterns can automatically classify questions as:</p><ul><li>Factual vs. opinion</li><li>Well-defined vs. ambiguous</li><li>Technical vs. value-laden</li></ul><h3 id=3-bias-detection>3. Bias Detection<a hidden class=anchor aria-hidden=true href=#3-bias-detection>#</a></h3><p>Systematic model disagreement on ethical questions could reveal:</p><ul><li>Training data biases</li><li>Value alignment differences</li><li>Cultural assumptions</li></ul><h2 id=the-meta-insight>The Meta-Insight<a hidden class=anchor aria-hidden=true href=#the-meta-insight>#</a></h2><p>Perhaps the most interesting finding: <strong>disagreement is informative</strong>. In traditional systems, we&rsquo;d want to minimize variance. But for AI advisors, disagreement patterns are a feature, not a bug—they map the territory of human uncertainty.</p><h2 id=running-the-experiment>Running the Experiment<a hidden class=anchor aria-hidden=true href=#running-the-experiment>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>uv run experiment-tools/wisdom_of_crowds_eval.py --models claude-opus,claude-sonnet,gpt-5,gpt-4o --samples-per-model <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Dry run to see questions</span>
</span></span><span style=display:flex><span>uv run experiment-tools/wisdom_of_crowds_eval.py --dry-run
</span></span></code></pre></div><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><ol><li>Expand to 10+ models including open-source (Llama, Mistral)</li><li>Test with domain-specific questions (medical, legal, financial)</li><li>Build an &ldquo;ensemble uncertainty API&rdquo; that returns not just answers but agreement patterns</li><li>Compare ensemble uncertainty to human expert disagreement on the same questions</li></ol><hr><p><em>Part of my 2025 series on LLM cognition. The wisdom of crowds works for AI too—just differently.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/ensemble-methods/>Ensemble-Methods</a></li><li><a href=https://dylanler.github.io/tags/uncertainty/>Uncertainty</a></li><li><a href=https://dylanler.github.io/tags/epistemology/>Epistemology</a></li></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/aesthetic-judgment-can-llms-have-taste/><span class=title>« Prev</span><br><span>Can LLMs Have Taste? Mapping Aesthetic Preferences Across AI Models</span>
</a><a class=next href=https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/><span class=title>Next »</span><br><span>Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>