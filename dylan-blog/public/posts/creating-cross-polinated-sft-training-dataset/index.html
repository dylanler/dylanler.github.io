<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination | Dylan Ler</title>
<meta name=keywords content><meta name=description content="In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model&rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there&rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.
This blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination"><meta property="og:description" content="In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model&rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there&rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.
This blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-13T03:41:39-07:00"><meta property="article:modified_time" content="2025-03-13T03:41:39-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination"><meta name=twitter:description content="In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model&rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there&rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.
This blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination","item":"https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination","name":"Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination","description":"In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model\u0026rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there\u0026rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.\nThis blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history.","keywords":[],"articleBody":"In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model’s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there’s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.\nThis blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history. The goal is to create embedding links in the LLM’s weights that enable it to recombine knowledge in novel ways, essentially teaching the model the epistemology of how knowledge forms and interconnects.\nWhy Cross-Pollinate Knowledge Domains? Research has consistently shown that increased diversity in training data improves cross-domain knowledge and downstream generalization in large language models. For example, The Pile dataset (825 GB from 22 diverse sources) yielded models with stronger broad knowledge than those trained on single-source data.\nHowever, simply including diverse texts isn’t enough. By deliberately aligning and connecting concepts across domains, we can:\nTeach analogical reasoning: Help models understand how concepts in one domain might map to another Encourage novel insights: Create neural pathways that facilitate unexpected but valuable connections Develop epistemological understanding: Help models grasp how knowledge is structured and interconnected across fields Reduce domain isolation: Prevent the model from treating knowledge areas as completely separate entities The Cross-Pollination Process Let’s break down the process of creating this specialized dataset:\n1. Extracting Textual Data from Textbooks The first step is obtaining raw text from source textbooks. Depending on the format, you have several options:\nFor Digital Textbooks (PDF, EPUB) import fitz # PyMuPDF import re def extract_text_from_pdf(pdf_path): doc = fitz.open(pdf_path) full_text = \"\" for page in doc: full_text += page.get_text() return full_text # Extract text from math and history textbooks math_text = extract_text_from_pdf(\"math_textbook.pdf\") history_text = extract_text_from_pdf(\"history_textbook.pdf\") # Clean and preprocess the text def clean_text(text): # Remove page numbers text = re.sub(r'\\n\\d+\\n', '\\n', text) # Remove headers/footers (customize based on your textbooks) text = re.sub(r'Chapter \\d+.*\\n', '', text) # Normalize whitespace text = re.sub(r'\\s+', ' ', text) return text math_text = clean_text(math_text) history_text = clean_text(history_text) For Scanned Books (Images) from PIL import Image import pytesseract def extract_text_from_image(image_path): image = Image.open(image_path) text = pytesseract.image_to_string(image) return text # Process multiple pages history_text = \"\" for i in range(1, 100): # Adjust range based on number of pages page_text = extract_text_from_image(f\"history_page{i}.jpg\") history_text += page_text Segmenting into Manageable Units After extraction, segment the text into logical units for easier processing:\ndef segment_text(text): # Split by paragraphs (double newlines) sections = text.split(\"\\n\\n\") # Filter out very short sections (likely headers, page numbers, etc.) sections = [s for s in sections if len(s.split()) \u003e 15] return sections math_sections = segment_text(math_text) history_sections = segment_text(history_text) 2. Aligning and Cross-Pollinating Content This is the core of our approach. We need to find meaningful connections between content in different domains.\nMethod 1: Entity-Based Alignment Find sections that mention the same entities (people, places, concepts) across domains:\nimport spacy # Load NLP model nlp = spacy.load(\"en_core_web_lg\") def extract_key_entities(sections): entities = {} for i, section in enumerate(sections): doc = nlp(section) for ent in doc.ents: if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"EVENT\", \"WORK_OF_ART\"]: if ent.text not in entities: entities[ent.text] = [] entities[ent.text].append(i) return entities # Extract entities from both domains math_entities = extract_key_entities(math_sections) history_entities = extract_key_entities(history_sections) # Find overlapping entities common_entities = set(math_entities.keys()) \u0026 set(history_entities.keys()) # Create paired sections based on common entities entity_based_pairs = [] for entity in common_entities: for math_idx in math_entities[entity]: for history_idx in history_entities[entity]: entity_based_pairs.append({ \"math_section\": math_sections[math_idx], \"history_section\": history_sections[history_idx], \"linking_entity\": entity }) Method 2: Semantic Similarity Matching Even when specific entities don’t match, we can find conceptually similar passages:\nfrom sklearn.feature_extraction.text import TfidfVectorizer import numpy as np # Create TF-IDF vectors for all sections all_sections = math_sections + history_sections vectorizer = TfidfVectorizer(max_df=0.8, stop_words='english') tfidf = vectorizer.fit_transform(all_sections) # Split vectors by domain math_vecs = tfidf[:len(math_sections)] history_vecs = tfidf[len(math_sections):] # Find similar sections across domains similarity_based_pairs = [] similarity_threshold = 0.1 # Adjust based on your needs for i, math_vec in enumerate(math_vecs): # Calculate similarity between this math section and all history sections similarities = (history_vecs * math_vec.T).toarray().flatten() # Find top matches top_indices = np.argsort(similarities)[-3:] # Get top 3 matches for idx in top_indices: sim_score = similarities[idx] if sim_score \u003e= similarity_threshold: similarity_based_pairs.append({ \"math_section\": math_sections[i], \"history_section\": history_sections[idx], \"similarity_score\": sim_score }) Method 3: Using Advanced Embeddings For more sophisticated semantic matching, use transformer-based embeddings:\nfrom sentence_transformers import SentenceTransformer from sklearn.metrics.pairwise import cosine_similarity # Load pre-trained model model = SentenceTransformer('all-MiniLM-L6-v2') # Generate embeddings math_embeddings = model.encode(math_sections) history_embeddings = model.encode(history_sections) # Find similar sections transformer_based_pairs = [] for i, math_emb in enumerate(math_embeddings): # Calculate similarities similarities = cosine_similarity([math_emb], history_embeddings)[0] # Find top matches top_indices = np.argsort(similarities)[-5:] # Top 5 matches for idx in reversed(top_indices): sim_score = similarities[idx] if sim_score \u003e= 0.5: # Higher threshold for better quality transformer_based_pairs.append({ \"math_section\": math_sections[i], \"history_section\": history_sections[idx], \"similarity_score\": sim_score }) 3. Constructing the Cross-Pollinated Dataset Now that we have paired sections, we need to format them for training:\nFormat 1: Combined Expository Text def create_combined_text(math_section, history_section, linking_term=None): if linking_term: connector = f\"The concept of {linking_term} appears in both mathematics and history. \" else: connector = \"This concept has interesting parallels in mathematics and history. \" combined = f\"In mathematics: {math_section}\\n\\n{connector}\\n\\nIn history: {history_section}\" return combined # Create combined texts from our pairs dataset_entries = [] # From entity-based pairs for pair in entity_based_pairs: combined = create_combined_text( pair[\"math_section\"], pair[\"history_section\"], pair[\"linking_entity\"] ) dataset_entries.append(combined) # From similarity-based pairs for pair in similarity_based_pairs: combined = create_combined_text( pair[\"math_section\"], pair[\"history_section\"] ) dataset_entries.append(combined) Format 2: Question-Answer Pairs def create_qa_pairs(math_section, history_section, linking_term=None): if linking_term: question = f\"Explain the significance of '{linking_term}' in both mathematics and history.\" else: question = \"How might these concepts from different domains relate to each other?\" answer = f\"In mathematics: {math_section}\\n\\nIn history: {history_sections}\\n\\nThese concepts relate through their shared patterns of {linking_term or 'structure and development'}.\" return {\"instruction\": question, \"response\": answer} # Create QA pairs qa_dataset = [] for pair in entity_based_pairs[:100]: # Limit to first 100 for example qa_pair = create_qa_pairs( pair[\"math_section\"], pair[\"history_section\"], pair[\"linking_entity\"] ) qa_dataset.append(qa_pair) Format 3: Simulated Dialogues def create_dialogue(math_section, history_section, linking_term=None): if linking_term: intro = f\"A mathematician and historian discuss the concept of {linking_term}.\" else: intro = \"A mathematician and historian discuss connections between their fields.\" dialogue = f\"{intro}\\n\\nMathematician: {math_section}\\n\\nHistorian: Interestingly, we see similar patterns in history. {history_section}\\n\\nMathematician: That's fascinating! The parallel between these concepts shows how knowledge transcends disciplinary boundaries.\" return dialogue # Create dialogues dialogue_dataset = [] for pair in transformer_based_pairs[:50]: # Limit to first 50 for example dialogue = create_dialogue( pair[\"math_section\"], pair[\"history_section\"] ) dialogue_dataset.append(dialogue) 4. Balancing and Finalizing the Dataset To ensure a well-rounded dataset:\n# Combine all formats all_entries = dataset_entries + [item[\"instruction\"] + \"\\n\\n\" + item[\"response\"] for item in qa_dataset] + dialogue_dataset # Add some standalone domain-specific entries for balance all_entries.extend(math_sections[:100]) # Add 100 pure math sections all_entries.extend(history_sections[:100]) # Add 100 pure history sections # Shuffle the dataset import random random.shuffle(all_entries) # Save to Hugging Face dataset format from datasets import Dataset dataset = Dataset.from_dict({\"text\": all_entries}) # Preview a few examples print(dataset[:3][\"text\"]) # Save the dataset dataset.save_to_disk(\"cross_pollinated_dataset\") # Optionally push to Hugging Face Hub # dataset.push_to_hub(\"username/cross-pollinated-sft-dataset\") Best Practices for Effective Cross-Pollination When building your cross-pollinated dataset, keep these guidelines in mind:\nMaintain context clarity: Provide clear signals when switching between domains to avoid confusing the model.\nQuality over quantity: Focus on meaningful connections rather than forcing tenuous links.\nBalance domain representation: Ensure roughly equal representation of all domains in your final dataset.\nPreserve factual accuracy: Be careful not to distort facts when creating analogies or connections.\nInclude epistemological content: Add meta-content about how knowledge is formed in different fields.\nUse diverse formats: Mix standalone domain content, cross-domain pairs, QA formats, and dialogues.\nIntermix domains during training: Don’t segregate domains; shuffle examples to prevent the model from partitioning knowledge.\nEvaluating Cross-Domain Understanding After fine-tuning, test your model with prompts that require cross-domain reasoning:\n“Draw an analogy between calculus and the Industrial Revolution.” “How might Euler’s identity relate to Renaissance art?” “What mathematical principles could help understand the rise and fall of ancient civilizations?” A model trained on well-structured cross-pollinated data should produce insightful, linked answers that demonstrate it has learned to connect knowledge across domains.\nConclusion By deliberately cross-pollinating content from diverse textbooks, we can create SFT datasets that teach LLMs not just to memorize facts, but to understand the interconnected nature of knowledge. This approach encourages models to develop a more holistic understanding of information, enabling them to make novel connections and generate more insightful responses.\nThe code provided in this post offers a starting point for creating your own cross-pollinated dataset. The specific domains can be expanded beyond mathematics and history to include science, literature, philosophy, or any other fields you wish to connect. The key is to create meaningful bridges between domains that encourage the model to develop a unified understanding of knowledge.\nBy training models to see connections across traditionally separate domains, we move closer to AI systems that can reason more like humans do—drawing from diverse knowledge sources to generate novel insights and solve complex problems.\nReferences Gao et al., “The Pile: An 800GB Dataset of Diverse Text for Language Modeling.” (2020) SciPhi Project, “Textbooks are All You Need – A Library of Alexandria for LLMs.” (2023) Li et al., “CulturePark: Boosting Cross-cultural Understanding in LLMs.” (2024) Yuan et al., “ANALOGYKB: Unlocking Analogical Reasoning of LMs with a Million-scale Knowledge Base.” (2024) ","wordCount":"1572","inLanguage":"en","datePublished":"2025-03-13T03:41:39-07:00","dateModified":"2025-03-13T03:41:39-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/creating-cross-polinated-sft-training-dataset/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Creating Cross-Pollinated SFT Training Dataset for Novel Knowledge Recombination</h1><div class=post-meta><span title='2025-03-13 03:41:39 -0700 PDT'>March 13, 2025</span>&nbsp;·&nbsp;8 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#why-cross-pollinate-knowledge-domains aria-label="Why Cross-Pollinate Knowledge Domains?">Why Cross-Pollinate Knowledge Domains?</a></li><li><a href=#the-cross-pollination-process aria-label="The Cross-Pollination Process">The Cross-Pollination Process</a><ul><li><a href=#1-extracting-textual-data-from-textbooks aria-label="1. Extracting Textual Data from Textbooks">1. Extracting Textual Data from Textbooks</a><ul><li><a href=#for-digital-textbooks-pdf-epub aria-label="For Digital Textbooks (PDF, EPUB)">For Digital Textbooks (PDF, EPUB)</a></li><li><a href=#for-scanned-books-images aria-label="For Scanned Books (Images)">For Scanned Books (Images)</a></li><li><a href=#segmenting-into-manageable-units aria-label="Segmenting into Manageable Units">Segmenting into Manageable Units</a></li></ul></li><li><a href=#2-aligning-and-cross-pollinating-content aria-label="2. Aligning and Cross-Pollinating Content">2. Aligning and Cross-Pollinating Content</a><ul><li><a href=#method-1-entity-based-alignment aria-label="Method 1: Entity-Based Alignment">Method 1: Entity-Based Alignment</a></li><li><a href=#method-2-semantic-similarity-matching aria-label="Method 2: Semantic Similarity Matching">Method 2: Semantic Similarity Matching</a></li><li><a href=#method-3-using-advanced-embeddings aria-label="Method 3: Using Advanced Embeddings">Method 3: Using Advanced Embeddings</a></li></ul></li><li><a href=#3-constructing-the-cross-pollinated-dataset aria-label="3. Constructing the Cross-Pollinated Dataset">3. Constructing the Cross-Pollinated Dataset</a><ul><li><a href=#format-1-combined-expository-text aria-label="Format 1: Combined Expository Text">Format 1: Combined Expository Text</a></li><li><a href=#format-2-question-answer-pairs aria-label="Format 2: Question-Answer Pairs">Format 2: Question-Answer Pairs</a></li><li><a href=#format-3-simulated-dialogues aria-label="Format 3: Simulated Dialogues">Format 3: Simulated Dialogues</a></li></ul></li><li><a href=#4-balancing-and-finalizing-the-dataset aria-label="4. Balancing and Finalizing the Dataset">4. Balancing and Finalizing the Dataset</a></li></ul></li><li><a href=#best-practices-for-effective-cross-pollination aria-label="Best Practices for Effective Cross-Pollination">Best Practices for Effective Cross-Pollination</a></li><li><a href=#evaluating-cross-domain-understanding aria-label="Evaluating Cross-Domain Understanding">Evaluating Cross-Domain Understanding</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>In the realm of large language models (LLMs), the quality and diversity of training data significantly impact a model&rsquo;s ability to generate creative, insightful responses. While traditional training approaches often treat different knowledge domains as separate silos, there&rsquo;s a compelling opportunity to create more versatile models by deliberately cross-pollinating knowledge across domains.</p><p>This blog post explores a methodology for creating a specialized Supervised Fine-Tuning (SFT) dataset that deliberately bridges diverse knowledge domains—specifically, how to extract, align, and combine content from textbooks of vastly different genres such as mathematics and history. The goal is to create embedding links in the LLM&rsquo;s weights that enable it to recombine knowledge in novel ways, essentially teaching the model the epistemology of how knowledge forms and interconnects.</p><h2 id=why-cross-pollinate-knowledge-domains>Why Cross-Pollinate Knowledge Domains?<a hidden class=anchor aria-hidden=true href=#why-cross-pollinate-knowledge-domains>#</a></h2><p>Research has consistently shown that increased diversity in training data improves cross-domain knowledge and downstream generalization in large language models. For example, The Pile dataset (825 GB from 22 diverse sources) yielded models with stronger broad knowledge than those trained on single-source data.</p><p>However, simply including diverse texts isn&rsquo;t enough. By deliberately aligning and connecting concepts across domains, we can:</p><ol><li><strong>Teach analogical reasoning</strong>: Help models understand how concepts in one domain might map to another</li><li><strong>Encourage novel insights</strong>: Create neural pathways that facilitate unexpected but valuable connections</li><li><strong>Develop epistemological understanding</strong>: Help models grasp how knowledge is structured and interconnected across fields</li><li><strong>Reduce domain isolation</strong>: Prevent the model from treating knowledge areas as completely separate entities</li></ol><h2 id=the-cross-pollination-process>The Cross-Pollination Process<a hidden class=anchor aria-hidden=true href=#the-cross-pollination-process>#</a></h2><p>Let&rsquo;s break down the process of creating this specialized dataset:</p><h3 id=1-extracting-textual-data-from-textbooks>1. Extracting Textual Data from Textbooks<a hidden class=anchor aria-hidden=true href=#1-extracting-textual-data-from-textbooks>#</a></h3><p>The first step is obtaining raw text from source textbooks. Depending on the format, you have several options:</p><h4 id=for-digital-textbooks-pdf-epub>For Digital Textbooks (PDF, EPUB)<a hidden class=anchor aria-hidden=true href=#for-digital-textbooks-pdf-epub>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> fitz  <span style=color:#75715e># PyMuPDF</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_text_from_pdf</span>(pdf_path):
</span></span><span style=display:flex><span>    doc <span style=color:#f92672>=</span> fitz<span style=color:#f92672>.</span>open(pdf_path)
</span></span><span style=display:flex><span>    full_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> page <span style=color:#f92672>in</span> doc:
</span></span><span style=display:flex><span>        full_text <span style=color:#f92672>+=</span> page<span style=color:#f92672>.</span>get_text()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> full_text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Extract text from math and history textbooks</span>
</span></span><span style=display:flex><span>math_text <span style=color:#f92672>=</span> extract_text_from_pdf(<span style=color:#e6db74>&#34;math_textbook.pdf&#34;</span>)
</span></span><span style=display:flex><span>history_text <span style=color:#f92672>=</span> extract_text_from_pdf(<span style=color:#e6db74>&#34;history_textbook.pdf&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Clean and preprocess the text</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>clean_text</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Remove page numbers</span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\n\d+\n&#39;</span>, <span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, text)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Remove headers/footers (customize based on your textbooks)</span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;Chapter \d+.*\n&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, text)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Normalize whitespace</span>
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\s+&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>, text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>math_text <span style=color:#f92672>=</span> clean_text(math_text)
</span></span><span style=display:flex><span>history_text <span style=color:#f92672>=</span> clean_text(history_text)
</span></span></code></pre></div><h4 id=for-scanned-books-images>For Scanned Books (Images)<a hidden class=anchor aria-hidden=true href=#for-scanned-books-images>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pytesseract
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_text_from_image</span>(image_path):
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(image_path)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> pytesseract<span style=color:#f92672>.</span>image_to_string(image)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Process multiple pages</span>
</span></span><span style=display:flex><span>history_text <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>100</span>):  <span style=color:#75715e># Adjust range based on number of pages</span>
</span></span><span style=display:flex><span>    page_text <span style=color:#f92672>=</span> extract_text_from_image(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;history_page</span><span style=color:#e6db74>{</span>i<span style=color:#e6db74>}</span><span style=color:#e6db74>.jpg&#34;</span>)
</span></span><span style=display:flex><span>    history_text <span style=color:#f92672>+=</span> page_text
</span></span></code></pre></div><h4 id=segmenting-into-manageable-units>Segmenting into Manageable Units<a hidden class=anchor aria-hidden=true href=#segmenting-into-manageable-units>#</a></h4><p>After extraction, segment the text into logical units for easier processing:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>segment_text</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Split by paragraphs (double newlines)</span>
</span></span><span style=display:flex><span>    sections <span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Filter out very short sections (likely headers, page numbers, etc.)</span>
</span></span><span style=display:flex><span>    sections <span style=color:#f92672>=</span> [s <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> sections <span style=color:#66d9ef>if</span> len(s<span style=color:#f92672>.</span>split()) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>15</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sections
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>math_sections <span style=color:#f92672>=</span> segment_text(math_text)
</span></span><span style=display:flex><span>history_sections <span style=color:#f92672>=</span> segment_text(history_text)
</span></span></code></pre></div><h3 id=2-aligning-and-cross-pollinating-content>2. Aligning and Cross-Pollinating Content<a hidden class=anchor aria-hidden=true href=#2-aligning-and-cross-pollinating-content>#</a></h3><p>This is the core of our approach. We need to find meaningful connections between content in different domains.</p><h4 id=method-1-entity-based-alignment>Method 1: Entity-Based Alignment<a hidden class=anchor aria-hidden=true href=#method-1-entity-based-alignment>#</a></h4><p>Find sections that mention the same entities (people, places, concepts) across domains:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> spacy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load NLP model</span>
</span></span><span style=display:flex><span>nlp <span style=color:#f92672>=</span> spacy<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;en_core_web_lg&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>extract_key_entities</span>(sections):
</span></span><span style=display:flex><span>    entities <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, section <span style=color:#f92672>in</span> enumerate(sections):
</span></span><span style=display:flex><span>        doc <span style=color:#f92672>=</span> nlp(section)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> ent <span style=color:#f92672>in</span> doc<span style=color:#f92672>.</span>ents:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> ent<span style=color:#f92672>.</span>label_ <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#34;PERSON&#34;</span>, <span style=color:#e6db74>&#34;ORG&#34;</span>, <span style=color:#e6db74>&#34;GPE&#34;</span>, <span style=color:#e6db74>&#34;EVENT&#34;</span>, <span style=color:#e6db74>&#34;WORK_OF_ART&#34;</span>]:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> ent<span style=color:#f92672>.</span>text <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> entities:
</span></span><span style=display:flex><span>                    entities[ent<span style=color:#f92672>.</span>text] <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>                entities[ent<span style=color:#f92672>.</span>text]<span style=color:#f92672>.</span>append(i)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> entities
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Extract entities from both domains</span>
</span></span><span style=display:flex><span>math_entities <span style=color:#f92672>=</span> extract_key_entities(math_sections)
</span></span><span style=display:flex><span>history_entities <span style=color:#f92672>=</span> extract_key_entities(history_sections)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Find overlapping entities</span>
</span></span><span style=display:flex><span>common_entities <span style=color:#f92672>=</span> set(math_entities<span style=color:#f92672>.</span>keys()) <span style=color:#f92672>&amp;</span> set(history_entities<span style=color:#f92672>.</span>keys())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create paired sections based on common entities</span>
</span></span><span style=display:flex><span>entity_based_pairs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> entity <span style=color:#f92672>in</span> common_entities:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> math_idx <span style=color:#f92672>in</span> math_entities[entity]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> history_idx <span style=color:#f92672>in</span> history_entities[entity]:
</span></span><span style=display:flex><span>            entity_based_pairs<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;math_section&#34;</span>: math_sections[math_idx],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;history_section&#34;</span>: history_sections[history_idx],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;linking_entity&#34;</span>: entity
</span></span><span style=display:flex><span>            })
</span></span></code></pre></div><h4 id=method-2-semantic-similarity-matching>Method 2: Semantic Similarity Matching<a hidden class=anchor aria-hidden=true href=#method-2-semantic-similarity-matching>#</a></h4><p>Even when specific entities don&rsquo;t match, we can find conceptually similar passages:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> TfidfVectorizer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create TF-IDF vectors for all sections</span>
</span></span><span style=display:flex><span>all_sections <span style=color:#f92672>=</span> math_sections <span style=color:#f92672>+</span> history_sections
</span></span><span style=display:flex><span>vectorizer <span style=color:#f92672>=</span> TfidfVectorizer(max_df<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>, stop_words<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;english&#39;</span>)
</span></span><span style=display:flex><span>tfidf <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(all_sections)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Split vectors by domain</span>
</span></span><span style=display:flex><span>math_vecs <span style=color:#f92672>=</span> tfidf[:len(math_sections)]
</span></span><span style=display:flex><span>history_vecs <span style=color:#f92672>=</span> tfidf[len(math_sections):]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Find similar sections across domains</span>
</span></span><span style=display:flex><span>similarity_based_pairs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>similarity_threshold <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>  <span style=color:#75715e># Adjust based on your needs</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, math_vec <span style=color:#f92672>in</span> enumerate(math_vecs):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Calculate similarity between this math section and all history sections</span>
</span></span><span style=display:flex><span>    similarities <span style=color:#f92672>=</span> (history_vecs <span style=color:#f92672>*</span> math_vec<span style=color:#f92672>.</span>T)<span style=color:#f92672>.</span>toarray()<span style=color:#f92672>.</span>flatten()
</span></span><span style=display:flex><span>    <span style=color:#75715e># Find top matches</span>
</span></span><span style=display:flex><span>    top_indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(similarities)[<span style=color:#f92672>-</span><span style=color:#ae81ff>3</span>:]  <span style=color:#75715e># Get top 3 matches</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> top_indices:
</span></span><span style=display:flex><span>        sim_score <span style=color:#f92672>=</span> similarities[idx]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> sim_score <span style=color:#f92672>&gt;=</span> similarity_threshold:
</span></span><span style=display:flex><span>            similarity_based_pairs<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;math_section&#34;</span>: math_sections[i],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;history_section&#34;</span>: history_sections[idx],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;similarity_score&#34;</span>: sim_score
</span></span><span style=display:flex><span>            })
</span></span></code></pre></div><h4 id=method-3-using-advanced-embeddings>Method 3: Using Advanced Embeddings<a hidden class=anchor aria-hidden=true href=#method-3-using-advanced-embeddings>#</a></h4><p>For more sophisticated semantic matching, use transformer-based embeddings:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sentence_transformers <span style=color:#f92672>import</span> SentenceTransformer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics.pairwise <span style=color:#f92672>import</span> cosine_similarity
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load pre-trained model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> SentenceTransformer(<span style=color:#e6db74>&#39;all-MiniLM-L6-v2&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate embeddings</span>
</span></span><span style=display:flex><span>math_embeddings <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>encode(math_sections)
</span></span><span style=display:flex><span>history_embeddings <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>encode(history_sections)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Find similar sections</span>
</span></span><span style=display:flex><span>transformer_based_pairs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, math_emb <span style=color:#f92672>in</span> enumerate(math_embeddings):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Calculate similarities</span>
</span></span><span style=display:flex><span>    similarities <span style=color:#f92672>=</span> cosine_similarity([math_emb], history_embeddings)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># Find top matches</span>
</span></span><span style=display:flex><span>    top_indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argsort(similarities)[<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>:]  <span style=color:#75715e># Top 5 matches</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> reversed(top_indices):
</span></span><span style=display:flex><span>        sim_score <span style=color:#f92672>=</span> similarities[idx]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> sim_score <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0.5</span>:  <span style=color:#75715e># Higher threshold for better quality</span>
</span></span><span style=display:flex><span>            transformer_based_pairs<span style=color:#f92672>.</span>append({
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;math_section&#34;</span>: math_sections[i],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;history_section&#34;</span>: history_sections[idx],
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;similarity_score&#34;</span>: sim_score
</span></span><span style=display:flex><span>            })
</span></span></code></pre></div><h3 id=3-constructing-the-cross-pollinated-dataset>3. Constructing the Cross-Pollinated Dataset<a hidden class=anchor aria-hidden=true href=#3-constructing-the-cross-pollinated-dataset>#</a></h3><p>Now that we have paired sections, we need to format them for training:</p><h4 id=format-1-combined-expository-text>Format 1: Combined Expository Text<a hidden class=anchor aria-hidden=true href=#format-1-combined-expository-text>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_combined_text</span>(math_section, history_section, linking_term<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> linking_term:
</span></span><span style=display:flex><span>        connector <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;The concept of </span><span style=color:#e6db74>{</span>linking_term<span style=color:#e6db74>}</span><span style=color:#e6db74> appears in both mathematics and history. &#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        connector <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;This concept has interesting parallels in mathematics and history. &#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    combined <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;In mathematics: </span><span style=color:#e6db74>{</span>math_section<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>{</span>connector<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>In history: </span><span style=color:#e6db74>{</span>history_section<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> combined
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create combined texts from our pairs</span>
</span></span><span style=display:flex><span>dataset_entries <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># From entity-based pairs</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> pair <span style=color:#f92672>in</span> entity_based_pairs:
</span></span><span style=display:flex><span>    combined <span style=color:#f92672>=</span> create_combined_text(
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;math_section&#34;</span>], 
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;history_section&#34;</span>],
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;linking_entity&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    dataset_entries<span style=color:#f92672>.</span>append(combined)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># From similarity-based pairs</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> pair <span style=color:#f92672>in</span> similarity_based_pairs:
</span></span><span style=display:flex><span>    combined <span style=color:#f92672>=</span> create_combined_text(
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;math_section&#34;</span>], 
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;history_section&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    dataset_entries<span style=color:#f92672>.</span>append(combined)
</span></span></code></pre></div><h4 id=format-2-question-answer-pairs>Format 2: Question-Answer Pairs<a hidden class=anchor aria-hidden=true href=#format-2-question-answer-pairs>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_qa_pairs</span>(math_section, history_section, linking_term<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> linking_term:
</span></span><span style=display:flex><span>        question <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Explain the significance of &#39;</span><span style=color:#e6db74>{</span>linking_term<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39; in both mathematics and history.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        question <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;How might these concepts from different domains relate to each other?&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    answer <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;In mathematics: </span><span style=color:#e6db74>{</span>math_section<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>In history: </span><span style=color:#e6db74>{</span>history_sections<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>These concepts relate through their shared patterns of </span><span style=color:#e6db74>{</span>linking_term <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;structure and development&#39;</span><span style=color:#e6db74>}</span><span style=color:#e6db74>.&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;instruction&#34;</span>: question, <span style=color:#e6db74>&#34;response&#34;</span>: answer}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create QA pairs</span>
</span></span><span style=display:flex><span>qa_dataset <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> pair <span style=color:#f92672>in</span> entity_based_pairs[:<span style=color:#ae81ff>100</span>]:  <span style=color:#75715e># Limit to first 100 for example</span>
</span></span><span style=display:flex><span>    qa_pair <span style=color:#f92672>=</span> create_qa_pairs(
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;math_section&#34;</span>],
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;history_section&#34;</span>],
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;linking_entity&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    qa_dataset<span style=color:#f92672>.</span>append(qa_pair)
</span></span></code></pre></div><h4 id=format-3-simulated-dialogues>Format 3: Simulated Dialogues<a hidden class=anchor aria-hidden=true href=#format-3-simulated-dialogues>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>create_dialogue</span>(math_section, history_section, linking_term<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> linking_term:
</span></span><span style=display:flex><span>        intro <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;A mathematician and historian discuss the concept of </span><span style=color:#e6db74>{</span>linking_term<span style=color:#e6db74>}</span><span style=color:#e6db74>.&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        intro <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;A mathematician and historian discuss connections between their fields.&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    dialogue <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>intro<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>Mathematician: </span><span style=color:#e6db74>{</span>math_section<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>Historian: Interestingly, we see similar patterns in history. </span><span style=color:#e6db74>{</span>history_section<span style=color:#e6db74>}</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>Mathematician: That&#39;s fascinating! The parallel between these concepts shows how knowledge transcends disciplinary boundaries.&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> dialogue
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Create dialogues</span>
</span></span><span style=display:flex><span>dialogue_dataset <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> pair <span style=color:#f92672>in</span> transformer_based_pairs[:<span style=color:#ae81ff>50</span>]:  <span style=color:#75715e># Limit to first 50 for example</span>
</span></span><span style=display:flex><span>    dialogue <span style=color:#f92672>=</span> create_dialogue(
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;math_section&#34;</span>],
</span></span><span style=display:flex><span>        pair[<span style=color:#e6db74>&#34;history_section&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    dialogue_dataset<span style=color:#f92672>.</span>append(dialogue)
</span></span></code></pre></div><h3 id=4-balancing-and-finalizing-the-dataset>4. Balancing and Finalizing the Dataset<a hidden class=anchor aria-hidden=true href=#4-balancing-and-finalizing-the-dataset>#</a></h3><p>To ensure a well-rounded dataset:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Combine all formats</span>
</span></span><span style=display:flex><span>all_entries <span style=color:#f92672>=</span> dataset_entries <span style=color:#f92672>+</span> [item[<span style=color:#e6db74>&#34;instruction&#34;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>+</span> item[<span style=color:#e6db74>&#34;response&#34;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> qa_dataset] <span style=color:#f92672>+</span> dialogue_dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Add some standalone domain-specific entries for balance</span>
</span></span><span style=display:flex><span>all_entries<span style=color:#f92672>.</span>extend(math_sections[:<span style=color:#ae81ff>100</span>])  <span style=color:#75715e># Add 100 pure math sections</span>
</span></span><span style=display:flex><span>all_entries<span style=color:#f92672>.</span>extend(history_sections[:<span style=color:#ae81ff>100</span>])  <span style=color:#75715e># Add 100 pure history sections</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Shuffle the dataset</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>shuffle(all_entries)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save to Hugging Face dataset format</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> Dataset
</span></span><span style=display:flex><span>dataset <span style=color:#f92672>=</span> Dataset<span style=color:#f92672>.</span>from_dict({<span style=color:#e6db74>&#34;text&#34;</span>: all_entries})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Preview a few examples</span>
</span></span><span style=display:flex><span>print(dataset[:<span style=color:#ae81ff>3</span>][<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Save the dataset</span>
</span></span><span style=display:flex><span>dataset<span style=color:#f92672>.</span>save_to_disk(<span style=color:#e6db74>&#34;cross_pollinated_dataset&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Optionally push to Hugging Face Hub</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dataset.push_to_hub(&#34;username/cross-pollinated-sft-dataset&#34;)</span>
</span></span></code></pre></div><h2 id=best-practices-for-effective-cross-pollination>Best Practices for Effective Cross-Pollination<a hidden class=anchor aria-hidden=true href=#best-practices-for-effective-cross-pollination>#</a></h2><p>When building your cross-pollinated dataset, keep these guidelines in mind:</p><ol><li><p><strong>Maintain context clarity</strong>: Provide clear signals when switching between domains to avoid confusing the model.</p></li><li><p><strong>Quality over quantity</strong>: Focus on meaningful connections rather than forcing tenuous links.</p></li><li><p><strong>Balance domain representation</strong>: Ensure roughly equal representation of all domains in your final dataset.</p></li><li><p><strong>Preserve factual accuracy</strong>: Be careful not to distort facts when creating analogies or connections.</p></li><li><p><strong>Include epistemological content</strong>: Add meta-content about how knowledge is formed in different fields.</p></li><li><p><strong>Use diverse formats</strong>: Mix standalone domain content, cross-domain pairs, QA formats, and dialogues.</p></li><li><p><strong>Intermix domains during training</strong>: Don&rsquo;t segregate domains; shuffle examples to prevent the model from partitioning knowledge.</p></li></ol><h2 id=evaluating-cross-domain-understanding>Evaluating Cross-Domain Understanding<a hidden class=anchor aria-hidden=true href=#evaluating-cross-domain-understanding>#</a></h2><p>After fine-tuning, test your model with prompts that require cross-domain reasoning:</p><ul><li>&ldquo;Draw an analogy between calculus and the Industrial Revolution.&rdquo;</li><li>&ldquo;How might Euler&rsquo;s identity relate to Renaissance art?&rdquo;</li><li>&ldquo;What mathematical principles could help understand the rise and fall of ancient civilizations?&rdquo;</li></ul><p>A model trained on well-structured cross-pollinated data should produce insightful, linked answers that demonstrate it has learned to connect knowledge across domains.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>By deliberately cross-pollinating content from diverse textbooks, we can create SFT datasets that teach LLMs not just to memorize facts, but to understand the interconnected nature of knowledge. This approach encourages models to develop a more holistic understanding of information, enabling them to make novel connections and generate more insightful responses.</p><p>The code provided in this post offers a starting point for creating your own cross-pollinated dataset. The specific domains can be expanded beyond mathematics and history to include science, literature, philosophy, or any other fields you wish to connect. The key is to create meaningful bridges between domains that encourage the model to develop a unified understanding of knowledge.</p><p>By training models to see connections across traditionally separate domains, we move closer to AI systems that can reason more like humans do—drawing from diverse knowledge sources to generate novel insights and solve complex problems.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Gao et al., &ldquo;The Pile: An 800GB Dataset of Diverse Text for Language Modeling.&rdquo; (2020)</li><li>SciPhi Project, &ldquo;Textbooks are All You Need – A Library of Alexandria for LLMs.&rdquo; (2023)</li><li>Li et al., &ldquo;CulturePark: Boosting Cross-cultural Understanding in LLMs.&rdquo; (2024)</li><li>Yuan et al., &ldquo;ANALOGYKB: Unlocking Analogical Reasoning of LMs with a Million-scale Knowledge Base.&rdquo; (2024)</li></ol></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/><span class=title>« Prev</span><br><span>Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty</span>
</a><a class=next href=https://dylanler.github.io/posts/creating-a-video-dataset-with-precise-camera-movement-prompt/><span class=title>Next »</span><br><span>Creating a Video Dataset With Precise Camera Movement Prompts</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>