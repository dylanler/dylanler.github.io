<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations | Dylan Ler</title>
<meta name=keywords content="AI,LLM,fine-tuning,LoRA,code-generation,physics-simulation,education,synthetic-data"><meta name=description content="What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &ndash; they&rsquo;re all racing to hundreds of billions of parameters."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations"><meta property="og:description" content="What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &ndash; they&rsquo;re all racing to hundreds of billions of parameters."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-04T09:30:00-08:00"><meta property="article:modified_time" content="2026-02-04T09:30:00-08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations"><meta name=twitter:description content="What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &ndash; they&rsquo;re all racing to hundreds of billions of parameters."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations","item":"https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations","name":"Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations","description":"What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?\nYou get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.\nThe Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra \u0026ndash; they\u0026rsquo;re all racing to hundreds of billions of parameters.","keywords":["AI","LLM","fine-tuning","LoRA","code-generation","physics-simulation","education","synthetic-data"],"articleBody":"What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?\nYou get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.\nThe Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra – they’re all racing to hundreds of billions of parameters. But there’s a parallel question that doesn’t get enough attention: how small can a model be and still do something genuinely useful?\nThis experiment answers that for a specific domain: generating p5.js animations that teach physics and science concepts. Think interactive simulations of gravity, wave interference, circuit flow, volcano eruptions – the kind of visual explanations that make abstract physics click for students.\nThe pipeline: use Claude Opus as a “teacher” to generate 1,036 high-quality examples, then distill that knowledge into Qwen3-0.6B (792M parameters) using LoRA fine-tuning. The entire training takes 2 minutes and 51 seconds on 4x A100 GPUs.\nWhy p5.js? p5.js hits a sweet spot for AI-generated educational content:\nSelf-contained: A single file runs in any browser. No build systems, no dependencies. Visual by default: Every sketch has a setup() and draw() loop – the model must think in terms of animation frames. Physics-native: Vectors, forces, particles, collisions – p5.js’s API maps naturally onto physics concepts. Immediately verifiable: Run the output, see if the physics looks right. No ambiguous evaluation. The constraint of a 600x400 canvas with setup()/draw() structure also gives the model a consistent “grammar” to learn – which matters enormously when your model has only 792M parameters.\nThe Dataset: 100 Claude Agents Working in Parallel The most interesting engineering decision was how to generate the training data. Rather than manually curating examples or scraping the web, the pipeline spawns 100+ parallel Claude Opus agents, each assigned a subset of 124 K-12 science topics.\nEach agent generates 10 variations per topic, varying:\nVisual style: Particle-based, diagram-based, simulation-based, story-based Interactivity: Mouse-driven, automatic, key-press controlled Complexity: From K-2 (colored balls falling) to 11-12 (double-slit interference) Aesthetics: Color schemes, label placement, animation speed The result: 1,036 validated examples covering everything from simple gravity demonstrations to nuclear fission animations. Code lengths range from 695 to 9,090 characters, with an average of 2,845 characters per sketch.\nTopic Coverage The breadth is impressive:\nDomain Example Topics Forces \u0026 Motion Gravity, Newton’s 3 laws, friction, projectile motion, circular motion Waves Sound propagation, Doppler effect, interference, standing waves Light Reflection, refraction, prisms, double-slit experiment, rainbows Electricity Circuits, series/parallel, Ohm’s law, generators, static electricity Space Moon phases, eclipses, orbital mechanics, black holes, star life cycles Chemistry Atomic structure, chemical reactions, diffusion, phase changes Biology Photosynthesis, mitosis, circulation, food chains, respiration Modern Physics Radioactive decay, nuclear fission/fusion, special relativity What strikes me is that this is essentially curriculum-complete for K-12 science. A single sub-billion-parameter model, if it works, could generate visual explanations for virtually any concept a student encounters.\nTraining: LoRA in Under 3 Minutes The training setup is deliberately efficient:\nBase model: Qwen3-0.6B Method: LoRA (rank 64, alpha 128) Trainable parameters: 40.4M (5.1% of the model) Hardware: 4x A100 GPUs with bf16 mixed precision Training time: 171.87 seconds (2.9 minutes) Effective batch size: 32 (4 per device x 2 gradient accumulation x 4 GPUs) Loss Progression Step Loss Token Accuracy 10 0.909 77.0% 30 0.621 82.3% 50 0.549 84.0% 70 0.510 84.9% 93 (final) 0.592 85.6% The loss curve tells an interesting story. The model converges rapidly – 45% relative loss reduction in under 3 minutes – and the evaluation loss tracks training loss closely, suggesting good generalization rather than memorization.\n85.6% token accuracy means the model correctly predicts the next token in p5.js code ~86% of the time. For code generation, this is remarkably high. The structured nature of p5.js (consistent function signatures, canvas operations, physics math patterns) gives the model strong priors to work with.\nWhat This Actually Means 1. Knowledge Distillation Works for Code Generation The core insight: Claude Opus (hundreds of billions of parameters, massive training data) can “teach” Qwen3-0.6B (792M parameters) to generate domain-specific code. The small model doesn’t need to understand physics from first principles – it needs to learn the mapping from natural language descriptions to p5.js patterns.\nThis is closer to how human students learn programming: you don’t derive JavaScript from mathematical axioms, you see patterns and internalize them.\n2. Small Models + Narrow Domains = Surprising Capability A 0.6B model can’t write arbitrary code. But restrict the domain to “p5.js animations on a 600x400 canvas teaching K-12 physics” and suddenly the problem becomes tractable. The constraints reduce the output space dramatically:\nFixed canvas size Standard setup()/draw() structure Limited API surface (vectors, shapes, text, color) Predictable physics patterns (gravity = vy += 0.15, bounce = vy *= -0.8) This has implications for edge deployment. A 0.6B model runs on a phone, a Raspberry Pi, or embedded in a web app. You could have an offline-capable physics animation generator that works without internet access.\n3. Parallel Agent Generation Creates Better Data The 100-agent approach to dataset generation isn’t just faster – it produces more diverse data. Each agent independently varies its style, producing natural variation that a single sequential generation would struggle to match. The result is a dataset where the model learns multiple ways to visualize the same concept.\nThe Bigger Picture This experiment is a proof of concept for a broader pattern:\nLarge model generates domain-specific training data → Small model learns the domain → Deploy small model at edge/scale → Iterate with human feedback The economics are compelling. Training takes 3 minutes on rented GPUs (~$1). Inference runs on consumer hardware. And the model produces genuinely useful educational content.\nImagine this applied to:\nMath visualization: Interactive proofs and geometric constructions Chemistry: Molecular dynamics and reaction animations Music theory: Audio-visual representations of harmony and rhythm History: Animated timelines and interactive maps The pattern scales. The models shrink. The content gets better.\nRunning It Yourself The entire pipeline is open source:\ngit clone https://github.com/dylanler/qwen3-p5js-physics cd qwen3-p5js-physics uv sync # Generate dataset (needs ANTHROPIC_API_KEY) uv run python scripts/generate_dataset.py # Fine-tune on 4 GPUs uv run accelerate launch --config_file configs/accelerate_config.yaml scripts/train.py # Generate an animation uv run python scripts/inference.py \"Show me how gravity affects falling objects\" # Serve as API uv run python scripts/serve.py --port 8000 The served model exposes an OpenAI-compatible API, so you can drop it into any existing application.\nWhat I’d Do Next Human evaluation loop: Have actual teachers rate generated animations for accuracy and pedagogical value Multi-turn refinement: “Make the gravity stronger” / “Add a label showing velocity” Model scaling ladder: Compare 0.6B, 1.5B, 3B, 7B to find the accuracy/size sweet spot Domain expansion: Three.js for 3D physics, Manim for mathematical animations Student interaction data: Log which animations students actually find helpful and fine-tune on that signal The most exciting direction is closing the loop: students interact with generated animations, their engagement signals feed back into training, and the model gets better at explaining what students actually struggle with.\nSource code: github.com/dylanler/qwen3-p5js-physics\nThe smallest model in the lab might be the most useful one in the classroom.\n","wordCount":"1197","inLanguage":"en","datePublished":"2026-02-04T09:30:00-08:00","dateModified":"2026-02-04T09:30:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations</h1><div class=post-meta><span title='2026-02-04 09:30:00 -0800 PST'>February 4, 2026</span>&nbsp;·&nbsp;6 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-premise aria-label="The Premise">The Premise</a></li><li><a href=#why-p5js aria-label="Why p5.js?">Why p5.js?</a></li><li><a href=#the-dataset-100-claude-agents-working-in-parallel aria-label="The Dataset: 100 Claude Agents Working in Parallel">The Dataset: 100 Claude Agents Working in Parallel</a><ul><li><a href=#topic-coverage aria-label="Topic Coverage">Topic Coverage</a></li></ul></li><li><a href=#training-lora-in-under-3-minutes aria-label="Training: LoRA in Under 3 Minutes">Training: LoRA in Under 3 Minutes</a><ul><li><a href=#loss-progression aria-label="Loss Progression">Loss Progression</a></li></ul></li><li><a href=#what-this-actually-means aria-label="What This Actually Means">What This Actually Means</a><ul><li><a href=#1-knowledge-distillation-works-for-code-generation aria-label="1. Knowledge Distillation Works for Code Generation">1. Knowledge Distillation Works for Code Generation</a></li><li><a href=#2-small-models--narrow-domains--surprising-capability aria-label="2. Small Models + Narrow Domains = Surprising Capability">2. Small Models + Narrow Domains = Surprising Capability</a></li><li><a href=#3-parallel-agent-generation-creates-better-data aria-label="3. Parallel Agent Generation Creates Better Data">3. Parallel Agent Generation Creates Better Data</a></li></ul></li><li><a href=#the-bigger-picture aria-label="The Bigger Picture">The Bigger Picture</a></li><li><a href=#running-it-yourself aria-label="Running It Yourself">Running It Yourself</a></li><li><a href=#what-id-do-next aria-label="What I&rsquo;d Do Next">What I&rsquo;d Do Next</a></li></ul></div></details></div><div class=post-content><p>What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?</p><p>You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.</p><h2 id=the-premise>The Premise<a hidden class=anchor aria-hidden=true href=#the-premise>#</a></h2><p>LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &ndash; they&rsquo;re all racing to hundreds of billions of parameters. But there&rsquo;s a parallel question that doesn&rsquo;t get enough attention: <strong>how small can a model be and still do something genuinely useful?</strong></p><p>This experiment answers that for a specific domain: generating p5.js animations that teach physics and science concepts. Think interactive simulations of gravity, wave interference, circuit flow, volcano eruptions &ndash; the kind of visual explanations that make abstract physics click for students.</p><p>The pipeline: use Claude Opus as a &ldquo;teacher&rdquo; to generate 1,036 high-quality examples, then distill that knowledge into Qwen3-0.6B (792M parameters) using LoRA fine-tuning. The entire training takes <strong>2 minutes and 51 seconds</strong> on 4x A100 GPUs.</p><h2 id=why-p5js>Why p5.js?<a hidden class=anchor aria-hidden=true href=#why-p5js>#</a></h2><p>p5.js hits a sweet spot for AI-generated educational content:</p><ol><li><strong>Self-contained</strong>: A single file runs in any browser. No build systems, no dependencies.</li><li><strong>Visual by default</strong>: Every sketch has a <code>setup()</code> and <code>draw()</code> loop &ndash; the model must think in terms of animation frames.</li><li><strong>Physics-native</strong>: Vectors, forces, particles, collisions &ndash; p5.js&rsquo;s API maps naturally onto physics concepts.</li><li><strong>Immediately verifiable</strong>: Run the output, see if the physics looks right. No ambiguous evaluation.</li></ol><p>The constraint of a 600x400 canvas with <code>setup()</code>/<code>draw()</code> structure also gives the model a consistent &ldquo;grammar&rdquo; to learn &ndash; which matters enormously when your model has only 792M parameters.</p><h2 id=the-dataset-100-claude-agents-working-in-parallel>The Dataset: 100 Claude Agents Working in Parallel<a hidden class=anchor aria-hidden=true href=#the-dataset-100-claude-agents-working-in-parallel>#</a></h2><p>The most interesting engineering decision was how to generate the training data. Rather than manually curating examples or scraping the web, the pipeline spawns <strong>100+ parallel Claude Opus agents</strong>, each assigned a subset of 124 K-12 science topics.</p><p>Each agent generates 10 variations per topic, varying:</p><ul><li><strong>Visual style</strong>: Particle-based, diagram-based, simulation-based, story-based</li><li><strong>Interactivity</strong>: Mouse-driven, automatic, key-press controlled</li><li><strong>Complexity</strong>: From K-2 (colored balls falling) to 11-12 (double-slit interference)</li><li><strong>Aesthetics</strong>: Color schemes, label placement, animation speed</li></ul><p>The result: <strong>1,036 validated examples</strong> covering everything from simple gravity demonstrations to nuclear fission animations. Code lengths range from 695 to 9,090 characters, with an average of 2,845 characters per sketch.</p><h3 id=topic-coverage>Topic Coverage<a hidden class=anchor aria-hidden=true href=#topic-coverage>#</a></h3><p>The breadth is impressive:</p><table><thead><tr><th>Domain</th><th>Example Topics</th></tr></thead><tbody><tr><td><strong>Forces & Motion</strong></td><td>Gravity, Newton&rsquo;s 3 laws, friction, projectile motion, circular motion</td></tr><tr><td><strong>Waves</strong></td><td>Sound propagation, Doppler effect, interference, standing waves</td></tr><tr><td><strong>Light</strong></td><td>Reflection, refraction, prisms, double-slit experiment, rainbows</td></tr><tr><td><strong>Electricity</strong></td><td>Circuits, series/parallel, Ohm&rsquo;s law, generators, static electricity</td></tr><tr><td><strong>Space</strong></td><td>Moon phases, eclipses, orbital mechanics, black holes, star life cycles</td></tr><tr><td><strong>Chemistry</strong></td><td>Atomic structure, chemical reactions, diffusion, phase changes</td></tr><tr><td><strong>Biology</strong></td><td>Photosynthesis, mitosis, circulation, food chains, respiration</td></tr><tr><td><strong>Modern Physics</strong></td><td>Radioactive decay, nuclear fission/fusion, special relativity</td></tr></tbody></table><p>What strikes me is that this is essentially <strong>curriculum-complete</strong> for K-12 science. A single sub-billion-parameter model, if it works, could generate visual explanations for virtually any concept a student encounters.</p><h2 id=training-lora-in-under-3-minutes>Training: LoRA in Under 3 Minutes<a hidden class=anchor aria-hidden=true href=#training-lora-in-under-3-minutes>#</a></h2><p>The training setup is deliberately efficient:</p><ul><li><strong>Base model</strong>: Qwen3-0.6B</li><li><strong>Method</strong>: LoRA (rank 64, alpha 128)</li><li><strong>Trainable parameters</strong>: 40.4M (5.1% of the model)</li><li><strong>Hardware</strong>: 4x A100 GPUs with bf16 mixed precision</li><li><strong>Training time</strong>: 171.87 seconds (2.9 minutes)</li><li><strong>Effective batch size</strong>: 32 (4 per device x 2 gradient accumulation x 4 GPUs)</li></ul><h3 id=loss-progression>Loss Progression<a hidden class=anchor aria-hidden=true href=#loss-progression>#</a></h3><table><thead><tr><th>Step</th><th>Loss</th><th>Token Accuracy</th></tr></thead><tbody><tr><td>10</td><td>0.909</td><td>77.0%</td></tr><tr><td>30</td><td>0.621</td><td>82.3%</td></tr><tr><td>50</td><td>0.549</td><td>84.0%</td></tr><tr><td>70</td><td>0.510</td><td>84.9%</td></tr><tr><td>93 (final)</td><td>0.592</td><td>85.6%</td></tr></tbody></table><p>The loss curve tells an interesting story. The model converges rapidly &ndash; 45% relative loss reduction in under 3 minutes &ndash; and the evaluation loss tracks training loss closely, suggesting good generalization rather than memorization.</p><p><strong>85.6% token accuracy</strong> means the model correctly predicts the next token in p5.js code ~86% of the time. For code generation, this is remarkably high. The structured nature of p5.js (consistent function signatures, canvas operations, physics math patterns) gives the model strong priors to work with.</p><h2 id=what-this-actually-means>What This Actually Means<a hidden class=anchor aria-hidden=true href=#what-this-actually-means>#</a></h2><h3 id=1-knowledge-distillation-works-for-code-generation>1. Knowledge Distillation Works for Code Generation<a hidden class=anchor aria-hidden=true href=#1-knowledge-distillation-works-for-code-generation>#</a></h3><p>The core insight: Claude Opus (hundreds of billions of parameters, massive training data) can &ldquo;teach&rdquo; Qwen3-0.6B (792M parameters) to generate domain-specific code. The small model doesn&rsquo;t need to understand physics from first principles &ndash; it needs to learn the <strong>mapping from natural language descriptions to p5.js patterns</strong>.</p><p>This is closer to how human students learn programming: you don&rsquo;t derive JavaScript from mathematical axioms, you see patterns and internalize them.</p><h3 id=2-small-models--narrow-domains--surprising-capability>2. Small Models + Narrow Domains = Surprising Capability<a hidden class=anchor aria-hidden=true href=#2-small-models--narrow-domains--surprising-capability>#</a></h3><p>A 0.6B model can&rsquo;t write arbitrary code. But restrict the domain to &ldquo;p5.js animations on a 600x400 canvas teaching K-12 physics&rdquo; and suddenly the problem becomes tractable. The constraints reduce the output space dramatically:</p><ul><li>Fixed canvas size</li><li>Standard <code>setup()</code>/<code>draw()</code> structure</li><li>Limited API surface (vectors, shapes, text, color)</li><li>Predictable physics patterns (gravity = <code>vy += 0.15</code>, bounce = <code>vy *= -0.8</code>)</li></ul><p>This has implications for edge deployment. A 0.6B model runs on a phone, a Raspberry Pi, or embedded in a web app. You could have an offline-capable physics animation generator that works without internet access.</p><h3 id=3-parallel-agent-generation-creates-better-data>3. Parallel Agent Generation Creates Better Data<a hidden class=anchor aria-hidden=true href=#3-parallel-agent-generation-creates-better-data>#</a></h3><p>The 100-agent approach to dataset generation isn&rsquo;t just faster &ndash; it produces more diverse data. Each agent independently varies its style, producing natural variation that a single sequential generation would struggle to match. The result is a dataset where the model learns multiple ways to visualize the same concept.</p><h2 id=the-bigger-picture>The Bigger Picture<a hidden class=anchor aria-hidden=true href=#the-bigger-picture>#</a></h2><p>This experiment is a proof of concept for a broader pattern:</p><pre tabindex=0><code>Large model generates domain-specific training data
  → Small model learns the domain
  → Deploy small model at edge/scale
  → Iterate with human feedback
</code></pre><p>The economics are compelling. Training takes 3 minutes on rented GPUs (~$1). Inference runs on consumer hardware. And the model produces genuinely useful educational content.</p><p>Imagine this applied to:</p><ul><li><strong>Math visualization</strong>: Interactive proofs and geometric constructions</li><li><strong>Chemistry</strong>: Molecular dynamics and reaction animations</li><li><strong>Music theory</strong>: Audio-visual representations of harmony and rhythm</li><li><strong>History</strong>: Animated timelines and interactive maps</li></ul><p>The pattern scales. The models shrink. The content gets better.</p><h2 id=running-it-yourself>Running It Yourself<a hidden class=anchor aria-hidden=true href=#running-it-yourself>#</a></h2><p>The entire pipeline is open source:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/dylanler/qwen3-p5js-physics
</span></span><span style=display:flex><span>cd qwen3-p5js-physics
</span></span><span style=display:flex><span>uv sync
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate dataset (needs ANTHROPIC_API_KEY)</span>
</span></span><span style=display:flex><span>uv run python scripts/generate_dataset.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Fine-tune on 4 GPUs</span>
</span></span><span style=display:flex><span>uv run accelerate launch --config_file configs/accelerate_config.yaml scripts/train.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Generate an animation</span>
</span></span><span style=display:flex><span>uv run python scripts/inference.py <span style=color:#e6db74>&#34;Show me how gravity affects falling objects&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Serve as API</span>
</span></span><span style=display:flex><span>uv run python scripts/serve.py --port <span style=color:#ae81ff>8000</span>
</span></span></code></pre></div><p>The served model exposes an OpenAI-compatible API, so you can drop it into any existing application.</p><h2 id=what-id-do-next>What I&rsquo;d Do Next<a hidden class=anchor aria-hidden=true href=#what-id-do-next>#</a></h2><ol><li><strong>Human evaluation loop</strong>: Have actual teachers rate generated animations for accuracy and pedagogical value</li><li><strong>Multi-turn refinement</strong>: &ldquo;Make the gravity stronger&rdquo; / &ldquo;Add a label showing velocity&rdquo;</li><li><strong>Model scaling ladder</strong>: Compare 0.6B, 1.5B, 3B, 7B to find the accuracy/size sweet spot</li><li><strong>Domain expansion</strong>: Three.js for 3D physics, Manim for mathematical animations</li><li><strong>Student interaction data</strong>: Log which animations students actually find helpful and fine-tune on that signal</li></ol><p>The most exciting direction is <strong>closing the loop</strong>: students interact with generated animations, their engagement signals feed back into training, and the model gets better at explaining what students actually struggle with.</p><hr><p><em>Source code: <a href=https://github.com/dylanler/qwen3-p5js-physics>github.com/dylanler/qwen3-p5js-physics</a></em></p><p><em>The smallest model in the lab might be the most useful one in the classroom.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/fine-tuning/>Fine-Tuning</a></li><li><a href=https://dylanler.github.io/tags/lora/>LoRA</a></li><li><a href=https://dylanler.github.io/tags/code-generation/>Code-Generation</a></li><li><a href=https://dylanler.github.io/tags/physics-simulation/>Physics-Simulation</a></li><li><a href=https://dylanler.github.io/tags/education/>Education</a></li><li><a href=https://dylanler.github.io/tags/synthetic-data/>Synthetic Data</a></li></ul><nav class=paginav><a class=next href=https://dylanler.github.io/posts/physics-simulation-ai-developmental-learning/><span class=title>Next »</span><br><span>Learning Like Toddlers: Physics Simulation as a Foundation for AI Understanding</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>