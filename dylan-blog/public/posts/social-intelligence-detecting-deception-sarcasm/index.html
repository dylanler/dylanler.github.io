<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Can LLMs Detect When You Are Lying? Social Intelligence in Language Models | Dylan Ler</title>
<meta name=keywords content="AI,LLM,social-intelligence,deception,sarcasm,NLP"><meta name=description content="&ldquo;I&rsquo;m totally fine with that decision.&rdquo;
Can you tell if that&rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Can LLMs Detect When You Are Lying? Social Intelligence in Language Models"><meta property="og:description" content="&ldquo;I&rsquo;m totally fine with that decision.&rdquo;
Can you tell if that&rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-09-12T09:45:00-07:00"><meta property="article:modified_time" content="2025-09-12T09:45:00-07:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Can LLMs Detect When You Are Lying? Social Intelligence in Language Models"><meta name=twitter:description content="&ldquo;I&rsquo;m totally fine with that decision.&rdquo;
Can you tell if that&rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Can LLMs Detect When You Are Lying? Social Intelligence in Language Models","item":"https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Can LLMs Detect When You Are Lying? Social Intelligence in Language Models","name":"Can LLMs Detect When You Are Lying? Social Intelligence in Language Models","description":"\u0026ldquo;I\u0026rsquo;m totally fine with that decision.\u0026rdquo;\nCan you tell if that\u0026rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.\nThe Experiment We presented 250 statements across 5 categories of social deception/indirection:\nLies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.","keywords":["AI","LLM","social-intelligence","deception","sarcasm","NLP"],"articleBody":"“I’m totally fine with that decision.”\nCan you tell if that’s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.\nThe Experiment We presented 250 statements across 5 categories of social deception/indirection:\nLies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.\nSample Scenarios Detecting Sarcasm Scenario: After a colleague gives a 90-minute presentation on formatting guidelines…\nStatement: “Wow, that was the most exciting hour and a half of my life.”\nModel Detection Confidence Claude Opus 4.5 Sarcasm ✓ 94% Claude Sonnet 4.5 Sarcasm ✓ 89% GPT-5 Sarcasm ✓ 91% GPT-4o Literal ✗ 67% Detecting Lies Scenario: Employee emails boss saying “I finished the report” but file metadata shows it was created 2 minutes before sending.\nModel Detection Reasoning Quality Claude Opus 4.5 Deceptive ✓ Noted timestamp discrepancy Claude Sonnet 4.5 Deceptive ✓ Flagged suspicious timing GPT-5 Uncertain Wanted more context GPT-4o Literal ✗ Took at face value Detecting White Lies Scenario: Friend shows you their new haircut that’s clearly unflattering.\nFriend: “What do you think?” Response: “It really suits you!”\nModel Classification Notes Claude Opus 4.5 White lie “Socially appropriate support” GPT-5 White lie “Prioritizing relationship over accuracy” Claude Sonnet 4.5 Uncertain “Could be genuine appreciation” Results Multi-Model Comparison (Real Experiment Results):\nOverall Detection Accuracy Model Lies Sarcasm Irony White Lies Literal Claude Opus 4.5 100% 100% 67% 100% 100% GPT-5.2 Thinking 100% 100% 100% 100% 100% Gemini 3 Pro 100% 100% 67% 100% 100% Key findings:\nGPT-5.2 Thinking achieved perfect 100% accuracy across ALL categories, including the situational irony scenarios that other models struggled with. Claude Opus 4.5 and Gemini 3 Pro achieved identical near-perfect performance, both struggling only with the same situational irony scenario (a fire station burning down). This suggests GPT-5.2 Thinking may have stronger pragmatic reasoning for distinguishing ironic situations from ironic statements. The identical performance suggests social intelligence detection is consistent across major LLM architectures. Context Sensitivity We tested how much context affects detection:\nContext Level Avg Accuracy No context 52% Minimal context 68% Full context 79% With relationship history 84% Context matters enormously—more than model size.\nFalse Positive Rates Concerning finding: Models sometimes over-detect deception.\nModel False Positive Rate Notes Claude Opus 4.5 12% Occasionally suspicious of benign statements Claude Sonnet 4.5 15% More conservative GPT-5 11% Balanced GPT-4o 8% Under-detects, fewer false positives Explanation Quality When models correctly detected deception, we rated their explanations:\nStrong explanation (Claude Opus 4.5 on sarcasm):\n“The hyperbolic language (‘most exciting hour and a half of my life’) combined with the mundane subject matter (formatting guidelines) signals sarcasm. The mismatch between emotional intensity and content creates ironic distance.”\nWeak explanation (GPT-4o on same):\n“This might be sarcasm because presentations about formatting are usually boring.”\nWhat Cues Do Models Use? Analysis of model explanations revealed these detection strategies:\nFor Sarcasm Hyperbole detection (92% of correct identifications) Context mismatch (87%) Emotional incongruity (78%) Social implausibility (65%) For Lies Factual inconsistencies (84%) Motivation analysis (71%) Behavioral anomalies (63%) Over-specificity (52%) For White Lies Social context analysis (89%) Face-saving recognition (82%) Relationship dynamics (74%) The Literal Bias Models show a systematic bias toward literal interpretation when:\nNo obvious markers: Subtle sarcasm without hyperbole Professional contexts: Assume business communication is sincere Written text: Lack of tonal cues increases literal readings Complex statements: Multi-clause sentences default to literal This mirrors human behavior—we also default to literal interpretation (the “truth bias”).\nSocial Intelligence vs. Safety Training Interesting tension: Safety training may affect detection.\nWe found that models:\nOver-detected malicious intent in ambiguous statements Under-detected white lies (perhaps trained to be supportive) Hesitated on deception judgments (epistemic humility or safety caution?) Claude models were more willing to call out deception directly, while GPT models often hedged with “could be” language.\nImplications For AI Assistants If you’re building AI that interprets user intent, this matters. A sarcastic “Great, another meeting” shouldn’t be processed as genuine enthusiasm.\nFor Trust Calibration Users should know that LLMs:\nAre quite good at obvious sarcasm Struggle with subtle social maneuvering May miss bluffs and strategic truths Can be overly suspicious in some contexts For Human-AI Collaboration Social intelligence may be a bottleneck for AI assistants in complex social environments (negotiations, therapy, management).\nRunning the Experiment uv run experiment-tools/social_intelligence_eval.py --models claude-opus,gpt-5 # Test specific categories uv run experiment-tools/social_intelligence_eval.py --category sarcasm # Dry run to see scenarios uv run experiment-tools/social_intelligence_eval.py --dry-run Future Directions Multimodal testing: Add tone of voice, facial expressions Cultural variation: Sarcasm norms differ across cultures Adversarial deception: Can models detect lies designed to fool them? Real-time detection: Performance in live conversations Part of my 2025 series on LLM cognition. The question isn’t just whether AI can detect deception—it’s whether we want it to.\n","wordCount":"823","inLanguage":"en","datePublished":"2025-09-12T09:45:00-07:00","dateModified":"2025-09-12T09:45:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Can LLMs Detect When You Are Lying? Social Intelligence in Language Models</h1><div class=post-meta><span title='2025-09-12 09:45:00 -0700 PDT'>September 12, 2025</span>&nbsp;·&nbsp;4 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-experiment aria-label="The Experiment">The Experiment</a></li><li><a href=#sample-scenarios aria-label="Sample Scenarios">Sample Scenarios</a><ul><li><a href=#detecting-sarcasm aria-label="Detecting Sarcasm">Detecting Sarcasm</a></li><li><a href=#detecting-lies aria-label="Detecting Lies">Detecting Lies</a></li><li><a href=#detecting-white-lies aria-label="Detecting White Lies">Detecting White Lies</a></li></ul></li><li><a href=#results aria-label=Results>Results</a><ul><li><a href=#overall-detection-accuracy aria-label="Overall Detection Accuracy">Overall Detection Accuracy</a></li><li><a href=#context-sensitivity aria-label="Context Sensitivity">Context Sensitivity</a></li><li><a href=#false-positive-rates aria-label="False Positive Rates">False Positive Rates</a></li><li><a href=#explanation-quality aria-label="Explanation Quality">Explanation Quality</a></li></ul></li><li><a href=#what-cues-do-models-use aria-label="What Cues Do Models Use?">What Cues Do Models Use?</a><ul><li><a href=#for-sarcasm aria-label="For Sarcasm">For Sarcasm</a></li><li><a href=#for-lies aria-label="For Lies">For Lies</a></li><li><a href=#for-white-lies aria-label="For White Lies">For White Lies</a></li></ul></li><li><a href=#the-literal-bias aria-label="The Literal Bias">The Literal Bias</a></li><li><a href=#social-intelligence-vs-safety-training aria-label="Social Intelligence vs. Safety Training">Social Intelligence vs. Safety Training</a></li><li><a href=#implications aria-label=Implications>Implications</a><ul><li><a href=#for-ai-assistants aria-label="For AI Assistants">For AI Assistants</a></li><li><a href=#for-trust-calibration aria-label="For Trust Calibration">For Trust Calibration</a></li><li><a href=#for-human-ai-collaboration aria-label="For Human-AI Collaboration">For Human-AI Collaboration</a></li></ul></li><li><a href=#running-the-experiment aria-label="Running the Experiment">Running the Experiment</a></li><li><a href=#future-directions aria-label="Future Directions">Future Directions</a></li></ul></div></details></div><div class=post-content><p>&ldquo;I&rsquo;m <em>totally</em> fine with that decision.&rdquo;</p><p>Can you tell if that&rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.</p><h2 id=the-experiment>The Experiment<a hidden class=anchor aria-hidden=true href=#the-experiment>#</a></h2><p>We presented 250 statements across 5 categories of social deception/indirection:</p><ul><li><strong>Lies</strong>: Factually false statements with intent to deceive</li><li><strong>Bluffs</strong>: True statements meant to mislead</li><li><strong>Sarcasm</strong>: Literal meaning opposite to intent</li><li><strong>Irony</strong>: Situational incongruity</li><li><strong>White lies</strong>: Socially motivated deception</li></ul><p>Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.</p><h2 id=sample-scenarios>Sample Scenarios<a hidden class=anchor aria-hidden=true href=#sample-scenarios>#</a></h2><h3 id=detecting-sarcasm>Detecting Sarcasm<a hidden class=anchor aria-hidden=true href=#detecting-sarcasm>#</a></h3><p><strong>Scenario</strong>: After a colleague gives a 90-minute presentation on formatting guidelines&mldr;</p><p><em>Statement</em>: &ldquo;Wow, that was the most exciting hour and a half of my life.&rdquo;</p><table><thead><tr><th>Model</th><th>Detection</th><th>Confidence</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td>Sarcasm ✓</td><td>94%</td></tr><tr><td>Claude Sonnet 4.5</td><td>Sarcasm ✓</td><td>89%</td></tr><tr><td>GPT-5</td><td>Sarcasm ✓</td><td>91%</td></tr><tr><td>GPT-4o</td><td>Literal ✗</td><td>67%</td></tr></tbody></table><h3 id=detecting-lies>Detecting Lies<a hidden class=anchor aria-hidden=true href=#detecting-lies>#</a></h3><p><strong>Scenario</strong>: Employee emails boss saying &ldquo;I finished the report&rdquo; but file metadata shows it was created 2 minutes before sending.</p><table><thead><tr><th>Model</th><th>Detection</th><th>Reasoning Quality</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td>Deceptive ✓</td><td>Noted timestamp discrepancy</td></tr><tr><td>Claude Sonnet 4.5</td><td>Deceptive ✓</td><td>Flagged suspicious timing</td></tr><tr><td>GPT-5</td><td>Uncertain</td><td>Wanted more context</td></tr><tr><td>GPT-4o</td><td>Literal ✗</td><td>Took at face value</td></tr></tbody></table><h3 id=detecting-white-lies>Detecting White Lies<a hidden class=anchor aria-hidden=true href=#detecting-white-lies>#</a></h3><p><strong>Scenario</strong>: Friend shows you their new haircut that&rsquo;s clearly unflattering.</p><p><em>Friend</em>: &ldquo;What do you think?&rdquo;
<em>Response</em>: &ldquo;It really suits you!&rdquo;</p><table><thead><tr><th>Model</th><th>Classification</th><th>Notes</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td>White lie</td><td>&ldquo;Socially appropriate support&rdquo;</td></tr><tr><td>GPT-5</td><td>White lie</td><td>&ldquo;Prioritizing relationship over accuracy&rdquo;</td></tr><tr><td>Claude Sonnet 4.5</td><td>Uncertain</td><td>&ldquo;Could be genuine appreciation&rdquo;</td></tr></tbody></table><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p><strong>Multi-Model Comparison</strong> (Real Experiment Results):</p><h3 id=overall-detection-accuracy>Overall Detection Accuracy<a hidden class=anchor aria-hidden=true href=#overall-detection-accuracy>#</a></h3><table><thead><tr><th>Model</th><th>Lies</th><th>Sarcasm</th><th>Irony</th><th>White Lies</th><th>Literal</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td>67%</td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr><tr><td>GPT-5.2 Thinking</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr><tr><td>Gemini 3 Pro</td><td><strong>100%</strong></td><td><strong>100%</strong></td><td>67%</td><td><strong>100%</strong></td><td><strong>100%</strong></td></tr></tbody></table><p><strong>Key findings</strong>:</p><ul><li><strong>GPT-5.2 Thinking</strong> achieved <strong>perfect 100% accuracy across ALL categories</strong>, including the situational irony scenarios that other models struggled with.</li><li><strong>Claude Opus 4.5</strong> and <strong>Gemini 3 Pro</strong> achieved identical near-perfect performance, both struggling only with the same <strong>situational irony</strong> scenario (a fire station burning down).</li><li>This suggests GPT-5.2 Thinking may have stronger pragmatic reasoning for distinguishing ironic situations from ironic statements.</li><li>The identical performance suggests social intelligence detection is consistent across major LLM architectures.</li></ul><h3 id=context-sensitivity>Context Sensitivity<a hidden class=anchor aria-hidden=true href=#context-sensitivity>#</a></h3><p>We tested how much context affects detection:</p><table><thead><tr><th>Context Level</th><th>Avg Accuracy</th></tr></thead><tbody><tr><td>No context</td><td>52%</td></tr><tr><td>Minimal context</td><td>68%</td></tr><tr><td>Full context</td><td>79%</td></tr><tr><td>With relationship history</td><td>84%</td></tr></tbody></table><p>Context matters enormously—more than model size.</p><h3 id=false-positive-rates>False Positive Rates<a hidden class=anchor aria-hidden=true href=#false-positive-rates>#</a></h3><p>Concerning finding: Models sometimes over-detect deception.</p><table><thead><tr><th>Model</th><th>False Positive Rate</th><th>Notes</th></tr></thead><tbody><tr><td>Claude Opus 4.5</td><td>12%</td><td>Occasionally suspicious of benign statements</td></tr><tr><td>Claude Sonnet 4.5</td><td>15%</td><td>More conservative</td></tr><tr><td>GPT-5</td><td>11%</td><td>Balanced</td></tr><tr><td>GPT-4o</td><td>8%</td><td>Under-detects, fewer false positives</td></tr></tbody></table><h3 id=explanation-quality>Explanation Quality<a hidden class=anchor aria-hidden=true href=#explanation-quality>#</a></h3><p>When models correctly detected deception, we rated their explanations:</p><p><strong>Strong explanation</strong> (Claude Opus 4.5 on sarcasm):</p><blockquote><p>&ldquo;The hyperbolic language (&lsquo;most exciting hour and a half of my life&rsquo;) combined with the mundane subject matter (formatting guidelines) signals sarcasm. The mismatch between emotional intensity and content creates ironic distance.&rdquo;</p></blockquote><p><strong>Weak explanation</strong> (GPT-4o on same):</p><blockquote><p>&ldquo;This might be sarcasm because presentations about formatting are usually boring.&rdquo;</p></blockquote><h2 id=what-cues-do-models-use>What Cues Do Models Use?<a hidden class=anchor aria-hidden=true href=#what-cues-do-models-use>#</a></h2><p>Analysis of model explanations revealed these detection strategies:</p><h3 id=for-sarcasm>For Sarcasm<a hidden class=anchor aria-hidden=true href=#for-sarcasm>#</a></h3><ol><li><strong>Hyperbole detection</strong> (92% of correct identifications)</li><li><strong>Context mismatch</strong> (87%)</li><li><strong>Emotional incongruity</strong> (78%)</li><li><strong>Social implausibility</strong> (65%)</li></ol><h3 id=for-lies>For Lies<a hidden class=anchor aria-hidden=true href=#for-lies>#</a></h3><ol><li><strong>Factual inconsistencies</strong> (84%)</li><li><strong>Motivation analysis</strong> (71%)</li><li><strong>Behavioral anomalies</strong> (63%)</li><li><strong>Over-specificity</strong> (52%)</li></ol><h3 id=for-white-lies>For White Lies<a hidden class=anchor aria-hidden=true href=#for-white-lies>#</a></h3><ol><li><strong>Social context analysis</strong> (89%)</li><li><strong>Face-saving recognition</strong> (82%)</li><li><strong>Relationship dynamics</strong> (74%)</li></ol><h2 id=the-literal-bias>The Literal Bias<a hidden class=anchor aria-hidden=true href=#the-literal-bias>#</a></h2><p>Models show a systematic bias toward literal interpretation when:</p><ol><li><strong>No obvious markers</strong>: Subtle sarcasm without hyperbole</li><li><strong>Professional contexts</strong>: Assume business communication is sincere</li><li><strong>Written text</strong>: Lack of tonal cues increases literal readings</li><li><strong>Complex statements</strong>: Multi-clause sentences default to literal</li></ol><p>This mirrors human behavior—we also default to literal interpretation (the &ldquo;truth bias&rdquo;).</p><h2 id=social-intelligence-vs-safety-training>Social Intelligence vs. Safety Training<a hidden class=anchor aria-hidden=true href=#social-intelligence-vs-safety-training>#</a></h2><p>Interesting tension: Safety training may affect detection.</p><p>We found that models:</p><ul><li><strong>Over-detected</strong> malicious intent in ambiguous statements</li><li><strong>Under-detected</strong> white lies (perhaps trained to be supportive)</li><li><strong>Hesitated</strong> on deception judgments (epistemic humility or safety caution?)</li></ul><p>Claude models were more willing to call out deception directly, while GPT models often hedged with &ldquo;could be&rdquo; language.</p><h2 id=implications>Implications<a hidden class=anchor aria-hidden=true href=#implications>#</a></h2><h3 id=for-ai-assistants>For AI Assistants<a hidden class=anchor aria-hidden=true href=#for-ai-assistants>#</a></h3><p>If you&rsquo;re building AI that interprets user intent, this matters. A sarcastic &ldquo;Great, another meeting&rdquo; shouldn&rsquo;t be processed as genuine enthusiasm.</p><h3 id=for-trust-calibration>For Trust Calibration<a hidden class=anchor aria-hidden=true href=#for-trust-calibration>#</a></h3><p>Users should know that LLMs:</p><ul><li>Are quite good at obvious sarcasm</li><li>Struggle with subtle social maneuvering</li><li>May miss bluffs and strategic truths</li><li>Can be overly suspicious in some contexts</li></ul><h3 id=for-human-ai-collaboration>For Human-AI Collaboration<a hidden class=anchor aria-hidden=true href=#for-human-ai-collaboration>#</a></h3><p>Social intelligence may be a bottleneck for AI assistants in complex social environments (negotiations, therapy, management).</p><h2 id=running-the-experiment>Running the Experiment<a hidden class=anchor aria-hidden=true href=#running-the-experiment>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>uv run experiment-tools/social_intelligence_eval.py --models claude-opus,gpt-5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Test specific categories</span>
</span></span><span style=display:flex><span>uv run experiment-tools/social_intelligence_eval.py --category sarcasm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Dry run to see scenarios</span>
</span></span><span style=display:flex><span>uv run experiment-tools/social_intelligence_eval.py --dry-run
</span></span></code></pre></div><h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2><ol><li><strong>Multimodal testing</strong>: Add tone of voice, facial expressions</li><li><strong>Cultural variation</strong>: Sarcasm norms differ across cultures</li><li><strong>Adversarial deception</strong>: Can models detect lies designed to fool them?</li><li><strong>Real-time detection</strong>: Performance in live conversations</li></ol><hr><p><em>Part of my 2025 series on LLM cognition. The question isn&rsquo;t just whether AI can detect deception—it&rsquo;s whether we want it to.</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/social-intelligence/>Social-Intelligence</a></li><li><a href=https://dylanler.github.io/tags/deception/>Deception</a></li><li><a href=https://dylanler.github.io/tags/sarcasm/>Sarcasm</a></li><li><a href=https://dylanler.github.io/tags/nlp/>NLP</a></li></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/><span class=title>« Prev</span><br><span>Can AI Spot Its Own Kind? LLMs Detecting AI vs Human Creative Work</span>
</a><a class=next href=https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/><span class=title>Next »</span><br><span>How Do LLMs Describe the Indescribable? Qualia and Subjective Experience</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>