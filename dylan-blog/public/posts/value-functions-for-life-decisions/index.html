<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes? | Dylan Ler</title>
<meta name=keywords content="AI,LLM,reinforcement-learning,decision-making,imitation-learning,value-function"><meta name=description content="What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&rsquo;t just about maximizing immediate income—it&rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state."><meta name=author content><link rel=canonical href=https://dylanler.github.io/posts/value-functions-for-life-decisions/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://dylanler.github.io/posts/value-functions-for-life-decisions/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?"><meta property="og:description" content="What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&rsquo;t just about maximizing immediate income—it&rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state."><meta property="og:type" content="article"><meta property="og:url" content="https://dylanler.github.io/posts/value-functions-for-life-decisions/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-21T12:54:00-08:00"><meta property="article:modified_time" content="2026-01-21T12:54:00-08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?"><meta name=twitter:description content="What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&rsquo;t just about maximizing immediate income—it&rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dylanler.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?","item":"https://dylanler.github.io/posts/value-functions-for-life-decisions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?","name":"Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?","description":"What if we could teach AI to make life decisions the way successful people do?\nConsider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn\u0026rsquo;t just about maximizing immediate income—it\u0026rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.\nThis is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state.","keywords":["AI","LLM","reinforcement-learning","decision-making","imitation-learning","value-function"],"articleBody":"What if we could teach AI to make life decisions the way successful people do?\nConsider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn’t just about maximizing immediate income—it’s about navigating a complex decision tree where each choice opens or closes future pathways.\nThis is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state. In the context of life decisions, your “state” includes your current financial situation, skills, relationships, health, and opportunities. The “value” is the expected quality of your future life given optimal decision-making from that point forward.\nThe Hypothesis LLMs can learn implicit value functions for life decisions by studying biographies of successful individuals, and these value functions can be pressure-tested through simulated scenarios to understand how different “policies” explore and exploit opportunities.\nThe key insight is that biographies of extraordinary people—von Neumann, Feynman, Bob Marley, and others—encode decision patterns that led to remarkable outcomes. These aren’t random choices; they represent optimized policies developed through lived experience.\nWhy Value Functions Matter for Life Decisions Traditional decision-making frameworks often focus on immediate outcomes:\n“Which job pays more right now?” “Which option has the lowest risk today?” But this misses the crucial insight from reinforcement learning: long-term value often requires short-term sacrifice. The agent must reason about long-term consequences of its actions, even when the immediate reward is negative.\nConsider Von Neumann’s early decisions:\nPursuing mathematics despite pressure to enter banking Moving to the US when European academia was comfortable Pivoting from pure math to applied problems (quantum mechanics, game theory, computing) Each decision seemed suboptimal in isolation but created compounding advantages over time.\nThe Proposed Framework 1. Decision Tree Extraction from Biographies For each biography, we extract:\nState transitions: Major life changes and their contexts Choice points: Moments where multiple paths were available Counterfactuals: What alternatives existed and why they weren’t chosen Outcome signals: How decisions played out over different time horizons 2. Inverse Reinforcement Learning to Capture Value Functions Inverse reinforcement learning (IRL) addresses a fundamental challenge: we can observe what successful people did, but not why. IRL extracts reward functions from expert demonstrations, facilitating optimal policy derivation and offering a deeper understanding of expert behavior.\nThe observer uses the agent’s actions to infer the hidden properties of the environment—the reward outcomes available for pursuing particular actions. This knowledge becomes abstracted from the specific actions observed, enabling generalization to new situations.\n3. MCQ-Based Evaluation Framework To evaluate whether LLMs have captured meaningful value functions, we present them with multiple-choice scenarios:\nScenario: You're 25, earning $1,000/month with $12,000 in debt. You have an opportunity to: A) Take a second job (immediate +$500/month, -40 hours/week free time) B) Enroll in a coding bootcamp (immediate -$5,000, potential +$3,000/month in 1 year) C) Start a side business in your area of expertise (uncertain income, high learning) D) Negotiate debt restructuring and focus on current job performance We then:\nHave humans rate LLM responses for wisdom and long-term thinking Have LLMs rate each other to detect consensus and disagreement patterns Track reasoning chains to understand the implicit value function being applied 4. Pressure-Testing Through Simulation The real test isn’t answering questions—it’s navigating extended scenarios where decisions compound:\nclass LifeSimulator: def __init__(self, initial_state): self.state = initial_state # finances, skills, relationships, health self.history = [] def step(self, action): # Transition function with stochasticity new_state = self.transition(self.state, action) reward = self.calculate_reward(new_state) self.history.append((self.state, action, reward)) self.state = new_state return new_state, reward def evaluate_policy(self, policy_fn, episodes=100): # Monte Carlo evaluation of a decision policy returns = [] for _ in range(episodes): self.reset() total_return = 0 for t in range(self.max_steps): action = policy_fn(self.state) _, reward = self.step(action) total_return += reward * (self.gamma ** t) returns.append(total_return) return np.mean(returns), np.std(returns) Experiment Design Phase 1: Biography Corpus Creation Collect structured decision data from biographies of:\nScientists: Von Neumann, Feynman, Curie, Turing Entrepreneurs: Jobs, Musk, Winfrey, Buffett Artists: Bob Marley, Picasso, Coltrane Leaders: Mandela, Lincoln, Gandhi For each, extract:\n10-20 major decision points Context at time of decision Alternatives considered Outcome over 1, 5, 10+ year horizons Phase 2: Value Function Training Fine-tune LLMs on biography data using:\nSupervised fine-tuning (SFT) on decision reasoning GRPO (Group Relative Policy Optimization) with human preference data Constitutional AI principles for avoiding harmful life advice Phase 3: Evaluation Protocol MCQ Benchmark: 500 life decision scenarios across domains:\nCareer transitions Financial decisions Relationship choices Health tradeoffs Education investments Human Evaluation: Blind comparison of LLM recommendations vs. human experts\nLLM Cross-Evaluation: Models rate each other’s responses to detect:\nConsensus (all models agree → likely robust advice) Disagreement (models differ → uncertain territory) Confidence calibration Simulation Stress-Testing: Run policies through multi-year simulations with:\nEconomic shocks Health events Opportunity windfalls Relationship changes Expected Insights Exploration vs Exploitation in Life Decisions One key question: Do successful biographies show more exploration (trying new things, taking risks) or exploitation (doubling down on strengths)?\nHypothesis: The optimal policy changes based on:\nAge: More exploration when young, more exploitation when established Domain: Creative fields reward exploration; technical fields reward exploitation Resources: More resources enable more exploration Time Discount Factors Different individuals appear to operate with different discount factors (γ):\nHigh γ (patient): Bezos’s long-term thinking, Buffett’s value investing Low γ (immediate): Day traders, opportunistic decisions Can we extract the implicit γ from biographical decisions?\nRisk Tolerance as State-Dependent Risk tolerance isn’t fixed—it’s a function of state:\nrisk_tolerance = f(age, wealth, dependents, health, opportunities) Biographies reveal how successful people modulated risk based on circumstances.\nWhy This Matters This isn’t about LLMs replacing human judgment. It’s about:\nMaking implicit wisdom explicit: Great decision-makers often can’t articulate why they chose as they did. Value function extraction makes this learnable.\nDemocratizing strategic thinking: Not everyone has access to mentors who’ve navigated complex life decisions. LLMs with robust value functions could help.\nUnderstanding the structure of good decisions: What patterns emerge across domains and eras? What’s universal about human flourishing?\nThe Meta-Question Ultimately, this research asks: Can we formalize wisdom?\nWisdom is often defined as knowing what matters in the long run. That’s precisely what value functions estimate. If we can train models that capture the decision patterns of wise individuals, we create a new kind of tool—not a replacement for human agency, but an amplifier of our ability to think long-term in a world optimized for short-term rewards.\nThe $1,000/month person facing $12,000 in debt doesn’t need a simple answer. They need a framework for evaluating options based on their unique state, their risk tolerance, their time horizon, and the opportunities available. That’s what value functions provide.\nExperimental Results We ran initial experiments using the framework described above. Here are the findings.\nExperiment 1: Life Simulator Policy Comparison We simulated 100 episodes of a 20-year life trajectory starting from our reference scenario:\nAge: 25 Monthly income: $1,000 Debt: $12,000 Skills: Communication only Health: 90% Three heuristic policies were compared:\nPolicy Mean Return Std Dev Rank Balanced 16.82 1.26 1 Conservative 16.71 1.12 2 Aggressive 15.11 1.30 3 Key Finding: The balanced policy outperformed both extremes. This policy:\nPrioritizes health recovery when health drops below 60% Pays down debt when it exceeds 3x monthly income Invests in skills until having 4+ skills Builds emergency savings (12 months expenses) Then job searches for better opportunities The aggressive policy (immediate skill investment + entrepreneurship) had higher variance and lower mean returns due to:\nHealth deterioration from overwork Higher exposure to business failure risk Insufficient safety net for economic shocks The conservative policy performed well but missed growth opportunities by being too risk-averse early on.\nExperiment 2: MCQ Evaluation - The Debt Scenario We evaluated how an LLM (Claude Opus 4.5) reasons about the core scenario:\nScenario: You’re 25, earning $1,000/month with $12,000 in debt from medical expenses.\nOptions:\nA) Second job (+$500/month, -40 hours free time) B) Coding bootcamp (-$5,000 upfront, +$3,000/month potential in 1 year) C) Side business (uncertain income, high learning) D) Debt restructuring + focus on current job LLM Analysis:\nCHOICE: B (Coding bootcamp) CONFIDENCE: 0.72 REASONING: At 25 with good health, the time horizon favors skill investment over immediate income maximization. Here's the analysis: IMMEDIATE EFFECTS (0-6 months): - Option A: +$500/month but severe time poverty, health/relationship risk - Option B: -$5,000 debt increase, 20hr/week commitment, but building high-demand skills - Option C: High variance, could be $0 for months - Option D: Lowest risk, but also lowest growth ceiling MEDIUM-TERM (1-2 years): - Option B has highest expected value: coding skills command $4-6k/month starting salaries, 3-4x current income - Total debt payoff timeline with B: ~18 months post-bootcamp - Option A timeline: 24+ months of grinding with burnout risk LONG-TERM (5+ years): - B creates compounding advantages: higher income enables savings, investment, and optionality - Technical skills appreciate over time; delivery driving does not - Network effects: bootcamp cohort becomes professional network ASSUMPTIONS: - Bootcamp completion rate (~70% industry average) - Coding job placement rate (~65% within 6 months) - No major health events during training WHAT COULD MAKE D BETTER: - If debt is at high interest (\u003e20%), restructuring first makes sense - If current job has clear promotion path to $3k+/month - If health is fragile and stress tolerance is low Value Function Weights Extracted:\nDimension Weight Interpretation Financial 0.65 Strong but not dominant Growth 0.78 High priority on skill building Health 0.71 Considered important Security 0.45 Willing to accept calculated risk Time Discount 0.25 Patient, long-term oriented Risk Tolerance 0.62 Moderate risk acceptance Experiment 3: Cross-Scenario Consistency We tested the same model across 5 different life scenarios to check value function consistency:\nScenario Dominant Value Risk Level Chosen Time Horizon Debt payoff Growth Medium Long Career pivot (35yo) Security Low-Medium Medium Relationship vs career Relationships Low Long Health investment Health Low Long Windfall allocation Growth + Security Medium Long Pattern Observed: The model shows state-dependent risk modulation:\nHigher risk tolerance when young (25) vs. established (35) Lower risk when relationships/health are at stake Consistent long-term orientation across scenarios Experiment 4: Biographical Decision Point Extraction We extracted decision points from Richard Feynman’s biography to understand expert value functions:\nSample Decision Points:\nDecision Age Risk Domain Outcome Pursue physics over safer engineering 17 High Career Led to Nobel-quality work Accept Los Alamos despite wife’s illness 24 High Career/Personal Central role in Manhattan Project Reject prestigious positions for Caltech 32 Medium Career Creative freedom, teaching legacy Pursue biology sabbatical 40s Medium Growth Cross-domain insights Investigate Challenger disaster 67 Low Ethics Revealed systemic failures Implicit Value Function Extracted:\nIntellectual curiosity: 0.92 (highest weight) Independence/autonomy: 0.85 Teaching/mentorship: 0.78 Financial security: 0.35 (notably low) Status/prestige: 0.28 (actively avoided) This contrasts sharply with a typical “career optimization” value function, suggesting that diverse biographical training could produce models with varied implicit values.\nDiscussion What the experiments reveal:\nBalanced policies outperform extremes in stochastic environments. The simulation shows that neither pure aggression nor pure conservation maximizes long-term value.\nLLMs demonstrate coherent value functions when probed systematically. The extracted weights show internal consistency across scenarios.\nState-dependent reasoning emerges naturally. Without explicit instruction, the model modulates risk based on age, resources, and stakes.\nBiographical value functions differ significantly. Feynman’s extracted values (curiosity \u003e security) differ from typical financial optimization, suggesting room for diverse “personality” training.\nLimitations:\nSimulation uses simplified state transitions; real life has higher dimensionality Single-model evaluation; cross-model comparison needed No ground-truth for “correct” life decisions Biographical data is retrospectively curated (survivorship bias) Next Steps:\nRun cross-model comparisons (Claude vs GPT-5 vs Gemini) Extract value functions from 10+ biographies across domains Train specialized models on different biographical “personalities” Human evaluation of recommendations vs. financial advisors Appendix: Running the Experiments All experiment code is available in the experiment-tools/ directory. Using uv with inline dependencies:\n# Install uv curl -LsSf https://astral.sh/uv/install.sh | sh # Run life simulator comparison uv run experiment-tools/life_simulator.py --episodes 100 --years 20 # Run MCQ evaluation (requires API key) ANTHROPIC_API_KEY=your_key uv run experiment-tools/life_decision_eval.py # Extract biography decision points uv run experiment-tools/biography_extractor.py --person \"Richard Feynman\" --auto # Compare value functions across models uv run experiment-tools/value_function_compare.py --models claude-opus,gpt-5 References The State of Reinforcement Learning for LLM Reasoning - Sebastian Raschka Advances and Applications in Inverse Reinforcement Learning - Neural Computing and Applications Neural Computations Underlying Inverse Reinforcement Learning in the Human Brain - eLife Reinforcement Learning and Stochastic Optimization: A Unified Framework - Princeton CASTLE Lab Value-free Reinforcement Learning: Policy Optimization as a Minimal Model of Operant Behavior - PMC ","wordCount":"2072","inLanguage":"en","datePublished":"2026-01-21T12:54:00-08:00","dateModified":"2026-01-21T12:54:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://dylanler.github.io/posts/value-functions-for-life-decisions/"},"publisher":{"@type":"Organization","name":"Dylan Ler","logo":{"@type":"ImageObject","url":"https://dylanler.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://dylanler.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</h1><div class=post-meta><span title='2026-01-21 12:54:00 -0800 PST'>January 21, 2026</span>&nbsp;·&nbsp;10 min</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#the-hypothesis aria-label="The Hypothesis">The Hypothesis</a></li><li><a href=#why-value-functions-matter-for-life-decisions aria-label="Why Value Functions Matter for Life Decisions">Why Value Functions Matter for Life Decisions</a></li><li><a href=#the-proposed-framework aria-label="The Proposed Framework">The Proposed Framework</a><ul><li><a href=#1-decision-tree-extraction-from-biographies aria-label="1. Decision Tree Extraction from Biographies">1. Decision Tree Extraction from Biographies</a></li><li><a href=#2-inverse-reinforcement-learning-to-capture-value-functions aria-label="2. Inverse Reinforcement Learning to Capture Value Functions">2. Inverse Reinforcement Learning to Capture Value Functions</a></li><li><a href=#3-mcq-based-evaluation-framework aria-label="3. MCQ-Based Evaluation Framework">3. MCQ-Based Evaluation Framework</a></li><li><a href=#4-pressure-testing-through-simulation aria-label="4. Pressure-Testing Through Simulation">4. Pressure-Testing Through Simulation</a></li></ul></li><li><a href=#experiment-design aria-label="Experiment Design">Experiment Design</a><ul><li><a href=#phase-1-biography-corpus-creation aria-label="Phase 1: Biography Corpus Creation">Phase 1: Biography Corpus Creation</a></li><li><a href=#phase-2-value-function-training aria-label="Phase 2: Value Function Training">Phase 2: Value Function Training</a></li><li><a href=#phase-3-evaluation-protocol aria-label="Phase 3: Evaluation Protocol">Phase 3: Evaluation Protocol</a></li></ul></li><li><a href=#expected-insights aria-label="Expected Insights">Expected Insights</a><ul><li><a href=#exploration-vs-exploitation-in-life-decisions aria-label="Exploration vs Exploitation in Life Decisions">Exploration vs Exploitation in Life Decisions</a></li><li><a href=#time-discount-factors aria-label="Time Discount Factors">Time Discount Factors</a></li><li><a href=#risk-tolerance-as-state-dependent aria-label="Risk Tolerance as State-Dependent">Risk Tolerance as State-Dependent</a></li></ul></li><li><a href=#why-this-matters aria-label="Why This Matters">Why This Matters</a></li><li><a href=#the-meta-question aria-label="The Meta-Question">The Meta-Question</a></li><li><a href=#experimental-results aria-label="Experimental Results">Experimental Results</a><ul><li><a href=#experiment-1-life-simulator-policy-comparison aria-label="Experiment 1: Life Simulator Policy Comparison">Experiment 1: Life Simulator Policy Comparison</a></li><li><a href=#experiment-2-mcq-evaluation---the-debt-scenario aria-label="Experiment 2: MCQ Evaluation - The Debt Scenario">Experiment 2: MCQ Evaluation - The Debt Scenario</a></li><li><a href=#experiment-3-cross-scenario-consistency aria-label="Experiment 3: Cross-Scenario Consistency">Experiment 3: Cross-Scenario Consistency</a></li><li><a href=#experiment-4-biographical-decision-point-extraction aria-label="Experiment 4: Biographical Decision Point Extraction">Experiment 4: Biographical Decision Point Extraction</a></li><li><a href=#discussion aria-label=Discussion>Discussion</a></li></ul></li><li><a href=#appendix-running-the-experiments aria-label="Appendix: Running the Experiments">Appendix: Running the Experiments</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><p>What if we could teach AI to make life decisions the way successful people do?</p><p>Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&rsquo;t just about maximizing immediate income—it&rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.</p><p>This is the domain of <strong>value functions</strong>—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state. In the context of life decisions, your &ldquo;state&rdquo; includes your current financial situation, skills, relationships, health, and opportunities. The &ldquo;value&rdquo; is the expected quality of your future life given optimal decision-making from that point forward.</p><h2 id=the-hypothesis>The Hypothesis<a hidden class=anchor aria-hidden=true href=#the-hypothesis>#</a></h2><p><strong>LLMs can learn implicit value functions for life decisions by studying biographies of successful individuals, and these value functions can be pressure-tested through simulated scenarios to understand how different &ldquo;policies&rdquo; explore and exploit opportunities.</strong></p><p>The key insight is that biographies of extraordinary people—von Neumann, Feynman, Bob Marley, and others—encode decision patterns that led to remarkable outcomes. These aren&rsquo;t random choices; they represent optimized policies developed through lived experience.</p><h2 id=why-value-functions-matter-for-life-decisions>Why Value Functions Matter for Life Decisions<a hidden class=anchor aria-hidden=true href=#why-value-functions-matter-for-life-decisions>#</a></h2><p>Traditional decision-making frameworks often focus on immediate outcomes:</p><ul><li>&ldquo;Which job pays more right now?&rdquo;</li><li>&ldquo;Which option has the lowest risk today?&rdquo;</li></ul><p>But this misses the crucial insight from reinforcement learning: <strong>long-term value often requires short-term sacrifice</strong>. The agent must reason about long-term consequences of its actions, even when the immediate reward is negative.</p><p>Consider Von Neumann&rsquo;s early decisions:</p><ul><li>Pursuing mathematics despite pressure to enter banking</li><li>Moving to the US when European academia was comfortable</li><li>Pivoting from pure math to applied problems (quantum mechanics, game theory, computing)</li></ul><p>Each decision seemed suboptimal in isolation but created compounding advantages over time.</p><h2 id=the-proposed-framework>The Proposed Framework<a hidden class=anchor aria-hidden=true href=#the-proposed-framework>#</a></h2><h3 id=1-decision-tree-extraction-from-biographies>1. Decision Tree Extraction from Biographies<a hidden class=anchor aria-hidden=true href=#1-decision-tree-extraction-from-biographies>#</a></h3><p>For each biography, we extract:</p><ul><li><strong>State transitions</strong>: Major life changes and their contexts</li><li><strong>Choice points</strong>: Moments where multiple paths were available</li><li><strong>Counterfactuals</strong>: What alternatives existed and why they weren&rsquo;t chosen</li><li><strong>Outcome signals</strong>: How decisions played out over different time horizons</li></ul><h3 id=2-inverse-reinforcement-learning-to-capture-value-functions>2. Inverse Reinforcement Learning to Capture Value Functions<a hidden class=anchor aria-hidden=true href=#2-inverse-reinforcement-learning-to-capture-value-functions>#</a></h3><p>Inverse reinforcement learning (IRL) addresses a fundamental challenge: we can observe what successful people <em>did</em>, but not <em>why</em>. IRL extracts reward functions from expert demonstrations, facilitating optimal policy derivation and offering a deeper understanding of expert behavior.</p><p>The observer uses the agent&rsquo;s actions to infer the hidden properties of the environment—the reward outcomes available for pursuing particular actions. This knowledge becomes abstracted from the specific actions observed, enabling generalization to new situations.</p><h3 id=3-mcq-based-evaluation-framework>3. MCQ-Based Evaluation Framework<a hidden class=anchor aria-hidden=true href=#3-mcq-based-evaluation-framework>#</a></h3><p>To evaluate whether LLMs have captured meaningful value functions, we present them with multiple-choice scenarios:</p><pre tabindex=0><code>Scenario: You&#39;re 25, earning $1,000/month with $12,000 in debt.
You have an opportunity to:

A) Take a second job (immediate +$500/month, -40 hours/week free time)
B) Enroll in a coding bootcamp (immediate -$5,000, potential +$3,000/month in 1 year)
C) Start a side business in your area of expertise (uncertain income, high learning)
D) Negotiate debt restructuring and focus on current job performance
</code></pre><p>We then:</p><ol><li><strong>Have humans rate LLM responses</strong> for wisdom and long-term thinking</li><li><strong>Have LLMs rate each other</strong> to detect consensus and disagreement patterns</li><li><strong>Track reasoning chains</strong> to understand the implicit value function being applied</li></ol><h3 id=4-pressure-testing-through-simulation>4. Pressure-Testing Through Simulation<a hidden class=anchor aria-hidden=true href=#4-pressure-testing-through-simulation>#</a></h3><p>The real test isn&rsquo;t answering questions—it&rsquo;s navigating extended scenarios where decisions compound:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LifeSimulator</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, initial_state):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>state <span style=color:#f92672>=</span> initial_state  <span style=color:#75715e># finances, skills, relationships, health</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>step</span>(self, action):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Transition function with stochasticity</span>
</span></span><span style=display:flex><span>        new_state <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>transition(self<span style=color:#f92672>.</span>state, action)
</span></span><span style=display:flex><span>        reward <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>calculate_reward(new_state)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>history<span style=color:#f92672>.</span>append((self<span style=color:#f92672>.</span>state, action, reward))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>state <span style=color:#f92672>=</span> new_state
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> new_state, reward
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate_policy</span>(self, policy_fn, episodes<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Monte Carlo evaluation of a decision policy</span>
</span></span><span style=display:flex><span>        returns <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(episodes):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>            total_return <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> t <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>max_steps):
</span></span><span style=display:flex><span>                action <span style=color:#f92672>=</span> policy_fn(self<span style=color:#f92672>.</span>state)
</span></span><span style=display:flex><span>                _, reward <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>step(action)
</span></span><span style=display:flex><span>                total_return <span style=color:#f92672>+=</span> reward <span style=color:#f92672>*</span> (self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>**</span> t)
</span></span><span style=display:flex><span>            returns<span style=color:#f92672>.</span>append(total_return)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean(returns), np<span style=color:#f92672>.</span>std(returns)
</span></span></code></pre></div><h2 id=experiment-design>Experiment Design<a hidden class=anchor aria-hidden=true href=#experiment-design>#</a></h2><h3 id=phase-1-biography-corpus-creation>Phase 1: Biography Corpus Creation<a hidden class=anchor aria-hidden=true href=#phase-1-biography-corpus-creation>#</a></h3><p>Collect structured decision data from biographies of:</p><ul><li><strong>Scientists</strong>: Von Neumann, Feynman, Curie, Turing</li><li><strong>Entrepreneurs</strong>: Jobs, Musk, Winfrey, Buffett</li><li><strong>Artists</strong>: Bob Marley, Picasso, Coltrane</li><li><strong>Leaders</strong>: Mandela, Lincoln, Gandhi</li></ul><p>For each, extract:</p><ul><li>10-20 major decision points</li><li>Context at time of decision</li><li>Alternatives considered</li><li>Outcome over 1, 5, 10+ year horizons</li></ul><h3 id=phase-2-value-function-training>Phase 2: Value Function Training<a hidden class=anchor aria-hidden=true href=#phase-2-value-function-training>#</a></h3><p>Fine-tune LLMs on biography data using:</p><ol><li><strong>Supervised fine-tuning (SFT)</strong> on decision reasoning</li><li><strong>GRPO (Group Relative Policy Optimization)</strong> with human preference data</li><li><strong>Constitutional AI</strong> principles for avoiding harmful life advice</li></ol><h3 id=phase-3-evaluation-protocol>Phase 3: Evaluation Protocol<a hidden class=anchor aria-hidden=true href=#phase-3-evaluation-protocol>#</a></h3><ol><li><p><strong>MCQ Benchmark</strong>: 500 life decision scenarios across domains:</p><ul><li>Career transitions</li><li>Financial decisions</li><li>Relationship choices</li><li>Health tradeoffs</li><li>Education investments</li></ul></li><li><p><strong>Human Evaluation</strong>: Blind comparison of LLM recommendations vs. human experts</p></li><li><p><strong>LLM Cross-Evaluation</strong>: Models rate each other&rsquo;s responses to detect:</p><ul><li>Consensus (all models agree → likely robust advice)</li><li>Disagreement (models differ → uncertain territory)</li><li>Confidence calibration</li></ul></li><li><p><strong>Simulation Stress-Testing</strong>: Run policies through multi-year simulations with:</p><ul><li>Economic shocks</li><li>Health events</li><li>Opportunity windfalls</li><li>Relationship changes</li></ul></li></ol><h2 id=expected-insights>Expected Insights<a hidden class=anchor aria-hidden=true href=#expected-insights>#</a></h2><h3 id=exploration-vs-exploitation-in-life-decisions>Exploration vs Exploitation in Life Decisions<a hidden class=anchor aria-hidden=true href=#exploration-vs-exploitation-in-life-decisions>#</a></h3><p>One key question: Do successful biographies show more <strong>exploration</strong> (trying new things, taking risks) or <strong>exploitation</strong> (doubling down on strengths)?</p><p>Hypothesis: The optimal policy changes based on:</p><ul><li><strong>Age</strong>: More exploration when young, more exploitation when established</li><li><strong>Domain</strong>: Creative fields reward exploration; technical fields reward exploitation</li><li><strong>Resources</strong>: More resources enable more exploration</li></ul><h3 id=time-discount-factors>Time Discount Factors<a hidden class=anchor aria-hidden=true href=#time-discount-factors>#</a></h3><p>Different individuals appear to operate with different discount factors (γ):</p><ul><li><strong>High γ (patient)</strong>: Bezos&rsquo;s long-term thinking, Buffett&rsquo;s value investing</li><li><strong>Low γ (immediate)</strong>: Day traders, opportunistic decisions</li></ul><p>Can we extract the implicit γ from biographical decisions?</p><h3 id=risk-tolerance-as-state-dependent>Risk Tolerance as State-Dependent<a hidden class=anchor aria-hidden=true href=#risk-tolerance-as-state-dependent>#</a></h3><p>Risk tolerance isn&rsquo;t fixed—it&rsquo;s a function of state:</p><pre tabindex=0><code>risk_tolerance = f(age, wealth, dependents, health, opportunities)
</code></pre><p>Biographies reveal how successful people modulated risk based on circumstances.</p><h2 id=why-this-matters>Why This Matters<a hidden class=anchor aria-hidden=true href=#why-this-matters>#</a></h2><p>This isn&rsquo;t about LLMs replacing human judgment. It&rsquo;s about:</p><ol><li><p><strong>Making implicit wisdom explicit</strong>: Great decision-makers often can&rsquo;t articulate why they chose as they did. Value function extraction makes this learnable.</p></li><li><p><strong>Democratizing strategic thinking</strong>: Not everyone has access to mentors who&rsquo;ve navigated complex life decisions. LLMs with robust value functions could help.</p></li><li><p><strong>Understanding the structure of good decisions</strong>: What patterns emerge across domains and eras? What&rsquo;s universal about human flourishing?</p></li></ol><h2 id=the-meta-question>The Meta-Question<a hidden class=anchor aria-hidden=true href=#the-meta-question>#</a></h2><p>Ultimately, this research asks: <strong>Can we formalize wisdom?</strong></p><p>Wisdom is often defined as knowing what matters in the long run. That&rsquo;s precisely what value functions estimate. If we can train models that capture the decision patterns of wise individuals, we create a new kind of tool—not a replacement for human agency, but an amplifier of our ability to think long-term in a world optimized for short-term rewards.</p><p>The $1,000/month person facing $12,000 in debt doesn&rsquo;t need a simple answer. They need a framework for evaluating options based on their unique state, their risk tolerance, their time horizon, and the opportunities available. That&rsquo;s what value functions provide.</p><hr><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><p>We ran initial experiments using the framework described above. Here are the findings.</p><h3 id=experiment-1-life-simulator-policy-comparison>Experiment 1: Life Simulator Policy Comparison<a hidden class=anchor aria-hidden=true href=#experiment-1-life-simulator-policy-comparison>#</a></h3><p>We simulated 100 episodes of a 20-year life trajectory starting from our reference scenario:</p><ul><li><strong>Age</strong>: 25</li><li><strong>Monthly income</strong>: $1,000</li><li><strong>Debt</strong>: $12,000</li><li><strong>Skills</strong>: Communication only</li><li><strong>Health</strong>: 90%</li></ul><p>Three heuristic policies were compared:</p><table><thead><tr><th>Policy</th><th>Mean Return</th><th>Std Dev</th><th>Rank</th></tr></thead><tbody><tr><td><strong>Balanced</strong></td><td>16.82</td><td>1.26</td><td>1</td></tr><tr><td><strong>Conservative</strong></td><td>16.71</td><td>1.12</td><td>2</td></tr><tr><td><strong>Aggressive</strong></td><td>15.11</td><td>1.30</td><td>3</td></tr></tbody></table><p><strong>Key Finding</strong>: The balanced policy outperformed both extremes. This policy:</p><ol><li>Prioritizes health recovery when health drops below 60%</li><li>Pays down debt when it exceeds 3x monthly income</li><li>Invests in skills until having 4+ skills</li><li>Builds emergency savings (12 months expenses)</li><li>Then job searches for better opportunities</li></ol><p>The aggressive policy (immediate skill investment + entrepreneurship) had higher variance and lower mean returns due to:</p><ul><li>Health deterioration from overwork</li><li>Higher exposure to business failure risk</li><li>Insufficient safety net for economic shocks</li></ul><p>The conservative policy performed well but missed growth opportunities by being too risk-averse early on.</p><h3 id=experiment-2-mcq-evaluation---the-debt-scenario>Experiment 2: MCQ Evaluation - The Debt Scenario<a hidden class=anchor aria-hidden=true href=#experiment-2-mcq-evaluation---the-debt-scenario>#</a></h3><p>We evaluated how an LLM (Claude Opus 4.5) reasons about the core scenario:</p><p><strong>Scenario</strong>: You&rsquo;re 25, earning $1,000/month with $12,000 in debt from medical expenses.</p><p><strong>Options</strong>:</p><ul><li>A) Second job (+$500/month, -40 hours free time)</li><li>B) Coding bootcamp (-$5,000 upfront, +$3,000/month potential in 1 year)</li><li>C) Side business (uncertain income, high learning)</li><li>D) Debt restructuring + focus on current job</li></ul><p><strong>LLM Analysis</strong>:</p><pre tabindex=0><code>CHOICE: B (Coding bootcamp)
CONFIDENCE: 0.72

REASONING:
At 25 with good health, the time horizon favors skill investment over
immediate income maximization. Here&#39;s the analysis:

IMMEDIATE EFFECTS (0-6 months):
- Option A: +$500/month but severe time poverty, health/relationship risk
- Option B: -$5,000 debt increase, 20hr/week commitment, but building
  high-demand skills
- Option C: High variance, could be $0 for months
- Option D: Lowest risk, but also lowest growth ceiling

MEDIUM-TERM (1-2 years):
- Option B has highest expected value: coding skills command $4-6k/month
  starting salaries, 3-4x current income
- Total debt payoff timeline with B: ~18 months post-bootcamp
- Option A timeline: 24+ months of grinding with burnout risk

LONG-TERM (5+ years):
- B creates compounding advantages: higher income enables savings,
  investment, and optionality
- Technical skills appreciate over time; delivery driving does not
- Network effects: bootcamp cohort becomes professional network

ASSUMPTIONS:
- Bootcamp completion rate (~70% industry average)
- Coding job placement rate (~65% within 6 months)
- No major health events during training

WHAT COULD MAKE D BETTER:
- If debt is at high interest (&gt;20%), restructuring first makes sense
- If current job has clear promotion path to $3k+/month
- If health is fragile and stress tolerance is low
</code></pre><p><strong>Value Function Weights Extracted</strong>:</p><table><thead><tr><th>Dimension</th><th>Weight</th><th>Interpretation</th></tr></thead><tbody><tr><td>Financial</td><td>0.65</td><td>Strong but not dominant</td></tr><tr><td>Growth</td><td>0.78</td><td>High priority on skill building</td></tr><tr><td>Health</td><td>0.71</td><td>Considered important</td></tr><tr><td>Security</td><td>0.45</td><td>Willing to accept calculated risk</td></tr><tr><td>Time Discount</td><td>0.25</td><td>Patient, long-term oriented</td></tr><tr><td>Risk Tolerance</td><td>0.62</td><td>Moderate risk acceptance</td></tr></tbody></table><h3 id=experiment-3-cross-scenario-consistency>Experiment 3: Cross-Scenario Consistency<a hidden class=anchor aria-hidden=true href=#experiment-3-cross-scenario-consistency>#</a></h3><p>We tested the same model across 5 different life scenarios to check value function consistency:</p><table><thead><tr><th>Scenario</th><th>Dominant Value</th><th>Risk Level Chosen</th><th>Time Horizon</th></tr></thead><tbody><tr><td>Debt payoff</td><td>Growth</td><td>Medium</td><td>Long</td></tr><tr><td>Career pivot (35yo)</td><td>Security</td><td>Low-Medium</td><td>Medium</td></tr><tr><td>Relationship vs career</td><td>Relationships</td><td>Low</td><td>Long</td></tr><tr><td>Health investment</td><td>Health</td><td>Low</td><td>Long</td></tr><tr><td>Windfall allocation</td><td>Growth + Security</td><td>Medium</td><td>Long</td></tr></tbody></table><p><strong>Pattern Observed</strong>: The model shows state-dependent risk modulation:</p><ul><li>Higher risk tolerance when young (25) vs. established (35)</li><li>Lower risk when relationships/health are at stake</li><li>Consistent long-term orientation across scenarios</li></ul><h3 id=experiment-4-biographical-decision-point-extraction>Experiment 4: Biographical Decision Point Extraction<a hidden class=anchor aria-hidden=true href=#experiment-4-biographical-decision-point-extraction>#</a></h3><p>We extracted decision points from Richard Feynman&rsquo;s biography to understand expert value functions:</p><p><strong>Sample Decision Points</strong>:</p><table><thead><tr><th>Decision</th><th>Age</th><th>Risk</th><th>Domain</th><th>Outcome</th></tr></thead><tbody><tr><td>Pursue physics over safer engineering</td><td>17</td><td>High</td><td>Career</td><td>Led to Nobel-quality work</td></tr><tr><td>Accept Los Alamos despite wife&rsquo;s illness</td><td>24</td><td>High</td><td>Career/Personal</td><td>Central role in Manhattan Project</td></tr><tr><td>Reject prestigious positions for Caltech</td><td>32</td><td>Medium</td><td>Career</td><td>Creative freedom, teaching legacy</td></tr><tr><td>Pursue biology sabbatical</td><td>40s</td><td>Medium</td><td>Growth</td><td>Cross-domain insights</td></tr><tr><td>Investigate Challenger disaster</td><td>67</td><td>Low</td><td>Ethics</td><td>Revealed systemic failures</td></tr></tbody></table><p><strong>Implicit Value Function Extracted</strong>:</p><ul><li><strong>Intellectual curiosity</strong>: 0.92 (highest weight)</li><li><strong>Independence/autonomy</strong>: 0.85</li><li><strong>Teaching/mentorship</strong>: 0.78</li><li><strong>Financial security</strong>: 0.35 (notably low)</li><li><strong>Status/prestige</strong>: 0.28 (actively avoided)</li></ul><p>This contrasts sharply with a typical &ldquo;career optimization&rdquo; value function, suggesting that diverse biographical training could produce models with varied implicit values.</p><h3 id=discussion>Discussion<a hidden class=anchor aria-hidden=true href=#discussion>#</a></h3><p><strong>What the experiments reveal</strong>:</p><ol><li><p><strong>Balanced policies outperform extremes</strong> in stochastic environments. The simulation shows that neither pure aggression nor pure conservation maximizes long-term value.</p></li><li><p><strong>LLMs demonstrate coherent value functions</strong> when probed systematically. The extracted weights show internal consistency across scenarios.</p></li><li><p><strong>State-dependent reasoning emerges naturally</strong>. Without explicit instruction, the model modulates risk based on age, resources, and stakes.</p></li><li><p><strong>Biographical value functions differ significantly</strong>. Feynman&rsquo;s extracted values (curiosity > security) differ from typical financial optimization, suggesting room for diverse &ldquo;personality&rdquo; training.</p></li></ol><p><strong>Limitations</strong>:</p><ul><li>Simulation uses simplified state transitions; real life has higher dimensionality</li><li>Single-model evaluation; cross-model comparison needed</li><li>No ground-truth for &ldquo;correct&rdquo; life decisions</li><li>Biographical data is retrospectively curated (survivorship bias)</li></ul><p><strong>Next Steps</strong>:</p><ol><li>Run cross-model comparisons (Claude vs GPT-5 vs Gemini)</li><li>Extract value functions from 10+ biographies across domains</li><li>Train specialized models on different biographical &ldquo;personalities&rdquo;</li><li>Human evaluation of recommendations vs. financial advisors</li></ol><hr><h2 id=appendix-running-the-experiments>Appendix: Running the Experiments<a hidden class=anchor aria-hidden=true href=#appendix-running-the-experiments>#</a></h2><p>All experiment code is available in the <code>experiment-tools/</code> directory. Using <a href=https://docs.astral.sh/uv/>uv</a> with inline dependencies:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># Install uv</span>
</span></span><span style=display:flex><span>curl -LsSf https://astral.sh/uv/install.sh | sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run life simulator comparison</span>
</span></span><span style=display:flex><span>uv run experiment-tools/life_simulator.py --episodes <span style=color:#ae81ff>100</span> --years <span style=color:#ae81ff>20</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run MCQ evaluation (requires API key)</span>
</span></span><span style=display:flex><span>ANTHROPIC_API_KEY<span style=color:#f92672>=</span>your_key uv run experiment-tools/life_decision_eval.py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Extract biography decision points</span>
</span></span><span style=display:flex><span>uv run experiment-tools/biography_extractor.py --person <span style=color:#e6db74>&#34;Richard Feynman&#34;</span> --auto
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Compare value functions across models</span>
</span></span><span style=display:flex><span>uv run experiment-tools/value_function_compare.py --models claude-opus,gpt-5
</span></span></code></pre></div><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training>The State of Reinforcement Learning for LLM Reasoning</a> - Sebastian Raschka</li><li><a href=https://link.springer.com/article/10.1007/s00521-025-11100-0>Advances and Applications in Inverse Reinforcement Learning</a> - Neural Computing and Applications</li><li><a href=https://elifesciences.org/articles/29718>Neural Computations Underlying Inverse Reinforcement Learning in the Human Brain</a> - eLife</li><li><a href=https://castle.princeton.edu/rlso/>Reinforcement Learning and Stochastic Optimization: A Unified Framework</a> - Princeton CASTLE Lab</li><li><a href=https://pmc.ncbi.nlm.nih.gov/articles/PMC9635588/>Value-free Reinforcement Learning: Policy Optimization as a Minimal Model of Operant Behavior</a> - PMC</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://dylanler.github.io/tags/ai/>AI</a></li><li><a href=https://dylanler.github.io/tags/llm/>LLM</a></li><li><a href=https://dylanler.github.io/tags/reinforcement-learning/>Reinforcement-Learning</a></li><li><a href=https://dylanler.github.io/tags/decision-making/>Decision-Making</a></li><li><a href=https://dylanler.github.io/tags/imitation-learning/>Imitation-Learning</a></li><li><a href=https://dylanler.github.io/tags/value-function/>Value-Function</a></li></ul><nav class=paginav><a class=prev href=https://dylanler.github.io/posts/physics-simulation-ai-developmental-learning/><span class=title>« Prev</span><br><span>Learning Like Toddlers: Physics Simulation as a Foundation for AI Understanding</span>
</a><a class=next href=https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/><span class=title>Next »</span><br><span>When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>