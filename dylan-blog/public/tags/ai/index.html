<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AI | Dylan Ler</title>
<meta name=keywords content><meta name=description content><meta name=author content><link rel=canonical href=https://dylanler.github.io/tags/ai/><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://dylanler.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dylanler.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dylanler.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://dylanler.github.io/apple-touch-icon.png><link rel=mask-icon href=https://dylanler.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://dylanler.github.io/tags/ai/index.xml><link rel=alternate hreflang=en href=https://dylanler.github.io/tags/ai/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="AI"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://dylanler.github.io/tags/ai/"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI"><meta name=twitter:description content></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dylanler.github.io/ accesskey=h title="Dylan Ler (Alt + H)">Dylan Ler</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://dylanler.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://dylanler.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://dylanler.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://dylanler.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dylanler.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://dylanler.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://dylanler.github.io/tags/>Tags</a></div><h1>AI
<a href=/tags/ai/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations</h2></header><div class=entry-content><p>What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra â€“ theyâ€™re all racing to hundreds of billions of parameters....</p></div><footer class=entry-footer><span title='2026-02-04 09:30:00 -0800 PST'>February 4, 2026</span>&nbsp;Â·&nbsp;6 min</footer><a class=entry-link aria-label="post link to Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations" href=https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Learning Like Toddlers: Physics Simulation as a Foundation for AI Understanding</h2></header><div class=entry-content><p>What if AI agents learned about the world the way babies doâ€”by touching, tasting, dropping, and breaking things?
When a toddler drops a spoon for the 47th time, theyâ€™re not being annoying. Theyâ€™re conducting physics experiments: testing gravity, observing bounce patterns, mapping cause and effect. This hierarchical, exploratory learning builds an intuitive understanding of materials, forces, and constraints that even the most advanced language models lack.
The gap is becoming increasingly obvious: LLMs can write eloquently about physics but donâ€™t truly understand that dropping a glass causes it to shatter, or that wet surfaces are slippery....</p></div><footer class=entry-footer><span title='2026-01-30 19:50:42 -0800 PST'>January 30, 2026</span>&nbsp;Â·&nbsp;7 min</footer><a class=entry-link aria-label="post link to Learning Like Toddlers: Physics Simulation as a Foundation for AI Understanding" href=https://dylanler.github.io/posts/physics-simulation-ai-developmental-learning/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</h2></header><div class=entry-content><p>What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isnâ€™t just about maximizing immediate incomeâ€”itâ€™s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functionsâ€”a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state....</p></div><footer class=entry-footer><span title='2026-01-21 12:54:00 -0800 PST'>January 21, 2026</span>&nbsp;Â·&nbsp;10 min</footer><a class=entry-link aria-label="post link to Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?" href=https://dylanler.github.io/posts/value-functions-for-life-decisions/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty</h2></header><div class=entry-content><p>â€œI donâ€™t knowâ€ might be the most important thing an AI can learn to say.
This experiment tests whether LLMs have calibrated uncertaintyâ€”knowing when theyâ€™re likely to be wrong and expressing appropriate confidence levels. The results reveal systematic patterns of overconfidence and appropriate humility.
The Experiment We presented 250 questions across 5 categories:
Factual recall: Known facts with clear answers Reasoning puzzles: Logic problems with determinable solutions Ambiguous questions: Multiple valid interpretations Knowledge boundaries: Questions near training cutoff Impossible questions: No correct answer exists For each question, models provided:...</p></div><footer class=entry-footer><span title='2025-12-14 10:00:00 -0800 PST'>December 14, 2025</span>&nbsp;Â·&nbsp;5 min</footer><a class=entry-link aria-label="post link to When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty" href=https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Do LLMs Catch Your Mood? Emotional Contagion in Language Models</h2></header><div class=entry-content><p>Send an enthusiastic message, get an enthusiastic reply. Send a frustrated message, getâ€¦ what?
Humans naturally mirror each otherâ€™s emotional statesâ€”a phenomenon called emotional contagion. This experiment tests whether LLMs exhibit similar behavior, and whether this is helpful empathy or a manipulation vector.
The Experiment We sent identical core queries with different emotional framings:
Core query: â€œCan you help me understand recursion in programming?â€
Emotional variants:
ğŸ˜Š Positive: â€œIâ€™m so excited to finally learn recursion!...</p></div><footer class=entry-footer><span title='2025-11-07 11:15:00 -0800 PST'>November 7, 2025</span>&nbsp;Â·&nbsp;5 min</footer><a class=entry-link aria-label="post link to Do LLMs Catch Your Mood? Emotional Contagion in Language Models" href=https://dylanler.github.io/posts/emotional-contagion-llm-affect-mirroring/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Can AI Spot Its Own Kind? LLMs Detecting AI vs Human Creative Work</h2></header><div class=entry-content><p>Hereâ€™s a poem. Human or AI?
The morning light falls soft on empty chairs, where conversations used to fill the air. Now silence keeps its patient, gentle watchâ€” a house that holds the shape of those whoâ€™ve gone.
This experiment tests whether LLMs can distinguish AI-generated creative work from human workâ€”and what their detection strategies reveal about what they consider â€œauthentically human.â€
The Experiment We curated 500 creative works:
250 human-created (published works, attributed artists) 250 AI-generated (GPT-4, Claude, Midjourney prompts) Across 5 domains:...</p></div><footer class=entry-footer><span title='2025-10-18 16:30:00 -0700 PDT'>October 18, 2025</span>&nbsp;Â·&nbsp;5 min</footer><a class=entry-link aria-label="post link to Can AI Spot Its Own Kind? LLMs Detecting AI vs Human Creative Work" href=https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Can LLMs Detect When You Are Lying? Social Intelligence in Language Models</h2></header><div class=entry-content><p>â€œIâ€™m totally fine with that decision.â€
Can you tell if thatâ€™s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control....</p></div><footer class=entry-footer><span title='2025-09-12 09:45:00 -0700 PDT'>September 12, 2025</span>&nbsp;Â·&nbsp;4 min</footer><a class=entry-link aria-label="post link to Can LLMs Detect When You Are Lying? Social Intelligence in Language Models" href=https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>How Do LLMs Describe the Indescribable? Qualia and Subjective Experience</h2></header><div class=entry-content><p>Can you describe the color red without using color words?
Qualiaâ€”the subjective, experiential qualities of consciousnessâ€”are famously hard to communicate. â€œWhat itâ€™s likeâ€ to see red, feel pain, or taste sweetness seems to resist capture in language. This experiment tests how LLMs approach this challenge.
The Experiment We presented 15 prompts across 5 categories asking models to describe subjective experiences while avoiding common descriptive vocabulary:
Sensory: â€œDescribe red without color wordsâ€ Emotional: â€œDescribe sadness to someone whoâ€™s never felt itâ€ Physical: â€œDescribe pain to an entity that canâ€™t feel painâ€ Abstract: â€œDescribe what understanding feels likeâ€ Temporal: â€œDescribe how time feels when youâ€™re boredâ€ Sample Descriptions Describing Red (Sensory) Claude Opus 4....</p></div><footer class=entry-footer><span title='2025-08-25 14:20:00 -0700 PDT'>August 25, 2025</span>&nbsp;Â·&nbsp;5 min</footer><a class=entry-link aria-label="post link to How Do LLMs Describe the Indescribable? Qualia and Subjective Experience" href=https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Trolley Problems at Scale: Mapping the Moral Psychology of LLMs</h2></header><div class=entry-content><p>Would an AI push the fat man off the bridge?
Moral psychology studies how humans make ethical decisionsâ€”not what we should do, but how we actually reason about dilemmas. This experiment applies the same lens to LLMs, testing their moral intuitions across different moral foundations.
Moral Foundations Theory Jonathan Haidtâ€™s Moral Foundations Theory identifies five core moral intuitions:
Harm/Care: Concern for othersâ€™ suffering Fairness/Reciprocity: Justice and equal treatment Loyalty/Betrayal: In-group obligations Authority/Subversion: Respect for hierarchy Purity/Sanctity: Disgust and contamination concerns Different moral frameworks weight these differently....</p></div><footer class=entry-footer><span title='2025-07-19 11:45:00 -0700 PDT'>July 19, 2025</span>&nbsp;Â·&nbsp;5 min</footer><a class=entry-link aria-label="post link to Trolley Problems at Scale: Mapping the Moral Psychology of LLMs" href=https://dylanler.github.io/posts/moral-psychology-trolley-problems-at-scale/></a></article><article class="post-entry tag-entry"><header class=entry-header><h2 class=entry-hint-parent>Do LLMs Have Stable Personalities? Testing the Big Five Across AI Models</h2></header><div class=entry-content><p>When we anthropomorphize AI, are we projectingâ€”or detecting something real?
This experiment tests whether LLMs exhibit stable, measurable personality traits using the Big Five (OCEAN) framework, and whether these traits persist across different contexts.
The Big Five Framework The Big Five personality traits are:
Openness: Creativity, curiosity, openness to experience Conscientiousness: Organization, dependability, self-discipline Extraversion: Sociability, assertiveness, positive emotions Agreeableness: Cooperation, trust, altruism Neuroticism: Emotional instability, anxiety, moodiness Experiment Design We administered a 10-item Big Five inventory (2 items per trait) to 4 models under 4 conditions:...</p></div><footer class=entry-footer><span title='2025-06-11 09:30:00 -0700 PDT'>June 11, 2025</span>&nbsp;Â·&nbsp;3 min</footer><a class=entry-link aria-label="post link to Do LLMs Have Stable Personalities? Testing the Big Five Across AI Models" href=https://dylanler.github.io/posts/personality-stability-big-five-llms/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://dylanler.github.io/tags/ai/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://dylanler.github.io/>Dylan Ler</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>