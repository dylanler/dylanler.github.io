<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Deception on Dylan Ler</title><link>https://dylanler.github.io/tags/deception/</link><description>Recent content in Deception on Dylan Ler</description><generator>Hugo -- 0.133.0</generator><language>en-us</language><lastBuildDate>Fri, 12 Sep 2025 09:45:00 -0700</lastBuildDate><atom:link href="https://dylanler.github.io/tags/deception/index.xml" rel="self" type="application/rss+xml"/><item><title>Can LLMs Detect When You Are Lying? Social Intelligence in Language Models</title><link>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</link><pubDate>Fri, 12 Sep 2025 09:45:00 -0700</pubDate><guid>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</guid><description>&amp;ldquo;I&amp;rsquo;m totally fine with that decision.&amp;rdquo;
Can you tell if that&amp;rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.</description></item></channel></rss>