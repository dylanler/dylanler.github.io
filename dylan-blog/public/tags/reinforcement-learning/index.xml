<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reinforcement-Learning on Dylan Ler</title><link>https://dylanler.github.io/tags/reinforcement-learning/</link><description>Recent content in Reinforcement-Learning on Dylan Ler</description><generator>Hugo -- 0.133.0</generator><language>en-us</language><lastBuildDate>Wed, 21 Jan 2026 12:54:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</title><link>https://dylanler.github.io/posts/value-functions-for-life-decisions/</link><pubDate>Wed, 21 Jan 2026 12:54:00 -0800</pubDate><guid>https://dylanler.github.io/posts/value-functions-for-life-decisions/</guid><description>What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&amp;rsquo;t just about maximizing immediate income—it&amp;rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state.</description></item><item><title>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</title><link>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</guid><description>A proposed experiment to improve LLM reasoning through diverse token sampling and Group Relative Policy Optimization (GRPO)</description></item></channel></rss>