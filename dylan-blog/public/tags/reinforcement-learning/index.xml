<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement-Learning on Dylan Ler</title><link>https://dylanler.github.io/tags/reinforcement-learning/</link><description>Recent content in Reinforcement-Learning on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 21 Jan 2026 12:54:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</title><link>https://dylanler.github.io/posts/value-functions-for-life-decisions/</link><pubDate>Wed, 21 Jan 2026 12:54:00 -0800</pubDate><guid>https://dylanler.github.io/posts/value-functions-for-life-decisions/</guid><description>What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&amp;rsquo;t just about maximizing immediate income—it&amp;rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functions—a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state.</description></item><item><title>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</title><link>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</guid><description>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO The Challenge: Efficient Reasoning in LLMs Large Language Models (LLMs) have become remarkably capable at complex reasoning tasks, but this often comes at a cost: verbose outputs that consume significant computational resources. The Chain of Thought (CoT) prompting technique, while effective for accuracy, generates lengthy reasoning steps that increase token usage and latency.
Enter Chain of Draft (CoD), a promising alternative introduced by Xu et al.</description></item></channel></rss>