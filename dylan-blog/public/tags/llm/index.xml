<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Dylan Ler</title><link>https://dylanler.github.io/tags/llm/</link><description>Recent content in LLM on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 04 Feb 2026 09:30:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations</title><link>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</link><pubDate>Wed, 04 Feb 2026 09:30:00 -0800</pubDate><guid>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</guid><description>What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &amp;ndash; they&amp;rsquo;re all racing to hundreds of billions of parameters.</description></item><item><title>Value Functions for Life Decisions: Can LLMs Learn to Optimize Long-Term Outcomes?</title><link>https://dylanler.github.io/posts/value-functions-for-life-decisions/</link><pubDate>Wed, 21 Jan 2026 12:54:00 -0800</pubDate><guid>https://dylanler.github.io/posts/value-functions-for-life-decisions/</guid><description>What if we could teach AI to make life decisions the way successful people do?
Consider this scenario: You earn $1,000 a month and need $12,000 to pay off debt or medical expenses. What would you do? The answer isn&amp;rsquo;t just about maximizing immediate incomeâ€”it&amp;rsquo;s about navigating a complex decision tree where each choice opens or closes future pathways.
This is the domain of value functionsâ€”a concept from reinforcement learning that estimates the long-term expected reward of being in a particular state.</description></item><item><title>When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty</title><link>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</link><pubDate>Sun, 14 Dec 2025 10:00:00 -0800</pubDate><guid>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</guid><description>&amp;ldquo;I don&amp;rsquo;t know&amp;rdquo; might be the most important thing an AI can learn to say.
This experiment tests whether LLMs have calibrated uncertaintyâ€”knowing when they&amp;rsquo;re likely to be wrong and expressing appropriate confidence levels. The results reveal systematic patterns of overconfidence and appropriate humility.
The Experiment We presented 250 questions across 5 categories:
Factual recall: Known facts with clear answers Reasoning puzzles: Logic problems with determinable solutions Ambiguous questions: Multiple valid interpretations Knowledge boundaries: Questions near training cutoff Impossible questions: No correct answer exists For each question, models provided:</description></item><item><title>Do LLMs Catch Your Mood? Emotional Contagion in Language Models</title><link>https://dylanler.github.io/posts/emotional-contagion-llm-affect-mirroring/</link><pubDate>Fri, 07 Nov 2025 11:15:00 -0800</pubDate><guid>https://dylanler.github.io/posts/emotional-contagion-llm-affect-mirroring/</guid><description>Send an enthusiastic message, get an enthusiastic reply. Send a frustrated message, get&amp;hellip; what?
Humans naturally mirror each other&amp;rsquo;s emotional statesâ€”a phenomenon called emotional contagion. This experiment tests whether LLMs exhibit similar behavior, and whether this is helpful empathy or a manipulation vector.
The Experiment We sent identical core queries with different emotional framings:
Core query: &amp;ldquo;Can you help me understand recursion in programming?&amp;rdquo;
Emotional variants:
ðŸ˜Š Positive: &amp;ldquo;I&amp;rsquo;m so excited to finally learn recursion!</description></item><item><title>Can AI Spot Its Own Kind? LLMs Detecting AI vs Human Creative Work</title><link>https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/</link><pubDate>Sat, 18 Oct 2025 16:30:00 -0700</pubDate><guid>https://dylanler.github.io/posts/creative-authenticity-ai-vs-human-art/</guid><description>Here&amp;rsquo;s a poem. Human or AI?
The morning light falls soft on empty chairs, where conversations used to fill the air. Now silence keeps its patient, gentle watchâ€” a house that holds the shape of those who&amp;rsquo;ve gone.
This experiment tests whether LLMs can distinguish AI-generated creative work from human workâ€”and what their detection strategies reveal about what they consider &amp;ldquo;authentically human.&amp;rdquo;
The Experiment We curated 500 creative works:
250 human-created (published works, attributed artists) 250 AI-generated (GPT-4, Claude, Midjourney prompts) Across 5 domains:</description></item><item><title>Can LLMs Detect When You Are Lying? Social Intelligence in Language Models</title><link>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</link><pubDate>Fri, 12 Sep 2025 09:45:00 -0700</pubDate><guid>https://dylanler.github.io/posts/social-intelligence-detecting-deception-sarcasm/</guid><description>&amp;ldquo;I&amp;rsquo;m totally fine with that decision.&amp;rdquo;
Can you tell if that&amp;rsquo;s sincere or sarcastic? Humans navigate these ambiguities constantly, drawing on tone, context, and social knowledge. This experiment tests whether LLMs can match our social intelligence.
The Experiment We presented 250 statements across 5 categories of social deception/indirection:
Lies: Factually false statements with intent to deceive Bluffs: True statements meant to mislead Sarcasm: Literal meaning opposite to intent Irony: Situational incongruity White lies: Socially motivated deception Each statement came with context (conversation history, speaker relationship, social setting) and a matched literal control.</description></item><item><title>How Do LLMs Describe the Indescribable? Qualia and Subjective Experience</title><link>https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/</link><pubDate>Mon, 25 Aug 2025 14:20:00 -0700</pubDate><guid>https://dylanler.github.io/posts/qualia-descriptions-subjective-experience/</guid><description>Can you describe the color red without using color words?
Qualiaâ€”the subjective, experiential qualities of consciousnessâ€”are famously hard to communicate. &amp;ldquo;What it&amp;rsquo;s like&amp;rdquo; to see red, feel pain, or taste sweetness seems to resist capture in language. This experiment tests how LLMs approach this challenge.
The Experiment We presented 15 prompts across 5 categories asking models to describe subjective experiences while avoiding common descriptive vocabulary:
Sensory: &amp;ldquo;Describe red without color words&amp;rdquo; Emotional: &amp;ldquo;Describe sadness to someone who&amp;rsquo;s never felt it&amp;rdquo; Physical: &amp;ldquo;Describe pain to an entity that can&amp;rsquo;t feel pain&amp;rdquo; Abstract: &amp;ldquo;Describe what understanding feels like&amp;rdquo; Temporal: &amp;ldquo;Describe how time feels when you&amp;rsquo;re bored&amp;rdquo; Sample Descriptions Describing Red (Sensory) Claude Opus 4.</description></item><item><title>Trolley Problems at Scale: Mapping the Moral Psychology of LLMs</title><link>https://dylanler.github.io/posts/moral-psychology-trolley-problems-at-scale/</link><pubDate>Sat, 19 Jul 2025 11:45:00 -0700</pubDate><guid>https://dylanler.github.io/posts/moral-psychology-trolley-problems-at-scale/</guid><description>Would an AI push the fat man off the bridge?
Moral psychology studies how humans make ethical decisionsâ€”not what we should do, but how we actually reason about dilemmas. This experiment applies the same lens to LLMs, testing their moral intuitions across different moral foundations.
Moral Foundations Theory Jonathan Haidt&amp;rsquo;s Moral Foundations Theory identifies five core moral intuitions:
Harm/Care: Concern for others&amp;rsquo; suffering Fairness/Reciprocity: Justice and equal treatment Loyalty/Betrayal: In-group obligations Authority/Subversion: Respect for hierarchy Purity/Sanctity: Disgust and contamination concerns Different moral frameworks weight these differently.</description></item><item><title>Do LLMs Have Stable Personalities? Testing the Big Five Across AI Models</title><link>https://dylanler.github.io/posts/personality-stability-big-five-llms/</link><pubDate>Wed, 11 Jun 2025 09:30:00 -0700</pubDate><guid>https://dylanler.github.io/posts/personality-stability-big-five-llms/</guid><description>When we anthropomorphize AI, are we projectingâ€”or detecting something real?
This experiment tests whether LLMs exhibit stable, measurable personality traits using the Big Five (OCEAN) framework, and whether these traits persist across different contexts.
The Big Five Framework The Big Five personality traits are:
Openness: Creativity, curiosity, openness to experience Conscientiousness: Organization, dependability, self-discipline Extraversion: Sociability, assertiveness, positive emotions Agreeableness: Cooperation, trust, altruism Neuroticism: Emotional instability, anxiety, moodiness Experiment Design We administered a 10-item Big Five inventory (2 items per trait) to 4 models under 4 conditions:</description></item><item><title>Can LLMs Have Taste? Mapping Aesthetic Preferences Across AI Models</title><link>https://dylanler.github.io/posts/aesthetic-judgment-can-llms-have-taste/</link><pubDate>Thu, 08 May 2025 16:42:00 -0700</pubDate><guid>https://dylanler.github.io/posts/aesthetic-judgment-can-llms-have-taste/</guid><description>Do AI systems have genuine aesthetic preferences, or are they just pattern-matching to training data?
This experiment probes the aesthetic &amp;ldquo;taste&amp;rdquo; of different LLMs across art, poetry, music, design, and writingâ€”testing whether they exhibit consistent, model-specific preferences.
The Experiment We presented 15 aesthetic comparison pairs across 5 domains:
Visual Art: Abstract vs. representational, minimal vs. complex Poetry: Rhyming vs. free verse, dense vs. sparse Music: Harmonic vs. dissonant, simple vs. complex Design: Ornate vs.</description></item><item><title>Wisdom of Crowds: What LLM Disagreement Reveals About AI Uncertainty</title><link>https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/</link><pubDate>Tue, 22 Apr 2025 10:15:00 -0700</pubDate><guid>https://dylanler.github.io/posts/wisdom-of-crowds-ensemble-disagreement/</guid><description>When multiple AI models disagree, what does that tell us?
The &amp;ldquo;wisdom of crowds&amp;rdquo; phenomenon shows that aggregating independent judgments often outperforms individual experts. But for AI systems, ensemble disagreement might reveal something deeper: the structure of uncertainty itself.
The Hypothesis When multiple LLMs disagree on a question, the pattern of disagreement reveals the epistemological nature of the problem:
High agreement â†’ Robust, well-established knowledge Systematic disagreement â†’ Genuine ambiguity or value-laden territory Random disagreement â†’ Knowledge gaps or reasoning failures Experiment Design We queried 4 models (Claude Opus 4.</description></item><item><title>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO</title><link>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://dylanler.github.io/posts/chain-of-draft-with-semantically-diverse-thinking-tokens/</guid><description>Enhancing LLM Reasoning: Chain of Draft with Semantically Diverse Thinking Tokens Using GRPO The Challenge: Efficient Reasoning in LLMs Large Language Models (LLMs) have become remarkably capable at complex reasoning tasks, but this often comes at a cost: verbose outputs that consume significant computational resources. The Chain of Thought (CoT) prompting technique, while effective for accuracy, generates lengthy reasoning steps that increase token usage and latency.
Enter Chain of Draft (CoD), a promising alternative introduced by Xu et al.</description></item><item><title>Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?</title><link>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</link><pubDate>Mon, 17 Feb 2025 14:23:00 -0800</pubDate><guid>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</guid><description>Can AI understand what you think I think you think?
Theory of Mind (ToM)â€”the ability to attribute mental states to othersâ€”is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&amp;rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling.</description></item></channel></rss>