<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Theory-of-Mind on Dylan Ler</title><link>https://dylanler.github.io/tags/theory-of-mind/</link><description>Recent content in Theory-of-Mind on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 17 Feb 2025 14:23:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/theory-of-mind/index.xml" rel="self" type="application/rss+xml"/><item><title>Theory of Mind in LLMs: How Deep Can Recursive Belief Modeling Go?</title><link>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</link><pubDate>Mon, 17 Feb 2025 14:23:00 -0800</pubDate><guid>https://dylanler.github.io/posts/theory-of-mind-recursive-beliefs/</guid><description>Can AI understand what you think I think you think?
Theory of Mind (ToM)—the ability to attribute mental states to others—is considered a hallmark of human social intelligence. We naturally track what others believe, want, and intend. But it gets harder when beliefs nest: understanding what Alice thinks Bob believes about Carol&amp;rsquo;s intentions requires recursive modeling that strains even human cognition.
This experiment tests how deep LLMs can go in recursive belief modeling.</description></item></channel></rss>