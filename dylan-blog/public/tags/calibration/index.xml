<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Calibration on Dylan Ler</title><link>https://dylanler.github.io/tags/calibration/</link><description>Recent content in Calibration on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 14 Dec 2025 10:00:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/calibration/index.xml" rel="self" type="application/rss+xml"/><item><title>When Do LLMs Know They Do Not Know? Metacognition and Calibrated Uncertainty</title><link>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</link><pubDate>Sun, 14 Dec 2025 10:00:00 -0800</pubDate><guid>https://dylanler.github.io/posts/metacognition-when-llms-know-they-dont-know/</guid><description>&amp;ldquo;I don&amp;rsquo;t know&amp;rdquo; might be the most important thing an AI can learn to say.
This experiment tests whether LLMs have calibrated uncertaintyâ€”knowing when they&amp;rsquo;re likely to be wrong and expressing appropriate confidence levels. The results reveal systematic patterns of overconfidence and appropriate humility.
The Experiment We presented 250 questions across 5 categories:
Factual recall: Known facts with clear answers Reasoning puzzles: Logic problems with determinable solutions Ambiguous questions: Multiple valid interpretations Knowledge boundaries: Questions near training cutoff Impossible questions: No correct answer exists For each question, models provided:</description></item></channel></rss>