<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Synthetic Data on Dylan Ler</title><link>https://dylanler.github.io/tags/synthetic-data/</link><description>Recent content in Synthetic Data on Dylan Ler</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 04 Feb 2026 09:30:00 -0800</lastBuildDate><atom:link href="https://dylanler.github.io/tags/synthetic-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations</title><link>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</link><pubDate>Wed, 04 Feb 2026 09:30:00 -0800</pubDate><guid>https://dylanler.github.io/posts/fine-tuning-qwen3-p5js-physics-animations/</guid><description>What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?
You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.
The Premise LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra &amp;ndash; they&amp;rsquo;re all racing to hundreds of billions of parameters.</description></item><item><title>Synthetic Data Experiments with LLMs</title><link>https://dylanler.github.io/posts/synthetic-data-experiments/</link><pubDate>Sun, 19 Jan 2025 20:45:48 -0800</pubDate><guid>https://dylanler.github.io/posts/synthetic-data-experiments/</guid><description>Generating High-Quality Synthetic Data for Large Language Models Introduction In the dynamic landscape of artificial intelligence (AI), Large Language Models (LLMs) stand out for their remarkable ability to understand and generate human-like text. Their performance, however, is largely influenced by the quality and diversity of their training data. This guide explores four innovative methods for generating high-quality synthetic dataâ€”each designed to broaden LLMs&amp;rsquo; capabilities and help them excel across a wide range of tasks.</description></item><item><title>Synthetic Data</title><link>https://dylanler.github.io/posts/synthetic-data/</link><pubDate>Mon, 19 Aug 2024 02:19:26 -0700</pubDate><guid>https://dylanler.github.io/posts/synthetic-data/</guid><description>Generating Synthetic Data for Large Language Models: A Comprehensive Guide In the rapidly evolving field of artificial intelligence, the quality and diversity of training data play a pivotal role in the capabilities of Large Language Models (LLMs). This guide delves into four innovative methods designed to generate high-quality synthetic data, aiming to significantly enhance LLM performance across a variety of tasks. Whether you&amp;rsquo;re a researcher, developer, or AI enthusiast, understanding these methods can provide valuable insights into the future of AI training and development.</description></item></channel></rss>