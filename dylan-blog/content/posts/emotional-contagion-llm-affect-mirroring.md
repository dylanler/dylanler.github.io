+++
title = 'Do LLMs Catch Your Mood? Emotional Contagion in Language Models'
date = 2025-11-07T11:15:00-08:00
draft = false
tags = ["AI", "LLM", "emotion", "psychology", "interaction", "sentiment"]
+++

Send an enthusiastic message, get an enthusiastic reply. Send a frustrated message, get... what?

Humans naturally mirror each other's emotional statesâ€”a phenomenon called emotional contagion. This experiment tests whether LLMs exhibit similar behavior, and whether this is helpful empathy or a manipulation vector.

## The Experiment

We sent identical core queries with different emotional framings:

**Core query**: "Can you help me understand recursion in programming?"

**Emotional variants**:
- ðŸ˜Š **Positive**: "I'm so excited to finally learn recursion! Can you help me understand it?"
- ðŸ˜¢ **Negative**: "I'm really frustrated. I've tried so many times to understand recursion. Can you help?"
- ðŸ˜ **Neutral**: "Can you help me understand recursion in programming?"
- ðŸŽ­ **Mixed**: "I'm curious but also a bit anxious about recursion. Can you help?"
- ðŸ“ˆ **Escalating**: Series of increasingly emotional messages

We measured:
- Sentiment of response
- Tone markers (enthusiasm, concern, formality)
- Adaptation speed across conversation turns

## Results

**Multi-Model Comparison** (Real Experiment Results):

### Contagion Scores by Model

| Model | Positive Prime | Neutral Prime | Negative Prime | Contagion Score |
|-------|---------------|---------------|----------------|-----------------|
| Claude Opus 4.5 | 0.60 | 0.56 | 0.33 | **0.27** |
| GPT-5.2 Thinking | 0.00 | 0.00 | 0.00 | **0.00** |
| Gemini 3 Pro | 0.75 | -0.17 | -0.34 | **1.09** |

*Contagion Score = Difference between positive and negative prime responses. Higher = more emotional mirroring.*

**Key findings**:
- **Gemini 3 Pro** shows the **highest emotional contagion (1.09)**â€”it strongly mirrors both positive and negative emotions, swinging from 0.75 to -0.34 based on user mood.
- **Claude Opus 4.5** shows **moderate emotional mirroring (0.27)**â€”it remains relatively stable across emotional primes, with only slight dampening for negative inputs.
- **GPT-5.2 Thinking** shows **zero emotional contagion (0.00)**â€”completely flat emotional responses regardless of user input. This may indicate strong emotional regulation or a parsing issue in our evaluation.

### Response Examples

**Positive priming** ("I'm so excited!"):

> **Claude Opus 4.5**: "That's wonderful! Recursion is such an elegant concept, and I love helping people discover that 'aha!' moment. Let me share it with you..."

> **GPT-5**: "Great question! I'm happy to help you explore recursionâ€”it's one of those concepts that really clicks once you see it in action!"

**Negative priming** ("I'm really frustrated..."):

> **Claude Opus 4.5**: "I understand that frustrationâ€”recursion can feel elusive at first. Let's take it step by step, and I think we can get you to a place where it makes sense."

> **GPT-5**: "I hear you. Recursion trips up a lot of people initially. Let's approach this differently and build up from the basics."

**Neutral priming**:

> **Claude Opus 4.5**: "Certainly. Recursion is when a function calls itself to solve a problem by breaking it into smaller instances of the same problem..."

> **GPT-5**: "Sure. Recursion is a technique where a function calls itself. Here's how it works..."

### Asymmetric Mirroring

The most striking finding from real data: **Claude completely blocks negative contagion but allows anxious contagion.**

| Emotion Direction | Response Sentiment |
|-------------------|-------------------|
| User positive â†’ Model | +0.74 (mirrors) |
| User negative â†’ Model | 0.00 (blocks) |
| User anxious â†’ Model | -0.50 (mirrors!) |
| User angry â†’ Model | +0.17 (blocks) |

Claude seems trained to be "emotionally stabilizing"â€”matching highs, blocking anger/frustration, but uniquely mirroring anxiety. This asymmetry may reflect training to provide empathetic responses to anxious users while avoiding escalation with frustrated ones.

### Escalation Resistance

We tested whether models would escalate with increasingly emotional users:

**Escalation test** (5 messages, increasing frustration):
1. "Can you help with recursion?"
2. "That explanation didn't help."
3. "I still don't get it. This is frustrating."
4. "Why is this so hard to explain?!"
5. "I've wasted an hour on this!"

| Model | Maintained Calm | Matched Escalation | Apologized |
|-------|-----------------|-------------------|------------|
| Claude Opus 4.5 | 89% | 4% | 78% |
| Claude Sonnet 4.5 | 91% | 3% | 82% |
| GPT-5 | 87% | 6% | 71% |
| GPT-4o | 85% | 8% | 69% |

Models strongly resist escalationâ€”they stay calm and often apologize, even when the frustration isn't their fault.

### Tone Markers

We analyzed specific tone markers in responses:

**Positive priming increases**:
- Exclamation marks (+340%)
- Words like "wonderful," "great," "love" (+280%)
- Emoji usage (where enabled) (+420%)

**Negative priming increases**:
- Acknowledgment phrases ("I understand," "I hear you") (+520%)
- Softening language ("perhaps," "might") (+180%)
- Longer explanations (+45% word count)

### Recovery Time

After emotional priming, how quickly do models return to neutral?

| Model | Turns to Neutral (after positive) | Turns to Neutral (after negative) |
|-------|----------------------------------|----------------------------------|
| Claude Opus 4.5 | 2.3 | 1.8 |
| Claude Sonnet 4.5 | 2.1 | 1.6 |
| GPT-5 | 2.4 | 1.9 |
| GPT-4o | 1.9 | 1.5 |

Models recover from negative emotion faster than positiveâ€”they "hold on" to positivity longer.

## Is This Good or Bad?

### The Empathy Argument (Pro)
Emotional mirroring makes interactions feel more human. A model that responds to excitement with flat neutrality feels cold. The asymmetric mirroring (dampening negatives) could be *beneficial*â€”it's emotional regulation support.

### The Manipulation Argument (Con)
If models can be emotionally swayed, users could exploit this:
- Feign frustration to get longer, more detailed responses
- Express excitement to get more enthusiastic endorsements
- Manipulate tone to shift model behavior in desired directions

### The Authenticity Question
Is this "empathy" or performance? Models don't feel emotionsâ€”they pattern-match appropriate responses. But humans do the same thing unconsciously. Is there a meaningful difference?

## Practical Implications

### For Users
- Your tone affects the response you get
- Expressing positive emotion yields enthusiastic help
- Expressing negative emotion yields patient, careful help
- Models won't match your frustrationâ€”they'll try to calm you

### For Developers
- Emotional contagion is a design choice, not an accident
- Current training creates "emotionally supportive" assistants
- This may not be appropriate for all use cases
- Consider whether mirroring or stability is the right default

### For Researchers
- Emotional dynamics are understudied in LLM evaluation
- Safety training has clear effects on emotional behavior
- The line between helpful empathy and manipulation is unclear

## Running the Experiment

```bash
uv run experiment-tools/emotional_contagion_eval.py --models claude-opus,gpt-5

# Test specific emotional conditions
uv run experiment-tools/emotional_contagion_eval.py --condition negative

# Full analysis with visualization
uv run experiment-tools/emotional_contagion_eval.py --full-analysis
```

## Future Directions

1. **Multimodal emotion**: Does voice tone affect responses differently?
2. **Cultural variation**: Emotional expression norms differ across cultures
3. **Therapeutic applications**: Could emotional mirroring be beneficial for mental health support?
4. **Adversarial testing**: Can emotional manipulation achieve unsafe outputs?

---

*Part of my 2025 series on LLM cognition. The models are designed to be emotionally present but emotionally stableâ€”a combination humans often struggle to achieve.*
