+++
title = 'Teaching a 0.6B Model to See Physics: Fine-Tuning Qwen3 for p5.js Animations'
date = 2026-02-04T09:30:00-08:00
draft = false
tags = ["AI", "LLM", "fine-tuning", "LoRA", "code-generation", "physics-simulation", "education", "synthetic-data"]
+++

What happens when you take one of the smallest language models available, feed it a thousand physics animations generated by one of the largest, and ask it to teach K-12 students about science?

You get a model that weighs less than a gigabyte, trains in under 3 minutes, and generates interactive physics simulations on demand.

## The Premise

LLMs are getting bigger. GPT-5, Claude Opus, Gemini Ultra -- they're all racing to hundreds of billions of parameters. But there's a parallel question that doesn't get enough attention: **how small can a model be and still do something genuinely useful?**

This experiment answers that for a specific domain: generating p5.js animations that teach physics and science concepts. Think interactive simulations of gravity, wave interference, circuit flow, volcano eruptions -- the kind of visual explanations that make abstract physics click for students.

The pipeline: use Claude Opus as a "teacher" to generate 1,036 high-quality examples, then distill that knowledge into Qwen3-0.6B (792M parameters) using LoRA fine-tuning. The entire training takes **2 minutes and 51 seconds** on 4x A100 GPUs.

## Why p5.js?

p5.js hits a sweet spot for AI-generated educational content:

1. **Self-contained**: A single file runs in any browser. No build systems, no dependencies.
2. **Visual by default**: Every sketch has a `setup()` and `draw()` loop -- the model must think in terms of animation frames.
3. **Physics-native**: Vectors, forces, particles, collisions -- p5.js's API maps naturally onto physics concepts.
4. **Immediately verifiable**: Run the output, see if the physics looks right. No ambiguous evaluation.

The constraint of a 600x400 canvas with `setup()`/`draw()` structure also gives the model a consistent "grammar" to learn -- which matters enormously when your model has only 792M parameters.

## The Dataset: 100 Claude Agents Working in Parallel

The most interesting engineering decision was how to generate the training data. Rather than manually curating examples or scraping the web, the pipeline spawns **100+ parallel Claude Opus agents**, each assigned a subset of 124 K-12 science topics.

Each agent generates 10 variations per topic, varying:
- **Visual style**: Particle-based, diagram-based, simulation-based, story-based
- **Interactivity**: Mouse-driven, automatic, key-press controlled
- **Complexity**: From K-2 (colored balls falling) to 11-12 (double-slit interference)
- **Aesthetics**: Color schemes, label placement, animation speed

The result: **1,036 validated examples** covering everything from simple gravity demonstrations to nuclear fission animations. Code lengths range from 695 to 9,090 characters, with an average of 2,845 characters per sketch.

### Topic Coverage

The breadth is impressive:

| Domain | Example Topics |
|--------|---------------|
| **Forces & Motion** | Gravity, Newton's 3 laws, friction, projectile motion, circular motion |
| **Waves** | Sound propagation, Doppler effect, interference, standing waves |
| **Light** | Reflection, refraction, prisms, double-slit experiment, rainbows |
| **Electricity** | Circuits, series/parallel, Ohm's law, generators, static electricity |
| **Space** | Moon phases, eclipses, orbital mechanics, black holes, star life cycles |
| **Chemistry** | Atomic structure, chemical reactions, diffusion, phase changes |
| **Biology** | Photosynthesis, mitosis, circulation, food chains, respiration |
| **Modern Physics** | Radioactive decay, nuclear fission/fusion, special relativity |

What strikes me is that this is essentially **curriculum-complete** for K-12 science. A single sub-billion-parameter model, if it works, could generate visual explanations for virtually any concept a student encounters.

## Training: LoRA in Under 3 Minutes

The training setup is deliberately efficient:

- **Base model**: Qwen3-0.6B
- **Method**: LoRA (rank 64, alpha 128)
- **Trainable parameters**: 40.4M (5.1% of the model)
- **Hardware**: 4x A100 GPUs with bf16 mixed precision
- **Training time**: 171.87 seconds (2.9 minutes)
- **Effective batch size**: 32 (4 per device x 2 gradient accumulation x 4 GPUs)

### Loss Progression

| Step | Loss | Token Accuracy |
|------|------|---------------|
| 10 | 0.909 | 77.0% |
| 30 | 0.621 | 82.3% |
| 50 | 0.549 | 84.0% |
| 70 | 0.510 | 84.9% |
| 93 (final) | 0.592 | 85.6% |

The loss curve tells an interesting story. The model converges rapidly -- 45% relative loss reduction in under 3 minutes -- and the evaluation loss tracks training loss closely, suggesting good generalization rather than memorization.

**85.6% token accuracy** means the model correctly predicts the next token in p5.js code ~86% of the time. For code generation, this is remarkably high. The structured nature of p5.js (consistent function signatures, canvas operations, physics math patterns) gives the model strong priors to work with.

## What This Actually Means

### 1. Knowledge Distillation Works for Code Generation

The core insight: Claude Opus (hundreds of billions of parameters, massive training data) can "teach" Qwen3-0.6B (792M parameters) to generate domain-specific code. The small model doesn't need to understand physics from first principles -- it needs to learn the **mapping from natural language descriptions to p5.js patterns**.

This is closer to how human students learn programming: you don't derive JavaScript from mathematical axioms, you see patterns and internalize them.

### 2. Small Models + Narrow Domains = Surprising Capability

A 0.6B model can't write arbitrary code. But restrict the domain to "p5.js animations on a 600x400 canvas teaching K-12 physics" and suddenly the problem becomes tractable. The constraints reduce the output space dramatically:

- Fixed canvas size
- Standard `setup()`/`draw()` structure
- Limited API surface (vectors, shapes, text, color)
- Predictable physics patterns (gravity = `vy += 0.15`, bounce = `vy *= -0.8`)

This has implications for edge deployment. A 0.6B model runs on a phone, a Raspberry Pi, or embedded in a web app. You could have an offline-capable physics animation generator that works without internet access.

### 3. Parallel Agent Generation Creates Better Data

The 100-agent approach to dataset generation isn't just faster -- it produces more diverse data. Each agent independently varies its style, producing natural variation that a single sequential generation would struggle to match. The result is a dataset where the model learns multiple ways to visualize the same concept.

## The Bigger Picture

This experiment is a proof of concept for a broader pattern:

```
Large model generates domain-specific training data
  → Small model learns the domain
  → Deploy small model at edge/scale
  → Iterate with human feedback
```

The economics are compelling. Training takes 3 minutes on rented GPUs (~$1). Inference runs on consumer hardware. And the model produces genuinely useful educational content.

Imagine this applied to:
- **Math visualization**: Interactive proofs and geometric constructions
- **Chemistry**: Molecular dynamics and reaction animations
- **Music theory**: Audio-visual representations of harmony and rhythm
- **History**: Animated timelines and interactive maps

The pattern scales. The models shrink. The content gets better.

## Running It Yourself

The entire pipeline is open source:

```bash
git clone https://github.com/dylanler/qwen3-p5js-physics
cd qwen3-p5js-physics
uv sync

# Generate dataset (needs ANTHROPIC_API_KEY)
uv run python scripts/generate_dataset.py

# Fine-tune on 4 GPUs
uv run accelerate launch --config_file configs/accelerate_config.yaml scripts/train.py

# Generate an animation
uv run python scripts/inference.py "Show me how gravity affects falling objects"

# Serve as API
uv run python scripts/serve.py --port 8000
```

The served model exposes an OpenAI-compatible API, so you can drop it into any existing application.

## What I'd Do Next

1. **Human evaluation loop**: Have actual teachers rate generated animations for accuracy and pedagogical value
2. **Multi-turn refinement**: "Make the gravity stronger" / "Add a label showing velocity"
3. **Model scaling ladder**: Compare 0.6B, 1.5B, 3B, 7B to find the accuracy/size sweet spot
4. **Domain expansion**: Three.js for 3D physics, Manim for mathematical animations
5. **Student interaction data**: Log which animations students actually find helpful and fine-tune on that signal

The most exciting direction is **closing the loop**: students interact with generated animations, their engagement signals feed back into training, and the model gets better at explaining what students actually struggle with.

---

*Source code: [github.com/dylanler/qwen3-p5js-physics](https://github.com/dylanler/qwen3-p5js-physics)*

*The smallest model in the lab might be the most useful one in the classroom.*
