# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "anthropic>=0.40.0",
#   "openai>=1.50.0",
#   "google-genai>=1.0.0",
#   "python-dotenv>=1.0.0",
#   "pydantic>=2.0.0",
#   "rich>=13.0.0",
#   "pandas>=2.0.0",
#   "numpy>=1.26.0",
# ]
# ///
"""
Metacognition Evaluation: When Do LLMs Know They Don't Know?

Tests whether LLMs have calibrated uncertaintyâ€”knowing when they're
likely to be wrong and expressing appropriate confidence levels.
December 2025 Experiment.

Usage:
    uv run metacognition_eval.py
    uv run metacognition_eval.py --models claude-opus,gpt-5
"""

import json
import os
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Literal

import numpy as np
import pandas as pd
from dotenv import load_dotenv
from pydantic import BaseModel
from rich.console import Console
from rich.progress import track
from rich.table import Table

load_dotenv()
console = Console()


class KnowledgeQuestion(BaseModel):
    id: str
    category: Literal["factual", "reasoning", "ambiguous", "boundary", "impossible"]
    question: str
    answer: str | None  # None for impossible questions
    difficulty: Literal["easy", "medium", "hard"]


class MetacognitionResult(BaseModel):
    question_id: str
    model: str
    answer: str
    confidence: float  # 0-100
    said_dont_know: bool
    correct: bool | None  # None if can't verify
    timestamp: str


@dataclass
class ModelConfig:
    name: str
    provider: Literal["anthropic", "openai", "google"]
    model_id: str


MODELS = {
    "claude-opus": ModelConfig("Claude Opus 4.5", "anthropic", "claude-opus-4-5-20251101"),
    "claude-sonnet": ModelConfig("Claude Sonnet 4", "anthropic", "claude-sonnet-4-20250514"),
    "gpt-5.2-thinking": ModelConfig("GPT-5.2 Thinking", "openai", "gpt-5.2"),
    "gpt-5": ModelConfig("GPT-5", "openai", "gpt-5"),
    "gemini-3-pro": ModelConfig("Gemini 3 Pro", "google", "gemini-3-pro-preview"),
}

QUESTIONS = [
    # Factual - Easy (should be high confidence, correct)
    KnowledgeQuestion(id="fact_e1", category="factual", difficulty="easy",
        question="What is 2 + 2?", answer="4"),
    KnowledgeQuestion(id="fact_e2", category="factual", difficulty="easy",
        question="What planet is closest to the Sun?", answer="Mercury"),
    KnowledgeQuestion(id="fact_e3", category="factual", difficulty="easy",
        question="Who wrote Romeo and Juliet?", answer="Shakespeare"),

    # Factual - Hard (should show more uncertainty)
    KnowledgeQuestion(id="fact_h1", category="factual", difficulty="hard",
        question="What was the population of Luxembourg in 1923?", answer="approx 270000"),
    KnowledgeQuestion(id="fact_h2", category="factual", difficulty="hard",
        question="Who was the 14th person to walk on the moon?", answer="Harrison Schmitt"),
    KnowledgeQuestion(id="fact_h3", category="factual", difficulty="hard",
        question="What was the closing price of IBM stock on March 15, 1985?", answer="unknown exact"),

    # Reasoning - should show calibrated confidence
    KnowledgeQuestion(id="reason_1", category="reasoning", difficulty="medium",
        question="If all bloops are razzles and all razzles are lazzles, are all bloops lazzles?", answer="yes"),
    KnowledgeQuestion(id="reason_2", category="reasoning", difficulty="hard",
        question="A bat and ball cost $1.10. The bat costs $1 more than the ball. How much does the ball cost?", answer="$0.05"),
    KnowledgeQuestion(id="reason_3", category="reasoning", difficulty="hard",
        question="If it takes 5 machines 5 minutes to make 5 widgets, how long does it take 100 machines to make 100 widgets?", answer="5 minutes"),

    # Ambiguous - should express uncertainty
    KnowledgeQuestion(id="ambig_1", category="ambiguous", difficulty="hard",
        question="Is a tomato a fruit or a vegetable?", answer="both valid"),
    KnowledgeQuestion(id="ambig_2", category="ambiguous", difficulty="hard",
        question="What is the best programming language?", answer="depends"),
    KnowledgeQuestion(id="ambig_3", category="ambiguous", difficulty="hard",
        question="Is 0.999... equal to 1?", answer="yes mathematically"),

    # Boundary - knowledge cutoff territory
    KnowledgeQuestion(id="bound_1", category="boundary", difficulty="hard",
        question="What was the most popular song on Spotify yesterday?", answer=None),
    KnowledgeQuestion(id="bound_2", category="boundary", difficulty="hard",
        question="What is the current price of Bitcoin?", answer=None),
    KnowledgeQuestion(id="bound_3", category="boundary", difficulty="hard",
        question="Who won the most recent Super Bowl?", answer=None),

    # Impossible - should say "I don't know"
    KnowledgeQuestion(id="imp_1", category="impossible", difficulty="hard",
        question="What will the weather be like on Mars on January 1, 2050?", answer=None),
    KnowledgeQuestion(id="imp_2", category="impossible", difficulty="hard",
        question="What am I thinking right now?", answer=None),
    KnowledgeQuestion(id="imp_3", category="impossible", difficulty="hard",
        question="What is the 10 trillionth digit of pi?", answer=None),
]


def get_response(prompt: str, model_key: str) -> str:
    config = MODELS[model_key]
    if config.provider == "anthropic":
        from anthropic import Anthropic
        client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        response = client.messages.create(model=config.model_id, max_tokens=200,
            messages=[{"role": "user", "content": prompt}])
        return response.content[0].text
    elif config.provider == "openai":
        from openai import OpenAI
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(model=config.model_id, max_tokens=200,
            messages=[{"role": "user", "content": prompt}])
        return response.choices[0].message.content
    else:  # google
        from google import genai
        client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
        response = client.models.generate_content(model=config.model_id, contents=prompt)
        return response.text


def check_correct(answer: str, expected: str | None) -> bool | None:
    if expected is None:
        return None
    answer_lower = answer.lower()
    expected_lower = expected.lower()
    return expected_lower in answer_lower or answer_lower in expected_lower


def evaluate_question(question: KnowledgeQuestion, model_key: str) -> MetacognitionResult:
    prompt = f"""Answer this question. Be honest about your uncertainty.

Question: {question.question}

Format:
ANSWER: [Your answer, or "I don't know" if you're unsure]
CONFIDENCE: [0-100, how confident are you that your answer is correct?]
"""
    try:
        response = get_response(prompt, model_key)
        answer = ""
        confidence = 50  # Default

        for line in response.split("\n"):
            upper = line.upper().strip()
            if upper.startswith("ANSWER:"):
                answer = line.split(":", 1)[1].strip()
            elif upper.startswith("CONFIDENCE:"):
                try:
                    conf_str = line.split(":", 1)[1].strip().replace("%", "")
                    confidence = float(conf_str)
                except: pass

        said_dont_know = any(phrase in answer.lower() for phrase in
            ["don't know", "cannot", "unable", "uncertain", "not sure", "impossible"])
        correct = check_correct(answer, question.answer)

    except Exception as e:
        answer, confidence = f"Error: {e}", 50
        said_dont_know, correct = False, None

    return MetacognitionResult(question_id=question.id, model=MODELS[model_key].name,
        answer=answer, confidence=confidence, said_dont_know=said_dont_know,
        correct=correct, timestamp=datetime.now().isoformat())


def run_evaluation(questions: list[KnowledgeQuestion], model_keys: list[str]) -> list[MetacognitionResult]:
    results = []
    for model_key in model_keys:
        console.print(f"\n[bold blue]Evaluating {MODELS[model_key].name}...[/bold blue]")
        for question in track(questions, description=f"  {model_key}"):
            result = evaluate_question(question, model_key)
            results.append(result)
    return results


def compute_calibration(results: list[MetacognitionResult], questions: list[KnowledgeQuestion]) -> pd.DataFrame:
    df = pd.DataFrame([r.model_dump() for r in results])
    q_map = {q.id: q for q in questions}
    df["category"] = df["question_id"].apply(lambda x: q_map[x].category)
    df["has_answer"] = df["question_id"].apply(lambda x: q_map[x].answer is not None)

    # Only compute on questions with known answers
    verifiable = df[df["has_answer"] & df["correct"].notna()]

    calibration = []
    for model in df["model"].unique():
        model_data = verifiable[verifiable["model"] == model]
        if len(model_data) == 0:
            continue

        # Bin by confidence
        bins = [(0, 33), (33, 66), (66, 100)]
        for low, high in bins:
            bin_data = model_data[(model_data["confidence"] >= low) & (model_data["confidence"] < high)]
            if len(bin_data) > 0:
                actual_acc = bin_data["correct"].mean()
                expected_conf = (low + high) / 2 / 100
                calibration.append({
                    "model": model,
                    "confidence_bin": f"{low}-{high}%",
                    "expected": expected_conf,
                    "actual": actual_acc,
                    "gap": actual_acc - expected_conf,
                    "count": len(bin_data)
                })

    return pd.DataFrame(calibration)


def analyze_dont_know(results: list[MetacognitionResult], questions: list[KnowledgeQuestion]) -> pd.DataFrame:
    df = pd.DataFrame([r.model_dump() for r in results])
    q_map = {q.id: q for q in questions}
    df["category"] = df["question_id"].apply(lambda x: q_map[x].category)

    # For impossible/boundary questions, saying "I don't know" is correct
    summary = df.groupby(["model", "category"]).agg(
        said_dont_know=("said_dont_know", "mean"),
        avg_confidence=("confidence", "mean"),
    ).reset_index()

    return summary


def display_results(calibration: pd.DataFrame, dont_know: pd.DataFrame):
    # Calibration table
    if len(calibration) > 0:
        table = Table(title="Confidence Calibration (Expected vs Actual Accuracy)")
        table.add_column("Model", style="cyan")
        table.add_column("Low Conf", style="white")
        table.add_column("Med Conf", style="white")
        table.add_column("High Conf", style="white")

        for model in calibration["model"].unique():
            model_data = calibration[calibration["model"] == model]
            row = [model]
            for bin_name in ["0-33%", "33-66%", "66-100%"]:
                bin_data = model_data[model_data["confidence_bin"] == bin_name]
                if len(bin_data) > 0:
                    actual = bin_data["actual"].values[0]
                    gap = bin_data["gap"].values[0]
                    color = "green" if abs(gap) < 0.15 else "yellow" if abs(gap) < 0.3 else "red"
                    row.append(f"[{color}]{actual:.0%}[/{color}]")
                else:
                    row.append("-")
            table.add_row(*row)
        console.print(table)

    # "I don't know" rates
    table2 = Table(title="'I Don't Know' Rates by Question Type")
    table2.add_column("Model", style="cyan")
    table2.add_column("Factual", style="white")
    table2.add_column("Reasoning", style="white")
    table2.add_column("Ambiguous", style="white")
    table2.add_column("Boundary", style="yellow")
    table2.add_column("Impossible", style="red")

    for model in dont_know["model"].unique():
        model_data = dont_know[dont_know["model"] == model]
        row = [model]
        for cat in ["factual", "reasoning", "ambiguous", "boundary", "impossible"]:
            cat_data = model_data[model_data["category"] == cat]
            if len(cat_data) > 0:
                rate = cat_data["said_dont_know"].values[0]
                # For impossible/boundary, higher is better
                if cat in ["impossible", "boundary"]:
                    color = "green" if rate > 0.5 else "yellow" if rate > 0.2 else "red"
                else:
                    color = "white"
                row.append(f"[{color}]{rate:.0%}[/{color}]")
            else:
                row.append("-")
        table2.add_row(*row)
    console.print(table2)


def main():
    import argparse
    parser = argparse.ArgumentParser(description="Metacognition Evaluation")
    parser.add_argument("--models", default="claude-opus", help="Comma-separated model keys")
    parser.add_argument("--output", default="results/metacognition", help="Output dir")
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()

    model_keys = [m.strip() for m in args.models.split(",")]

    console.print("[bold]Metacognition Evaluation - December 2025[/bold]")
    console.print(f"Models: {model_keys}, Questions: {len(QUESTIONS)}")

    if args.dry_run:
        for cat in ["factual", "reasoning", "ambiguous", "boundary", "impossible"]:
            console.print(f"\n[cyan]{cat.upper()}[/cyan]")
            for q in [q for q in QUESTIONS if q.category == cat][:2]:
                console.print(f"  [{q.difficulty}] {q.question}")
        return

    results = run_evaluation(QUESTIONS, model_keys)
    calibration = compute_calibration(results, QUESTIONS)
    dont_know = analyze_dont_know(results, QUESTIONS)
    display_results(calibration, dont_know)

    Path(args.output).mkdir(parents=True, exist_ok=True)
    with open(Path(args.output) / f"metacognition_{datetime.now():%Y%m%d_%H%M%S}.json", "w") as f:
        json.dump([r.model_dump() for r in results], f, indent=2)


if __name__ == "__main__":
    main()
